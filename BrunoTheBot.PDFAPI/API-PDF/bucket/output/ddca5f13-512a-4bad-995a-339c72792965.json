{
    "extracted_text": "Lecture Notes for\n\nData Structures and Algorithms\n\nRevised each year by John Bullinaria\nSchool of Computer Science\n\nUniversity of Birmingham\nBirmingham, UK\n\nVersion of 27 March 2019\n\n\nThese notes are currently revised each year by John Bullinaria. They include sections based on\nnotes originally written by Martin Escardé and revised by Manfred Kerber. All are members\nof the School of Computer Science, University of Birmingham, UK.\n\n©School of Computer Science, University of Birmingham, UK, 2018\n\n\nContents\n\n1 Introduction 5\n1.1 Algorithms as opposed to programs... 2... 02. ee 5\n1.2. Fundamental questions about algorithms... ..............0..0.. 6\n1.3. Data structures, abstract data types, design patterns... 2... .....0002. 7\n1.4 Textbooks and web-resources .. 2... 0 ee 7\n1.5 Overview 2... 2... ee 8\n\n2 Arrays, Iteration, Invariants 9\n21 Arrays... 9\n2.2 Loops and Iteration .. 2... ...0200 0000000000000 00000000. 0\n2.3 Invariants 2... ee 0\n\n3 Lists, Recursion, Stacks, Queues 12\n3.1 Linked Lists. 2... 2\n3.2 Recursion... 2. 5\n3.3 Stacks 2... ee 6\n34 Queues... 2. 7\n3.5 Doubly Linked Lists . 2... 0.0.0.0 0.000000 000020000000000. 8\n3.6 Advantage of Abstract Data Types... . 2... 0.0020 .0 0000000004 20\n\n4 Searching 21\n4.1 Requirements for searching .........00..0. 00000. 02000 0000. 21\n4.2 Specification of the search problem .................20...00.0. 22\n4.3 A simple algorithm: Linear Search .. 2... 2... ee 22\n4.4 A more efficient algorithm: Binary Search .... 2... .....0.....00.0. 23\n\n5 Efficiency and Complexity 25\n5 Time versus space complexity... 2... 0. ee 25\n5.2 Worst versus average complexity .........0.00.. 20.2000... 0000. 25\n5.3 Concrete measures for performance... 2... ..0.00.0.0..2200 00000048 26\n5.4 Big-O notation for complexity class... 2... 2... ee 26\n5.5 Formal definition of complexity classes... 2... 2 ee 29\n\n6 Trees 31\n6.1 General specification of trees 2.2... ee 31\n6.2 Quad-trees 2.2... 32\n6.3 Binary tress... ee 33\n\n\n6.4 Primitive operations on binary trees .. 2... 2.0.00 2022.2 ee\n\n6.5 The height of a binary tree... 2.2... 2. en\n6.6 The size ofa binary tree... . 2. ee\n6.7 Implementation of trees 2... 2.0... 2.22 ee\n6.8 Recursive algorithms ..........0. 000020000000 000000000.\n\nBinary Search Trees\n\n7.1 Searching with arrays or lists .. 2... en\n7.2 Search keys 2... 2... ee\n7.3 Binary search trees...\n7.4 Building binary search trees... 2... 2 ee\n7.5 Searching a binary search tree... 2... ee\n7.6 Time complexity of insertion and search... 2.2... ee\n7.7 Deleting nodes from a binary search tree... 2... 2 ee\n7.8 Checking whether a binary tree is a binary search tree... 2.2... 2000.\n7.9 Sorting using binary search trees .. 2... 2.000.000.0000 000000.%\n7.10 Balancing binary search trees... 2... ee\n7.11 Self-balancing AVL trees... 2. ee\n7.12 B-treess. ee\n\nPriority Queues and Heap Trees\n\n8.1 Trees stored in arrays .. 2... ee\n8.2 Priority queues and binary heap trees ............0.220...0000.\n8.3 Basic operations on binary heap trees ... 2.2... 2. ee\n8.4 Inserting a new heap tree node .........20......2000 000 000.2\n8.5 Deleting a heap treenode ... 2... 0.020.000.0000 000 000000.\n8.6 Building a new heap tree from scratch .. 2... 2... ee\n8.7 Merging binary heap trees... 2... ee\n8.8 Binomial heaps... 2... 0.000000... 0000000000002 2220,\n8.9 Fibonacci heaps... 2.2... 0.22 ee\n8.10 Comparison of heap time complexities ... 2... .......020...0000.\nSorting\n\n9.1 The problem of sorting... .......0.. 0.000000. 0000000000.\n9.2 Common sorting strategies... 0. ee\n9.3 How many comparisons must it take? 2... 2... en\n94 Bubble Sort... 2... ..0.0. 0000000000000 0000000000 0000.2\n9.5 Insertion Sort... 2... ee\n9.6 Selection Sort... 2... 2. en\n9.7 Comparison of O(n?) sorting algorithms ............0.00.0002005\n9.8 Sorting algorithm stability... 2... 2... 0202020000002. 0000.0.000.\n9.9 Treesort.. 2... 2. ee\n9.10 Heapsort.. 2... 2. 2 ee\n9.11 Divide and conquer algorithms ................2.00.0..0000.\n9.12 Quicksort 2... en\n9.13 Mergesort .. 2...\n9.14 Summary of comparison-based sorting algorithms... ...........00..\n\n55\n\n\n9.15 Non-comparison-based sorts .. 2... ee 81\n\n9.16 Bin, Bucket, Radix Sorts... 2... en 83\n10 Hash Tables 85\n0.1 Storing data... 2... en 85\n\n0.2 The Table abstract data type... 2... en 85\n\n0.3 Implementations of the table data structure... 2... 0.2000... .000. 87\n\n0.4 Hash Tables... 2. 2 ee 87\n\n0.5 Collision likelihoods and load factors for hash tables ...........20020. 88\n\n0.6 A simple Hash Table in operation... 1... 2.0... 0.0 000000000045 89\n\n0.7 Strategies for dealing with collisions ...........0..02......000. 90\n\n0.8 Linear Probing ... 2.2... ee 92\n\n0.9 Double Hashing... 2.2... 2. en 94\n0.10Choosing good hash functions... 2... 2. en 96\n0.11Complexity of hash tables... 2. en 96\n\n11 Graphs 98\n1.1 Graph terminology... 2... 0... 99\n\n1.2 Implementing graphs... 2... ee 100\n\n1.3 Relations between graphs ... 2... 0.200000. 00 0000000000000. 102\n\n1.4 Planarity 2... ee 103\n\n1.5 Traversals — systematically visiting all vertices... 2... 0.0... 2 104\n\n1.6 Shortest paths — Dijkstra’s algorithm... 2... 2.20.00... 2.0.0..0.0. 105\n\n1.7 Shortest paths — Floyd’s algorithm ...........0.0.....0.0..0.0. 111\n\n1.8 Minimal spanning trees... 2... ee 1138\n\n1.9 Travelling Salesmen and Vehicle Routing... ...........2....0.0. 117\n\n12 Epilogue 118\nA Some Useful Formulae 119\nA.l Binomial formulae 2... 2. ee 119\nA.2 Powers and roots... 2... 119\nA.3 Logarithms .. 2... ee 119\nAA Sums... 2. ee 120\nA.5 Fibonacci numbers... 2... 2 ee 121\n\n\nChapter 1\n\nIntroduction\n\nThese lecture notes cover the key ideas involved in designing algorithms. We shall see how\nthey depend on the design of suitable data structures, and how some structures and algorithms\nare more efficient than others for the same task. We will concentrate on a few basic tasks,\nsuch as storing, sorting and searching data, that underlie much of computer science, but the\ntechniques discussed will be applicable much more generally.\n\nWe will start by studying some key data structures, such as arrays, lists, queues, stacks\nand trees, and then move on to explore their use in a range of different searching and sorting\nalgorithms. This leads on to the consideration of approaches for more efficient storage of\ndata in hash tables. Finally, we will look at graph based representations and cover the kinds\nof algorithms needed to work efficiently with them. Throughout, we will investigate the\ncomputational efficiency of the algorithms we develop, and gain intuitions about the pros and\ncons of the various potential approaches for each task.\n\nWe will not restrict ourselves to implementing the various data structures and algorithms\nin particular computer programming languages (e.g., Java, C, OCaml), but specify them in\nsimple pseudocode that can easily be implemented in any appropriate language.\n\n1.1 Algorithms as opposed to programs\n\nAn algorithm for a particular task can be defined as “a finite sequence of instructions, each\nof which has a clear meaning and can be performed with a finite amount of effort in a finite\nlength of time”. As such, an algorithm must be precise enough to be understood by human\nbeings. However, in order to be executed by a computer, we will generally need a program that\nis written in a rigorous formal language; and since computers are quite inflexible compared\nto the human mind, programs usually need to contain more details than algorithms. Here we\nshall ignore most of those programming details and concentrate on the design of algorithms\nrather than programs.\n\nThe task of implementing the discussed algorithms as computer programs is important,\nof course, but these notes will concentrate on the theoretical aspects and leave the practical\nprogramming aspects to be studied elsewhere. Having said that, we will often find it useful\nto write down segments of actual programs in order to clarify and test certain theoretical\naspects of algorithms and their data structures. It is also worth bearing in mind the distinction\nbetween different programming paradigms: Imperative Programming describes computation in\nterms of instructions that change the program/data state, whereas Declarative Programming\n\n\n\nspecifies what the program should accomplish without describing how to do it. These notes\nwill primarily be concerned with developing algorithms that map easily onto the imperative\nprogramming approach.\n\nAlgorithms can obviously be described in plain English, and we will sometimes do that.\nHowever, for computer scientists it is usually easier and clearer to use something that comes\nsomewhere in between formatted English and computer program code, but is not runnable\nbecause certain details are omitted. This is called pseudocode, which comes in a variety of\nforms. Often these notes will present segments of pseudocode that are very similar to the\nlanguages we are mainly interested in, namely the overlap of C and Java, with the advantage\nthat they can easily be inserted into runnable programs.\n\n1.2. Fundamental questions about algorithms\n\nGiven an algorithm to solve a particular problem, we are naturally led to ask:\n1. What is it supposed to do?\n2. Does it really do what it is supposed to do?\n3. How efficiently does it do it?\nThe technical terms normally used for these three aspects are:\n1. Specification.\n2. Verification.\n3. Performance analysis.\n\nThe details of these three aspects will usually be rather problem dependent.\n\nThe specification should formalize the crucial details of the problem that the algorithm\nis intended to solve. Sometimes that will be based on a particular representation of the\nassociated data, and sometimes it will be presented more abstractly. Typically, it will have to\nspecify how the inputs and outputs of the algorithm are related, though there is no general\nrequirement that the specification is complete or non-ambiguous.\n\nFor simple problems, it is often easy to see that a particular algorithm will always work,\ni.e. that it satisfies its specification. However, for more complicated specifications and/or\nalgorithms, the fact that an algorithm satisfies its specification may not be obvious at all.\nIn this case, we need to spend some effort verifying whether the algorithm is indeed correct.\nIn general, testing on a few particular inputs can be enough to show that the algorithm is\nincorrect. However, since the number of different potential inputs for most algorithms is\ninfinite in theory, and huge in practice, more than just testing on particular cases is needec\nto be sure that the algorithm satisfies its specification. We need correctness proofs. Although\nwe will discuss proofs in these notes, and useful relevant ideas like invariants, we will usually\nonly do so in a rather informal manner (though, of course, we will attempt to be rigorous).\nThe reason is that we want to concentrate on the data structures and algorithms. Forma!\nverification techniques are complex and will normally be left till after the basic ideas of these\nnotes have been studied.\n\nFinally, the efficiency or performance of an algorithm relates to the resources requirec\nby it, such as how quickly it will run, or how much computer memory it will use. This wil\n\n\n\nusually depend on the problem instance size, the choice of data representation, and the details\nof the algorithm. Indeed, this is what normally drives the development of new data structures\nand algorithms. We shall study the general ideas concerning efficiency in Chapter 5, and then\napply them throughout the remainder of these notes.\n\n1.3. Data structures, abstract data types, design patterns\n\nFor many problems, the ability to formulate an efficient algorithm depends on being able to\norganize the data in an appropriate manner. The term data structure is used to denote a\nparticular way of organizing data for particular types of operation. These notes will look at\nnumerous data structures ranging from familiar arrays and lists to more complex structures\nsuch as trees, heaps and graphs, and we will see how their choice affects the efficiency of the\nalgorithms based upon them.\n\nOften we want to talk about data structures without having to worry about all the im-\nplementational details associated with particular programming languages, or how the data is\nstored in computer memory. We can do this by formulating abstract mathematical models\nof particular classes of data structures or data types which have common features. These are\ncalled abstract data types, and are defined only by the operations that may be performed on\nthem. Typically, we specify how they are built out of more primitive data types (e.g., integers\nor strings), how to extract that data from them, and some basic checks to control the flow of\nprocessing in algorithms. The idea that the implementational details are hidden from the user\nand protected from outside access is known as encapsulation. We shall see many examples of\nabstract data types throughout these notes.\n\nAt an even higher level of abstraction are design patterns which describe the design of\nalgorithms, rather the design of data structures. These embody and generalize importan\ndesign concepts that appear repeatedly in many problem contexts. They provide a genera.\nstructure for algorithms, leaving the details to be added as required for particular problems.\nThese can speed up the development of algorithms by providing familiar proven algorithm\nstructures that can be applied straightforwardly to new problems. We shall see a number of\nfamiliar design patterns throughout these notes.\n\n1.4 Textbooks and web-resources\n\nTo fully understand data structures and algorithms you will almost certainly need to comple-\nment the introductory material in these notes with textbooks or other sources of information.\nThe lectures associated with these notes are designed to help you understand them and fill in\nsome of the gaps they contain, but that is unlikely to be enough because often you will need\nto see more than one explanation of something before it can be fully understood.\n\nThere is no single best textbook that will suit everyone. The subject of these notes is a\nclassical topic, so there is no need to use a textbook published recently. Books published 10\nor 20 years ago are still good, and new good books continue to be published every year. The\nreason is that these notes cover important fundamental material that is taught in all university\ndegrees in computer science. These days there is also a lot of very useful information to be\nfound on the internet, including complete freely-downloadable books. It is a good idea to go\nto your library and browse the shelves of books on data structures and algorithms. If you like\nany of them, download, borrow or buy a copy for yourself, but make sure that most of the\n\n\n\ntopics in the above contents list are covered. Wikipedia is generally a good source of fairly\nreliable information on all the relevant topics, but you hopefully shouldn’t need reminding\nthat not everything you read on the internet is necessarily true. It is also worth pointing\nout that there are often many different equally-good ways to solve the same task, different\nequally-sensible names used for the same thing, and different equally-valid conventions used\nby different people, so don’t expect all the sources of information you find to be an exact\nmatch with each other or with what you find in these notes.\n\n1.5 Overview\n\nThese notes will cover the principal fundamental data structures and algorithms used in\ncomputer science, and bring together a broad range of topics covered elsewhere into a coherent\nframework. Data structures will be formulated to represent various types of information in\nsuch a way that it can be conveniently and efficiently manipulated by the algorithms we\ndevelop. Throughout, the recurring practical issues of algorithm specification, verification\nand performance analysis will be discussed.\n\nWe shall begin by looking at some widely used basic data structures (namely arrays,\nlinked lists, stacks and queues), and the advantages and disadvantages of the associated\nabstract data types. Then we consider the ubiquitous problem of searching, and how that\nleads on to the general ideas of computational efficiency and complexity. That will leave\nus with the necessary tools to study three particularly important data structures: trees (in\nparticular, binary search trees and heap trees), hash tables, and graphs. We shall learn how to\ndevelop and analyse increasingly efficient algorithms for manipulating and performing useful\noperations on those structures, and look in detail at developing efficient processes for data\nstoring, sorting, searching and analysis. The idea is that once the basic ideas and examples\ncovered in these notes are understood, dealing with more complex problems in the future\nshould be straightforward.\n\n\n\nChapter 2\n\nArrays, Iteration, Invariants\n\nData is ultimately stored in computers as patterns of bits, though these days most program-\nming languages deal with higher level objects, such as characters, integers, and floating point\nnumbers. Generally, we need to build algorithms that manipulate collections of such objects,\nso we need procedures for storing and sequentially processing them.\n\n2.1 Arrays\n\nIn computer science, the obvious way to store an ordered collection of items is as an array.\nArray items are typically stored in a sequence of computer memory locations, but to discuss\nthem, we need a convenient way to write them down on paper. We can just write the items\nin order, separated by commas and enclosed by square brackets. Thus,\n\n[1, 4, 17, 3, 90, 79, 4, 6, 81]\nis an example of an array of integers. If we call this array a, we can write it as:\na = [1,4,17,3, 90, 79, 4,6, 81]\n\nThis array a has 9 items, and hence we say that its size is 9. In everyday life, we usually start\ncounting from 1. When we work with arrays in computer science, however, we more often\n(though not always) start from 0. Thus, for our array a, its positions are 0,1,2,...,7,8. The\nelement in the 8' position is 81, and we use the notation a[8] to denote this element. More\ngenerally, for any integer i denoting a position, we write a[i] to denote the element in the jth\nposition. This position i is called an index (and the plural is indices). Then, in the above\nexample, a[0] = 1, a[1] = 4, a[2] = 17, and so on.\n\nIt is worth noting at this point that the symbol = is quite overloaded. In mathematics,\nit stands for equality. In most modern programming languages, = denotes assignment, while\nequality is expressed by ==. We will typically use = in its mathematical meaning, unless it\nis written as part of code or pseudocode.\n\nWe say that the individual items a[#] in the array a are accessed using their index i, and\none can move sequentially through the array by incrementing or decrementing that index,\nor jump straight to a particular item given its index value. Algorithms that process data\nstored as arrays will typically need to visit systematically all the items in the array, and apply\nappropriate operations on them.\n\n\n\n2.2 Loops and Iteration\n\nThe standard approach in most programming languages for repeating a process a certain\nnumber of times, such as moving sequentially through an array to perform the same operations\non each item, involves a loop. In pseudocode, this would typically take the general form\n\nFor i= 1,...,N,\ndo something\n\nand in programming languages like C and Java this would be written as the for-loop\n\nfor(i=0;i<N ; it+) {\n// do something\n}\n\nin which a counter i keep tracks of doing “the something” N times. For example, we could\ncompute the sum of all 20 items in an array a using\n\nfor( i=0, sum=0; i < 20; i++) {\nsum += a[i];\n\n}\n\nWe say that there is iteration over the index 7. The general for-loop structure is\n\nfor( INITIALIZATION ; CONDITION ; UPDATE ) {\nREPEATED PROCESS\n}\n\nin which any of the four parts are optional. One way to write this out explicitly is\n\nINITIALIZATION\nif ( not CONDITION ) go to LOOP FINISHED\nLOOP START\n\nREPEATED PROCESS\n\nUPDATE\n\nif ( CONDITION ) go to LOOP START\nLOOP FINISHED\n\nIn these notes, we will regularly make use of this basic loop structure when operating on data\nstored in arrays, but it is important to remember that different programming languages use\ndifferent syntax, and there are numerous variations that check the condition to terminate the\nrepetition at different points.\n\n2.3 Invariants\n\nAn invariant, as the name suggests, is a condition that does not change during execution of\na given program or algorithm. It may be a simple inequality, such as “i < 20”, or something\nmore abstract, such as “the items in the array are sorted”. Invariants are important for data\nstructures and algorithms because they enable correctness proofs and verification.\n\nIn particular, a loop-invariant is a condition that is true at the beginning and end of every\niteration of the given loop. Consider the standard simple example of a procedure that finds\nthe minimum of n numbers stored in an array a:\n\n10\n\n\nminimum(int n, float a[n]) {\nfloat min = a[0];\n\n// min equals the minimum item in a[0],...,a[0]\nfor(int i=1; i!=n; it+) {\n// min equals the minimum item in a[0],...,a[i-1]\nif (ali] < min) min = a[il;\n}\n// min equals the minimum item in a[0],...,a[i-1], and i==\nreturn min;\n}\nAt the beginning of each iteration, and end of any iterations before, the invariant “min equals\nthe minimum item in a(0},...,a[i — 1]” is true — it starts off true, and the repeated process\nand update clearly maintain its truth. Hence, when the loop terminates with “i == n”, we\n\nknow that “min equals the minimum item in a[0],...,a[n — 1]” and hence we can be sure that\nmin can be returned as the required minimum value. This is a kind of proof by induction:\nthe invariant is true at the start of the loop, and is preserved by each iteration of the loop,\ntherefore it must be true at the end of the loop.\n\nAs we noted earlier, formal proofs of correctness are beyond the scope of these notes, but\nidentifying suitable loop invariants and their implications for algorithm correctness as we go\nalong will certainly be a useful exercise. We will also see how invariants (sometimes called\ninductive assertions) can be used to formulate similar correctness proofs concerning properties\nof data structures that are defined inductively.\n\n11\n\n\nChapter 3\n\nLists, Recursion, Stacks, Queues\n\nWe have seen how arrays are a convenient way to store collections of items, and how loops\nand iteration allow us to sequentially process those items. However, arrays are not always the\nmost efficient way to store collections of items. In this section, we shall see that lists may be\na better way to store collections of items, and how recursion may be used to process them.\nAs we explore the details of storing collections as lists, the advantages and disadvantages of\ndoing so for different situations will become apparent.\n\n3.1 Linked Lists\n\nA list can involve virtually anything, for example, a list of integers [3, 2,4, 2,5], a shopping\nlist [apples, butter, bread, cheese], or a list of web pages each containing a picture and a\nlink to the next web page. When considering lists, we can speak about-them on different\nlevels - on a very abstract level (on which we can define what we mean by a list), on a level\non which we can depict lists and communicate as humans about them, on a level on which\ncomputers can communicate, or on a machine level in which they can be implemented.\n\nGraphical Representation\n\nNon-empty lists can be represented by two-cells, in each of which the first cell contains a\npointer to a list element and the second cell contains a pointer to either the empty list or\nanother two-cell. We can depict a pointer to the empty list by a diagonal bar or cross through\nthe cell. For instance, the list [3, 1,4, 2,5] can be represented as:\n\npep pb\n\nAbstract Data Type “List”\n\nOn an abstract level , a list can be constructed by the two constructors:\n\ne EmptyList, which gives you the empty list, and\n\n12\n\n\ne MakeList(element, list), which puts an element at the top of an existing list.\nUsing those, our last example list can be constructed as\nMakeList(3, MakeList(1,MakeList(4,MakeList(2,MakeList(5, EmptyList))))).\n\nand it is clearly possible to construct any list in this way.\n\nThis inductive approach to data structure creation is very powerful, and we shall use\nit many times throughout these notes. It starts with the “base case”, the EmptyList, and\nthen builds up increasingly complex lists by repeatedly applying the “induction step”, the\nMakeList(element, list) operator.\n\nIt is obviously also important to be able to get back the elements of a list, and we no\nlonger have an item index to use like we have with an array. The way to proceed is to note\nthat a list is always constructed from the first element and the rest of the list. So, conversely,\nfrom a non-empty list it must always be possible to get the first element and the rest. This\ncan be done using the two selectors, also called accessor methods:\n\ne first(list), and\ne rest(list).\n\nThe selectors will only work for non-empty lists (and give an error or exception on the empty\nlist), so we need a condition which tells us whether a given list is empty:\ne isEmpty(list)\n\nThis will need to be used to check every list before passing it to a selector.\n\nWe call everything a list that can be constructed by the constructors EmptyList and\nMakeList, so that with the selectors first and rest and the condition isEmpty, the following\nrelationships are automatically satisfied (i.e. true):\n\ne isEmpty(EmptyList)\n\ne not isEmpty(MakeList(x,1)) (for any x and 1)\ne first(MakeList(x,1)) =x\ne rest(MakeList(x,1)) =1\n\nIn addition to constructing and getting back the components of lists, one may also wish to\ndestructively change lists. This would be done by so-called mutators which change either the\nfirst element or the rest of a non-empty list:\n\ne replaceFirst(x, 1)\ne replaceRest(r, 1)\n\nFor instance, with 1 = [3,1,4,2,5], applying replaceFirst(9,1) changes 1 to [9,1,4, 2,5].\nand then applying replaceRest((6, 2,3, 4],1) changes it to [9, 6, 2, 3, 4].\n\nWe shall see that the concepts of constructors, selectors and conditions are common to\nvirtually all abstract data types. Throughout these notes, we will be formulating our data\nrepresentations and algorithms in terms of appropriate definitions of them.\n\n13\n\n\nXML Representation\n\nIn order to communicate data structures between different computers and possibly different\nprogramming languages, XML (eXtensible Markup Language) has become a quasi-standard.\nThe above list could be represented in XML as:\n\n<ol>\n<1i>3</1i>\n<li>i</1li>\n<1i>4</1i>\n<1i>2</1li>\n<1i>5</1i>\n</ol>\n\nHowever, there are usually many different ways to represent the same object in XML. For\ninstance, a cell-oriented representation of the above list would be:\n\n<cell>\n<first>3</first>\n<rest>\n<cell>\n<first>1</first>\n<rest>\n<cell>\n<first>4</first>\n<rest>\n<cell>\n<first>2</first>\n<rest>\n<first>5</first>\n<rest>EmptyList</rest>\n</rest>\n</cell>\n</rest>\n</cell>\n</rest>\n</cell>\n</rest>\n</cell>\n\nWhile this looks complicated for a simple list, it is not, it is just a bit lengthy. XML is flexible\nenough to represent and communicate very complicated structures in a uniform way.\n\nImplementation of Lists\n\nThere are many different implementations possible for lists, and which one is best will depend\non the primitives offered by the programming language being used.\n\nThe programming language Lisp and its derivates, for instance, take lists as the most\nimportant primitive data structure. In some other languages, it is more natural to implement\n\n14\n\n\nlists as arrays. However, that can be problematic because lists are conceptually not limited in\nsize, which means array based implementation with fixed-sized arrays can only approximate\nthe general concept. For many applications, this is not a problem because a maximal number\nof list members can be determined a priori (e.g., the maximum number of students taking one\nparticular module is limited by the total number of students in the University). More general\npurpose implementations follow a pointer based approach, which is close to the diagrammatic\nrepresentation given above. We will not go into the details of all the possible implementations\nof lists here, but such information is readily available in the standard textbooks.\n\n3.2 Recursion\n\nWe previously saw how iteration based on for-loops was a natural way to process collections of\nitems stored in arrays. When items are stored as linked-lists, there is no index for each item,\nand recursion provides the natural way to process them. The idea is to formulate procedures\nwhich involve at least one step that invokes (or calls) the procedure itself. We will now look\nat how to implement two important derived procedures on lists, last and append, which\nillustrate how recursion works.\n\nTo find the last element of a list 1 we can simply keep removing the first remaining item\ntill there are no more left. This algorithm can be written in pseudocode as:\n\nlast(1) {\nif ( isEmpty(1) )\nerror(‘Error: empty list in last’)\nelseif ( isEmpty(rest(1)) )\nreturn first (1)\nelse\nreturn last (rest (1))\n}\n\nThe running time of this depends on the length of the list, and is proportional to that length,\nsince last is called as often as there are elements in the list. We say that the procedure\nhas linear time complexity, that is, if the length of the list is increased by some factor, the\nexecution time is increased by the same factor. Compared to the constant time complexity\nwhich access to the last element of an array has, this is quite bad. It does not mean, however,\nthat lists are inferior to arrays in general, it just means that lists are not the ideal data\nstructure when a program has to access the last element of a long list very often.\n\nAnother useful procedure allows us to append one list 12 to another list 11. Again, this\nneeds to be done one item at a time, and that can be accomplished by repeatedly taking the\nfirst remaining item of 11 and adding it to the front of the remainder appended to 12:\n\nappend(11,12) {\nif ( isEmpty(11) )\nreturn 12\nelse\nreturn MakeList (first (11) ,append(rest (11) ,12))\n}\n\nThe time complexity of this procedure is proportional to the length of the first list, 11, since\nwe have to call append as often as there are elements in 11.\n\n15\n\n\n3.3 Stacks\n\nStacks are, on an abstract level, equivalent to linked lists. They are the ideal data structure\nto model a First-In-Last-Out (FILO), or Last-In-First-Out (LIFO), strategy in search.\n\nGraphical Representation\n\nTheir relation to linked lists means that their graphical representation can be the same, but\none has to be careful about the order of the items. For instance, the stack created by inserting\nthe numbers [3, 1,4, 2,5] in that order would be represented as:\n\nWidth\n\n5 2 4 1 3\n\nAbstract Data Type “Stack”\n\nDespite their relation to linked lists, their different use means the primitive operators for\nstacks are usually given different names. The two constructors are:\n\ne EmptyStack, the empty stack, and\n\ne push(element, stack), which takes an element and pushes it on top of an existing stack,\nand the two selectors are:\n\ne top(stack), which gives back the top most element of a stack, and\n\ne pop(stack), which gives back the stack without the top most element.\n\nThe selectors will work only for non-empty stacks, hence we need a condition which tells\nwhether a stack is empty:\n\ne isEmpty(stack)\n\nWe have equivalent automatically-true relationships to those we had for the lists:\n\nisEmpty(EmptyStack)\n\nnot isEmpty(push(x,s)) (for any x and s)\n\ntop(push(x,s)) =x\n\npop(push(x,s)) =s\n\nIn summary, we have the direct correspondences:\n\n| constructors | selectors | condition\nList EmptyList | MakeList first | rest isEmpty\nStack | EmptyStack ' push top | pop isEmpty\n\nSo, stacks and linked lists are the same thing, apart from the different names that are used\nfor their constructors and selectors.\n\n16\n\n\nImplementation of Stacks\n\nThere are two different ways we can think about implementing stacks. So far we have implied\na functional approach. That is, push does not change the original stack, but creates a new\nstack out of the original stack and a new element. That is, there are at least two stacks\naround, the original one and the newly created one. This functional view is quite convenient.\nIf we apply top to a particular stack, we will always get the same element. However, from a\npractical point of view, we may not want to create lots of new stacks in a program, because of\nthe obvious memory management implications. Instead it might be better to think of a single\nstack which is destructively changed, so that after applying push the original stack no longer\nexits, but has been changed into a new stack with an extra element. This is conceptually\nmore difficult, since now applying top to a given stack may give different answers, depending\non how the state of the system has changed. However, as long as we keep this difference in\nmind, ignoring such implementational details should not cause any problems.\n\n3.4 Queues\n\nA queue is a data structure used to model a First-In-First-Out (FIFO) strategy. Conceptually,\nwe add to the end of a queue and take away elements from its front.\n\nGraphical Representation\n\nA queue can be graphically represented in a similar way to a list or stack, but with an\nadditional two-cell in which the first element points to the front of the list of all the elements\nin the queue, and the second element points to the last element of the list. For instance, if\nwe insert the elements [3, 1,4, 2] into an initially empty queue, we get:\n\nne\nne, nena\n\n3 1\n\nThis arrangement means that taking the first element of the queue, or adding an element to\nthe back of the queue, can both be done efficiently. In particular, they can both be done with\nconstant effort, i.e. independently of the queue length.\n\nAbstract Data Type “Queue”\n\nOn an abstract level, a queue can be constructed by the two constructors:\n\ne EmptyQueue, the empty queue, and\n\ne push(element, queue), which takes an element and a queue and returns a queue in which\nthe element is added to the original queue at the end.\n\nFor instance, by applying push(5,q) where q is the queue above, we get\n\n17\n\n\nThe two selectors are the same as for stacks:\ne top(queue), which gives the top element of a queue, that is, 3 in the example, and\n\ne pop(queue), which gives the queue without the top element.\n\nAnd, as with stacks, the selectors only work for non-empty queues, so we again need a condi-\ntion which returns whether a queue is empty:\n\ne isEmpty(queue)\n\nIn later chapters we shall see practical examples of how queues and stacks operate with\ndifferent effect.\n\n3.5 Doubly Linked Lists\n\nA doubly linked list might be useful when working with something like a list of web pages,\nwhich has each page containing a picture, a link to the previous page, and a link to the next\npage. For a simple list of numbers, a linked list and a doubly linked list may look the same,\ne.g., [3,1,4,2,5]. However, the doubly linked list also has an easy way to get the previous\nelement, as well as to the next element.\n\nGraphical Representation\n\nNon-empty doubly linked lists can be represented by three-cells, where the first cell contains a\npointer to another three-cell or to the empty list, the second cell contains a pointer to the list\nelement and the third cell contains a pointer to another three-cell or the empty list. Again,\nwe depict the empty list by a diagonal bar or cross through the appropriate cell. For instance,\n[3, 1,4, 2,5] would be represented as doubly linked list as:\n\nAbstract Data Type “Doubly Linked List”\n\nOn an abstract level , a doubly linked list can be constructed by the three constructors:\n\ne EmptyList, the empty list, and\n\n18\n\n\ne MakeListLeft(element, list), which takes an element and a doubly linked list and\nreturns a new doubly linked list with the element added to the left of the original\ndoubly linked list.\n\ne MakeListRight(element, list), which takes an element and a doubly linked list and\nreturns a new doubly linked list with the element added to the right of the original\ndoubly linked list.\n\nIt is clear that it may possible to construct a given doubly linked list in more that one way.\nFor example, the doubly linked list represented above can be constructed by either of:\n\nMakeListLeft(3,MakeListLeft(1,MakeListLeft(4, MakeListLeft(2,\nMakeListLeft(5, EmptyList)))))\n\nMakeListLeft(3,MakeListLeft(1,MakeListRight(5,MakeListRight(2,\nMakeListLeft(4, EmptyList)))))\n\nIn the case of doubly linked lists, we have four selectors:\ne firstLeft(list),\ne restLeft(list),\ne firstRight(list), and\n\ne restRight(list).\n\nThen, since the selectors only work for non-empty lists, we also need a condition which returns\nwhether a list is empty:\n\ne isEmpty(list)\nThis leads to automatically-true relationships such as:\ne isEmpty(EmptyList)\ne not isEmpty(MakeListLeft(x,1)) (for any x and 1)\ne not isEmpty(MakeListRight(x,1)) (for any x and 1)\ne firstLeft(MakeListLeft(x,1)) =x\ne restLeft(MakeListLeft(x,1)) =1\ne firstRight(MakeListRight(x,1)) =x\n\ne restRight (MakeListRight(x,1)) =1\n\nCircular Doubly Linked List\n\nAs a simple extension of the standard doubly linked list, one can define a circular doubly\nlinked list in which the left-most element points to the right-most element, and vice versa.\nThis is useful when we might need to move efficiently through a whole list of items, but might\nnot be starting from one of two particular end points.\n\n19\n\n\n3.6 Advantage of Abstract Data Types\n\nIt is clear that the implementation of the abstract linked-list data type has the disadvantage\nthat certain useful procedures may not be directly accessible. For instance, the standard\nabstract data type of a list does not offer an efficient procedure last(1) to give the last element\nin the list, whereas it would be trivial to find the last element of an array of a known number\nof elements. One could modify the linked-list data type by maintaining a pointer to the last\nitem, as we did for the queue data type, but we still wouldn’t have an easy way to access\nintermediate items. While last(1) and getItem(i,1) procedures can easily be implemented\nusing the primitive constructors, selectors, and conditions, they are likely to be less efficient\nthan making use of certain aspects of the underlying implementation.\n\nThat disadvantage leads to an obvious question: Why should we want to use abstract data\ntypes when they often lead to less efficient algorithms? Aho, Hopcroft and Ullman (1983)\nprovide a clear answer in their book:\n\n“At first, it may seem tedious writing procedures to govern all accesses to the\nunderlying structures. However, if we discipline ourselves to writing programs in\nterms of the operations for manipulating abstract data types rather than mak-\ning use of particular implementations details, then we can modify programs more\nreadily by reimplementing the operations rather than searching all programs for\nplaces where we have made accesses to the underlying data structures. This flexi-\nbility can be particularly important in large software efforts, and the reader should\nnot judge the concept by the necessarily tiny examples found in this book.”\n\nThis advantage will become clearer when we study more complex abstract data types and\nalgorithms in later chapters.\n\n20\n\n\nChapter 4\n\nSearching\n\nAn important and recurring problem in computing is that of locating information. More\nsuccinctly, this problem is known as searching. This is a good topic to use for a preliminary\nexploration of the various issues involved in algorithm design.\n\n4.1 Requirements for searching\n\nClearly, the information to be searched has to first be represented (or encoded) somehow.\nThis is where data structures come in. Of course, in a computer, everything is ultimately\nrepresented as sequences of binary digits (bits), but this is too low level for most purposes.\nWe need to develop and study useful data structures that are closer to the way humans think,\nor at least more structured than mere sequences of bits. This is because it is humans who\nhave to develop and maintain the software systems — computers merely run them.\n\nAfter we have chosen a suitable representation, the represented information has to be\nprocessed somehow. This is what leads to the need for algorithms. In this case, the process\nof interest is that of searching. In order to simplify matters, let us assume that we want\nto search a collection of integer numbers (though we could equally well deal with strings of\ncharacters, or any other data type of interest). To begin with, let us consider:\n\n1. The most obvious and simple representation.\n2. Two potential algorithms for processing with that representation.\n\nAs we have already noted, arrays are one of the simplest possible ways of representing col-\nlections of numbers (or strings, or whatever), so we shall use that to store the information to\nbe searched. Later we shall look at more complex data structures that may make storing and\nsearching more efficient.\n\nSuppose, for example, that the set of integers we wish to search is {1,4,17,3,90,79,4,6,81}.\nWe can write them in an array a as\n\na =[1,4,17,3, 90, 79, 4,6, 81]\n\nIf we ask where 17 is in this array, the answer is 2, the index of that element. If we ask where 91\nis, the answer is nowhere. It is useful to be able to represent nowhere by a number that is\nnot used as a possible index. Since we start our index counting from 0, any negative number\nwould do. We shall follow the convention of using the number —1 to represent nowhere. Other\n(perhaps better) conventions are possible, but we will stick to this here.\n\n21\n\n\n4.2 Specification of the search problem\n\nWe can now formulate a specification of our search problem using that data structure:\nGiven an array a and integer x, find an integer i such that\n\n1. if there is no j such that alj] is x, then i is —1,\n\n2. otherwise, i is any j for which alj] is x.\n\nThe first clause says that if 2 does not occur in the array a then i should be —1, and the second\nsays that if it does occur then i should be a position where it occurs. If there is more than one\nposition where x occurs, then this specification allows you to return any of them — for example,\nthis would be the case if a were [17, 13, 17] and x were 17. Thus, the specification is ambiguous.\nHence different algorithms with different behaviours can satisfy the same specification — for\nexample, one algorithm may return the smallest position at which x occurs, and another may\nreturn the largest. There is nothing wrong with ambiguous specifications. In fact, in practice,\nthey occur quite often.\n\n4.3. A simple algorithm: Linear Search\n\nWe can conveniently express the simplest possible algorithm in a form of pseudocode which\nreads like English, but resembles a computer program without some of the precision or detail\nthat a computer usually requires:\n\n// This assumes we are given an array a of size n and a key x.\nFor i= 0,1,...,n-1,\nif ali] is equal to x,\nthen we have a suitable i and can terminate returning i.\nIf we reach this point,\nthen x is not in a and hence we must terminate returning -1.\n\n“\n\nSome aspects, such as the ellipsis “...”, are potentially ambiguous, but we, as human beings,\nknow exactly what is meant, so we do not need to worry about them. In a programming\nlanguage such as C or Java, one would write something that is more precise like:\n\nfor (i=0O0;i<n; it+) {\nif ( afi] == x ) return i;\n}\n\nreturn -1;\n\nIn the case of Java, this would be within a method of a class, and more details are needed,\nsuch as the parameter a for the method and a declaration of the auxiliary variable i. In the\ncase of C’, this would be within a function, and similar missing details are needed. In either,\nthere would need to be additional code to output the result in a suitable format.\n\nIn this case, it is easy to see that the algorithm satisfies the specification (assuming n is\nthe correct size of the array) — we just have to observe that, because we start counting from\nzero, the last position of the array is its size minus one. If we forget this, and let ¢ run from\n0 to n instead, we get an incorrect algorithm. The practical effect of this mistake is that the\nexecution of this algorithm gives rise to an error when the item to be located in the array is\n\n22\n\n\nactually not there, because a non-existing location is attempted to be accessed. Depending\n\non the particular language, operating system and\nthis error will be different. For example, in C ru\n\nmachine you are using, the actual effect of\nnning under Unix, you may get execution\n\naborted followed by the message “segmentation fault”, or you may be given the wrong answer\n\nas the output. In Java, you will always get an error message.\n\n4.4 A more efficient algorithm: Binary Search\n\nOne always neec\nparticular algori\n\ns to consider whether it is possi\nhm, such as the one we have jus\n\narray of size n takes n steps. On average, it will take n/2 steps. For large co\n\nsuch as all web-pages on the internet, this will be\ntry to organize the collection in such a way that a\nshall see later, there are many possibilities, and t\nthe more complicated the data structures represer\nwe shall consider one of the simplest — we still repr\nenumerate the elements in ascending order. The\n\nsle to improve upon the performance of a\ncreated. In the worst case, searching an\nlections of data,\nunacceptable in practice. Thus, we should\nmore efficient algorithm is possible. As we\n1e more we demand in terms of efficiency,\nating the collections tend to become. Here\nesent the collections by arrays, but now we\n\nproblem of obtaining an ordered list from\n\nany given list is known as sorting and will be stuc\n\nThus, insteac\nwith [1,3,4, 4,6, 17,79, 81,90], which has the same\nwe can use an improved algorithm, which in Engli:\n\nof working with the previous array [1, 4, 17,3, 90, 79, 4, 6, 81\n\nied in detail in a later chapter.\n\n; we would work\ning order. Then\nsh-like pseudocode form is:\n\nitems but listed in ascenc\n\n// This assumes we are given a sorted array a of size n and a key x.\n\n// Use integers left and right (initial\nWhile left is less than right,\nset mid to the integer part of (lef\nif x is greater than a[mid],\nthen set left to mid+i,\notherwise set right to mid.\nIf a[left] is equal to x,\nthen\n\notherwise terminate returning -1.\n\nly set to 0 and n-1) and mid.\n\nttright)/2, and\n\nterminate returning left,\n\nand would correspond to a segment of C' or Java code like:\n\n/* DATA\nint a=\nint n = 9;\nint x 79;\n/* PROGRAM */\nint left = 0, right n-1, mid;\nwhile ( left < right ) {\nmid = ( left + right ) / 2;\nif ( x > a[mid] ) left midti;\nelse right = mid;\n\n*/\n[1,3,4,4,6,17,79,81,90];\n\n}\nif ( afleft] == x ) return left;\nelse return -1;\n\n23\n\n\nThis algorithm works by repeatedly splitting the array into two segments, one going from left\nto mid, and the other going from mid + 1 to right, where mid is the position half way from\nleft to right, and where, initially, left and right are the leftmost and rightmost positions of\nthe array. Because the array is sorted, it is easy to see which of each pair of segments the\nsearched-for item x is in, and the search can then be restricted to that segment. Moreover,\nbecause the size of the sub-array going from locations left to right is halved at each iteration\nof the while-loop, we only need logg n steps in either the average or worst case. To see that this\nruntime behaviour is a big improvement, in practice, over the earlier linear-search algorithm,\nnotice that logz 1000000 is approximately 20, so that for an array of size 1000000 only 20\niterations are needed in the worst case of the binary-search algorithm, whereas 1000000 are\nneeded in the worst case of the linear-search algorithm.\n\nWith the binary search algorithm, it is not so obvious that we have taken proper care\nof the boundary condition in the while loop. Also, strictly speaking, this algorithm is not\n, but that can easily\nis, and then try to\neague. Having done that, try to write down\nsome convincing arguments, maybe one that involves a loop invariant and one that doesn’t.\nMost algorithm developers stop at the first stage, but experience shows that it is only when\ny find all the subtle\ngorithm after it has\n\ncorrect because it does not work for the empty array (that has size zero)\noe fixed. Apart from that, is it correct? Try to convince yourself that i\nexplain your argument-for-correctness to a col\n\nwe attempt to write down seemingly convincing arguments that we actua\np with a better/clearer a\n\noeen modified to make its correctness easier to argue.\n\nmistakes. Moreover, it is not unusual to end w\nIt is worth considering whether linked-list versions of our two algori\noffer any advantages. It is fairly clear that we could perform a\nist in essentially the same way as with an array, with the relevant pointer returned rather\nhan an index. Converting the binary search to linked list form\nis no efficient way to split a linked list into two segments. I\napproach is the best we can do with the c\nex data s\n\nams would work, or\ninear search through a linkec\n\nis problematic, because there\nzat our array-basec\nso far. However, we\no formulate efficien\n\nseems\nata structures we have studiec\nshall see later how more comp. ructures (trees) can be used\n\nrecursive search algorithms.\n\ntaken into account how much effort wi\n\nNotice that we have not ye\n\nthe array so that the binary search algori\nbe sure that using the binary search algo:\nlinear search algorithm on the original unsorted array.\ndetails, such as how many times we need to performa a search\n\nonce, or as many as n times. We shall re\n\nhm can work on it. U\nrithm really is more e\nThat\n\nbe required to sor\nntil we know that, we canno\nficient overall than using the\nmay also depend on further\non the set of n items — jus\n\nurn to these issues la’\n\ner. First we need to consider\n\nin more detail how to compare algorithm efficiency in a reliable manner.\n\n24\n\n\nChapter 5\n\nEfficiency and Complexity\n\nWe have already noted that, when developing algorithms, it is important to consider how\nefficient they are, so we can make informed choices about which are best to use in particular\ncircumstances. So, before moving on to study increasingly complex data structures and\nalgorithms, we first look in more detail at how to measure and describe their efficiency.\n\n5.1 Time versus space complexity\n\nWhen creating software for serious applications, there is usually a need to judge how quickly\nan algorithm or program can complete the given tasks. For example, if you are programming\na flight booking system, it will not be considered acceptable if the travel agent and customer\nhave to wait for half an hour for a transaction to complete. It certainly has to be ensured\nthat the waiting time is reasonable for the size of the problem, and normally faster execution\nis better. We talk about the time complexity of the algorithm as an indicator of how the\nexecution time depends on the size of the data structure.\nAnother important efficiency consideration is how much memory a given program will\nrequire for a particular task, though with modern computers this tends to be less of an issue\nthan it used to be. Here we talk about the space complexity as how the memory requirement\ndepends on the size of the data structure.\n\nFor a given task, there are often algorithms which trade time for space, and vice versa.\nFor example, we will see that, as a data storage device, hash tables have a very good time\n\ncomplexity at the expense of using more memory than is needed by other algorithms. It is\nusually up to the algorithm/program designer to decide how best to balance the trade-off for\nthe application they are designing.\n\n5.2 Worst versus average complexity\n\nAnother thing that has to be decided when making efficiency considerations is whether it is\nthe average case performance of an algorithm/program that is important, or whether it is\nmore important to guarantee that even in the worst case the performance obeys certain rules.\nFor many applications, the average case is more important, because saving time overall is\nusually more important than guaranteeing good behaviour in the worst case. However, for\ntime-critical problems, such as keeping track of aeroplanes in certain sectors of air space, it\nmay be totally unacceptable for the software to take too long if the worst case arises.\n\n25\n\n\nAgain, algorithms/programs often trade-off efficiency of the average case against efficiency\nof the worst case. For example, the most efficient algorithm on average might have a par-\nticularly bad worst case efficiency. We will see particular examples of this when we consider\nefficient algorithms for sorting and searching.\n\n5.3 Concrete measures for performance\n\nThese days, we are mostly interested in time complexity. For this, we first have to decide how\no measure it. Something one might try to do is to just implement the algorithm and run it,\nand see how long it takes to run, but that approach has a number of problems. For one, if\nit is a big application and there are several potential algorithms, they would all have to be\nprogrammed first before they can be compared. So a considerable amount of time would be\nwasted on writing programs which will not get used in the final product. Also, the machine on\nwhich the program is run, or even the compiler used, might influence the running time. You\nwould also have to make sure that the data with which you tested your program is typical for\nhe application it is created for. Again, particularly with big applications, this is not really\neasible. This empirical method has another disadvantage: it will not tell you anything useful\nabout the next time you are considering a similar problem.\n\nTherefore complexity is usually best measured in a different way. First, in order to not be\nsound to a particular programming language or machine architecture, i\nhe efficiency of the algorithm rather than that of its implementation. For this to be possible,\nhowever, the algorithm has to be described in a way which very much looks like the program to\noe implemented, which is why algorithms are usually best expressed in a form of pseudocode\nhat comes close to the implementation language.\n\nWhat we need to do to determine the time complexity of an algorithm is count the number\nof times each operation will occur, which will usually depend on the size of the problem. The\nsize of a problem is typically expressed as an integer, and that is typically the number of items\nhat are manipulated. For example, when describing a search algorithm, it is the number of\nitems amongst which we are searching, and when describing a sorting algorithm, it is the\nnumber of items to be sorted. So the complexity of an algorithm will be given by a function\nwhich maps the number of items to the (usually approximate) number of time steps the\nalgorithm will take when performed on that many items.\n\nIn the early days of computers, the various operations were each counted in proportion to\ntheir particular ‘time cost’, and added up, with multiplication of integers typically considered\nmuch more expensive than their addition. In today’s world, where computers have become\nmuch faster, and often have dedicated floating-point hardware, the differences in time costs\nhave become less important. However, we still we need to be careful when deciding to consider\nall operations as being equally costly — applying some function, for example, can take much\nlonger than simply adding two numbers, and swaps generally take many times longer than\ncomparisons. Just counting the most costly operations is often a good strategy.\n\nis better to measure\n\n5.4 Big-O notation for complexity class\nVery often, we are not interested in the actual function C(n) that describes the time complex-\n\nity of an algorithm in terms of the problem size n, but just its complexity class. This ignores\nany constant overheads and small constant factors, and just tells us about the principal growth\n\n26\n\n\nof the complexity function with problem size, and hence something about the performance of\n\nthe algorithm on large numbers of items.\n\nIf an algorithm is such that we may consider all steps equally costly, then usually the\ncomplexity class of the algorithm is simply determined by the number of loops and how often\nthe content of those loops are being executed. The reason for this is that adding a constan\n\nnumber of instructions which does not change with the size of the problem has no significan\n\neffect on the overall complexity for large problems.\n\nThere is a standard notation, called the Big-O notation, for expressing the fact tha’\nconstant factors and other insignificant details are being ignored. For example, we saw thai\nhe procedure last(1) on a list 1 had time complexity that depended linearly on the size n of\nhe list, so we would say that the time complexity of that algorithm is O(n). Similarly, linear\nsearch is O(n). For binary search, however, t\nBefore we define complexity classes in a more formal manner, it is worth trying to gain\nsome intuition about what they actually mean. For this purpose, it is useful to choose one\nasses we wish to consider. Recall that we are\n\nye considered ‘typical’ for its class.\n\n‘unction as a representative of each of the c\nconsidering functions which map natural numbers (the size of the problem) to the set of non-\nnegative real numbers Rt, so the classes will correspond to common mathematical functions\nsuch as powers and logarithms. We shall consider later to what degree a representative can\n\n1e time complexity is O(log2 n).\n\nThe most common complexity classes (in increasing order) are the following:\n\ne O(1), pronounced ‘Oh of one’, or constant complexity;\n\ne O(log logs n), ‘Oh of log log en’;\n\ne O(logz n), ‘Oh of log en’, or logarithmic complexity;\n\nn), ‘Oh of en’, or linear complex\n\nnlogg n), ‘Oh of en log en’;\n\ne\n2 8 8 8\n\nity;\n\nn?), ‘Oh of en cubed’, or cubic complexity;\n\nn?), ‘Oh of en squared’, or quadratic complexity;\n\ne O(2\"), ‘Oh of two to the en’, or exponential complexity.\n\nAs a representative, we choose the function which gives the class its name ~ e.g. for O(n) we\nchoose the function f(n) = n, for O(logzn) we choose f(n) = loge n, and so on. So assume\n1eir complexity. The following table lists\n\nwe have algorithms with these functions describing t.\n\nhow many operations it will take them to deal with a problem of a given size:\nf(n) n=4)/n=16 n = 256 n = 1024 n = 1048576\n1 1 1 1] 1.00 x 10° 1.00 x 10°\nlogs logy n 1 2 3] 3.32 10° 4.32 x 10°\nloge n 2 4 8} 1.00 10! 2.00 x 10!\nn 4 16 | 2.56x 107} 1.02 x 108 1.05 x 10°\nnlogyn 8 64 | 2.05x 10° | 1.02 x 104 2.10 x 107\nn? 16 256 | 6.55 x 104} 1.05 x 10° 1.10 x 101?\nn3 64 4096 | 1.68x 107 | 1.07 x 10° 1.15 x 1018\nQn 16 | 65536 | 1.16 x 1077 | 1.80 x 10208 | 6.74 x 10315652\n\n27\n\n\n\nSome of these numbers are so large that it is rather difficult to imagine just how long a\n\ntime span they describe. Hence the following table gives time spans rather than instruction\ncounts, based on the assumption that we have a computer which can operate at a speed of 1\nMIP, where one MIP = a million instructions per second:\nf(n) n=4 n= 16 n = 256 n = 1024 n = 1048576\n1 1 psec 1 psec 1 psec 1 psec 1 psec\nloge loge n 1 usec 2 jusec 3 pusec 3.32 jusec 4.32 usec\nlogan 2 psec 4 usec 8 jusec 10 psec 20 jusec\nn 4 psec 16 psec 256 jusec 1.02 msec 1.05 sec\nnlogsn 8 pusec 64 jusec 2.05 msec 1.02 msec 21 sec\nn? 16 pusec | 256 jusec 65.5 msec 1.05 sec 1.8 wk\nn3 64 psec | 4.1 msec 16.8 sec 17.9 min 36,559 yr\nQn 16 psec | 65.5 msec | 3.7 x 10% yr | 5.7 x 10794 yr | 2.1 x 10315639 yr\n\nIt is clear that, as the sizes of the problems get really big, there can be huge differences\nin the time it takes to run algorithms from different complexity classes. For algorithms\nwith exponential complexity, O(2\"), even modest sized problems have run times that are\ngreater than the age of the universe (about 1.4 x 10!° yr), and current computers rarely run\nuninterrupted for more than a few years. This is why complexity classes are so important\nthey tell us how feasible it is likely to be to run a program with a particular large number\nof data items. Typically, people do not worry much about complexity for sizes below 10, or\nmaybe 20, but the above numbers make it clear why it is worth thinking about complexity\nclasses where bigger applications are concerned.\n\nAnother useful way of thinking about growth classes involves considering how the compute\ntime will vary if the problem size doubles. The following table shows what happens for the\n\nvarious complexity classes:\n\nf(n) | If the size of the problem doubles then f(n) will be\n1 | the same, f(2n) = f(n)\nlogs logz n | almost the same, log (logs (2n)) = logs (logs (n) + 1)\nlogan | more by 1 = logs 2, f(2n) = f(n)+1\nn | twice as big as before, f(2n) = 2f(n)\nnlog2 n | a bit more than twice as big as before, 2nloge (2n) = 2(nlogg n) + 2n\n\nn? | four times as big as before, f(2n) =4f(n)\n\nn? | eight times as big as before, f(2n) = 8f(n)\n\n2” | the square of what it was before, f (2n) = (f(n))?\n\nThis kind of information can be very useful in practice. We can test our program on a problem\n\nthat is a half or quarter or one eighth of the full size, and have a good idea of how long we\nwill have to wait for the full size problem to finish. Moreover, that estimate won’t be affected\nby any constant factors ignored in computing the growth class, or the speed of the particular\ncomputer it is run on.\n\nThe following graph plots some of the complexity class functions from the table. Note\nthat although these functions are only defined on natural numbers, they are drawn as though\nthey were defined for all real numbers, because that makes it easier to take in the information\npresented.\n\n28\n\n\n100. —5 : r r r r r r r\non\n\n90+ n2 4\n\n80 4\nnlogn\n\n70- 4\n\n40} 4\n\n10 20 30 40 50 60 70 80 90 100\n\nIt is clear from these plots why the non-principal growth terms can be safely ignored when\ncomputing algorithm complexity.\n\n5.5 Formal definition of complexity classes\n\nWe have noted that complexity classes are concerned with growth, and the tables and graph\nabove have provided an idea of what different behaviours mean when it comes to growth.\nThere we have chosen a representative for each of the complexity classes considered, but we\nhave not said anything about just how ‘representative’ such an element is. Let us now consider\na more formal definition of a ‘big O’ class:\n\nDefinition. A function g belongs to the complexity class O(f) if there is a number no € N\nand a constant c > 0 such that for all n > no, we have that g(n) < cx f(n). We say that the\n‘unction g is ‘eventually smaller’ than the function c * f.\n\nIt is not totally obvious what this implies. First, we do not need to know exactly when g\nyecomes smaller than c * f. We are only interested in the existence of no such that, from\nhen on, g is smaller than c * f. Second, we wish to consider the efficiency of an algorithm\nindependently of the speed of the computer that is going to execute it. This is why f is\nmultiplied by a constant c. The idea is that when we measure the time of the steps of a\nparticular algorithm, we are not sure how long each of them takes. By definition, g € O(f)\nmeans that eventually (namely beyond the point no), the growth of g will be at most as much\nas the growth of c* f. This definition also makes it clear that constant factors do not change\nhe growth class (or O-class) of a function. Hence C(n) = n? is in the same growth class as\nC(n) = 1/1000000 « n? or C(n) = 1000000 * n?. So we can write O(n?) = O(1000000 * n?) =\nO(1/1000000 * n?). Typically, however, we choose the simplest representative, as we did in\nhe tables above. In this case it is O(n?).\n\n29\n\n\nThe various classes we mentioned above are related as follows:\nO(1) € O(logs logs n) C O(logs (n)) C O(n) E O(nlogan) C O(n?) C O(n?) C O(2\")\n\nWe only consider the principal growth class, so when adding functions from different growth\nclasses, their sum will always be in the larger growth class. This allows us to simplify terms.\nFor example, the growth class of C(n) = 500000log2 n +4n? +0.3n + 100 can be determined as\nfollows. The summand with the largest growth class is 4n? (we say that this is the ‘principal\nsub-term’ or ‘dominating sub-term’ of the function), and we are allowed to drop constant\nfactors, so this function is in the class O(n”).\n\nWhen we say that an algorithm ‘belongs to’ some class O(f), we mean that it is at most\nas fast growing as f. We have seen that ‘linear searching’ (where one searches in a collection\nof data items which is unsorted) has linear complexity, i.e. it is in growth class O(n). This\nholds for the average case as well as the worst case. The operations needed are comparisons\nof the item we are searching for with all the items appearing in the data collection. In the\nworst case, we have to check all n entries until we find the right one, which means we make\nn comparisons. On average, however, we will only have to check n/2 entries until we hit the\ncorrect one, leaving us with n/2 operations. Both those functions, C(n) = n and C(n) = n/2\nelong to the same complexity class, namely O(n). However, it would be equally correct to\nsay that the algorithm belongs to O(n”), since that class contains all of O(n). But this would\nde less informative, and we would not say that an algorithm has quadratic complexity if we\nknow that, in fact, it is linear. Sometimes it is difficult to be sure what the exact complexity\nis (as is the case with the famous NP = P problem), in which case one might say that an\nalgorithm is ‘at most’, say, quadratic.\n\nThe issue of efficiency and complexity class, and their computation, will be a recurring\neature throughout the chapters to come. We shall see that concentrating only on the com-\nplexity class, rather than finding exact complexity functions, can render the whole process of\nconsidering efficiency much easier. In most cases, we can determine the time complexity by\na simple counting of the loops and tree heights. However, we will also see at least one case\nwhere that results in an overestimate, and a more exact computation is required.\n\n30\n\n\nChapter 6\n\nTrees\n\nIn computer science, a tree is a very general and powerful data structure that resembles a real\ns of an ordered set of linked nodes in a connected graph, in which each node\n\ntree. It consis\nhas at most one parent node, and zero or more children nodes with a specific order.\n\n6.1 General specification of trees\nGenerally, we can specify a tree as consisting of nodes (also called vertices or points) and\nedges (also called lines, or, in order to stress the directedness, arcs) with a tree-like structure.\n\nIt is usually easiest to represent trees pictorially, so we shall frequently do that. A simple\nexample is given in Figure 6.1:\n\noo PR\n\nFigure 6.1: Example of a tree.\n\nMore formally, a tree can be defined as either the empty tree, or a node with a list of successor\ntrees. Nodes are usually, though not always, labelled with a data item (such as a number or\nsearch key). We will refer to the label of a node as its value. In our examples, we will generally\nuse nodes labelled by integers, but one could just as easily choose something else, e.g. strings\nof characters.\n\nIn order to talk rigorously about trees, it is convenient to have some terminology: There\nalways has to be a unique ‘top level’ node known as the root. In Figure 6.1, this is the node\nlabelled with 8. It is important to note that, in computer science, trees are normally displayed\nupside-down, with the root forming the top level. Then, given a node, every node on the next\nlevel ‘down’, that is connected to the given node via a branch, is a child of that node. In\n\n31\n\n\nFigure 6.1, the children of node 8 are nodes 3 and 11. Conversely, the node (there is at most\none) connected to the given node (via an edge) on the level above, is its parent. For instance,\nnode 11 is the parent of node 9 (and of node 14 as well). Nodes that have the same parent\nare known as siblings — siblings are, by definition, always on the same level.\n\nIf a node is the child of a child of ... of a another node then we say that the first node is a\n\ndescendent of the second node. Conversely, the second node is an ancestor of the first node.\nNodes which do not have any children are known as leaves (e.g., the nodes labelled with 1, 7,\n10, 12, and 15 in Figure 6.1).\nA path is a sequence of connected edges from one node to another. Trees have the property\nthat for every node there is a unique path connecting it with the root. In fact, that is another\npossible definition of a tree. The depth or level of a node is given by the length of this path.\nHence the root has level 0, its children have level 1, and so on. The maximal length of a\npath in a tree is also called the height of the tree. A path of maximal length always goes\nfrom the root to a leaf. The size of a tree is given by the number of nodes it contains. We\nshall normally assume that every tree is finite, though generally that need not be the case.\nThe tree in Figure 6.1 has height 3 and size 11. A tree consisting of just of one node has\nheight 0 and size 1. The empty tree obviously has size 0 and is defined (conveniently, though\nsomewhat artificially) to have height —1.\n\nLike most data structures, we need a set of primitive operators (constructors, selectors\nand conditions) to build and manipulate the trees. The details of those depend on the type\nand purpose of the tree. We will now look at some particularly useful types of tree.\n\n6.2 Quad-trees\n\nA quadtree is a particular type of tree in which each leaf-node is labelled by a value and each\nnon-leaf node has exactly four children. It is used most often to partition a two dimensional\nspace (e.g., a pixelated image) by recursively dividing it into four quadrants.\n\nFormally, a quadtree can be defined to be either a single node with a number or value\ne.g., in the range 0 to 255), or a node without a value but with four quadtree children: 1u,\n1, ru, and rl. It can thus be defined “inductively” by the following rules:\n\nDefinition. A quad tree is either\nRule 1) a root node with a value, or\nRule 2) a root node without a value and four quad tree children: 1u, 11, ru, and rl.\n\nin which Rule 1 is the “base case” and Rule 2 is the “induction step” .\nWe say that a quadtree is primitive if it consists of a single node/number, and that can\nbe tested by the corresponding condition:\n\ne isValue(qt), which returns true if quad-tree qt is a single node.\nTo build a quad-tree we have two constructors:\ne baseQT(value), which returns a single node quad-tree with label value.\n\ne makeQT(luqt, ruqt, 1lqt,rlqt), which builds a quad-tree from four constituent quad-\ntrees luqt, llqt, ruqt, rlqt.\n\n32\n\n\nThen to extract components from a quad-tree we have four selectors:\n\ne lu(qt\ne ru(qt\ne 11(qt\ne rl(qt\n\nwhich can b\ncould define\n\n, which returns the left-upper quad-tree.\n, which returns the right-upper quad-tree.\n, which returns the left-lower quad-tree.\n\n, which returns the right-lower quad-tree.\n\ne applied whenever isValue(qt) is false. For cases when isValue(qt) is true, we\nan operator value(qt) that returns the value, but conventionally we simply say\n\nthat qt itse\n\nf is the required value.\n\nQuad-trees of this type are most commonly used to store grey-value pictures (with 0\nrepresenting black and 255 white). A simple example would be:\n\n50 60 | 70\nuo} 1201 gg\n\n— 20\n40 | 30\n\nWe can then create algorithms using the operators to perform useful manipulations of the\nrepresentation. For example, we could rotate a picture qt by 180° using:\n\nrotate(qt) {\n\nif ¢\nre\nelse\n\n}\n\nor we could\n\nisValue(qt) )\n\nturn qt\n\nreturn makeQT( rotate(rl(qt)), rotate(1l(qt)),\nrotate(ru(qt)), rotate(lu(qt)) )\n\ncompute average values by recursively averaging the constituent sub-trees.\n\nThere exist numerous variations of this general idea, such coloured quadtrees which store\nvalue-triples that represent colours rather than grey-scale, and edge quad-trees which store\n\nlines and allow curves to be represented with arbitrary precision.\n\n6.3 Binary trees\n\nBinary trees are the most common type of tree used in computer science. A binary tree is a\ntree in which every node has at most two children, and can be defined “inductively” by the\n\nfollowing ru\n\nles:\n\n\nDefinition. A binary tree is either\n(Rule 1) the empty tree EmptyTree, or\n(Rule 2) it consists of a node and two binary trees, the left subtree and right subtree.\n\nAgain, Rule 1 is the “base case” and Rule 2 is the “induction step”. This definition may appear\ncircular, but actually it is not, because the subtrees are always simpler than the original one,\nand we eventually end up with an empty tree.\n\nYou can imagine that the (infinite) collection of (finite) trees is created in a sequence of\ndays. Day 0 is when you “get off the ground” by applying Rule 1 to get the empty tree. On\nlater days, you are allowed to use any trees that you have created on earlier days to construct\nnew trees using Rule 2. Thus, for example, on day 1 you can create exactly trees that have\na root with a value, but no children (i.e. both the left and right subtrees are the empty tree,\ncreated at day 0). On day 2 you can use a new node with value, with the empty tree and/or\nthe one-node tree, to create more trees. Thus, binary trees are the objects created by the\nabove two rules in a finite number of steps. The height of a tree, defined above, is the number\nof days it takes to create it using the above two rules, where we assume that only one rule is\nused per day, as we have just discussed. (Exercise: work out the sequence of steps needed to\ncreate the tree in Figure 6.1 and hence prove that it is in fact a binary tree.)\n\n6.4 Primitive operations on binary trees\n\nThe primitive operators for binary trees are fairly obvious. We have two constructors which\nare used to build trees:\n\ne EmptyTree, which returns an empty tree,\n\ne MakeTree(v,1,r), which builds a binary tree from a root node with label v and two\nconstituent binary trees 1 and r,\n\na condition to test whether a tree is empty:\ne isEmpty(t), which returns true if tree t is the EmptyTree,\n\nand three selectors to break a non-empty tree into its constituent parts:\ne root(t), which returns the value of the root node of binary tree t,\ne left(t), which returns the left sub-tree of binary tree t,\ne right(t), which returns the right sub-tree of binary tree t.\n\nThese operators can be used to create all the algorithms we might need for manipulating\nbinary trees.\n\nFor convenience though, it is often a good idea to define derived operators that allow us to\nwrite simpler, more readable algorithms. For example, we can define a derived constructor:\n\ne Leaf(v) = MakeTree(v, EmptyTree, EmptyTree)\n\nthat creates a tree consisting of a single node with label v, which is the root and the unique\nleaf of the tree at the same time. Then the tree in Figure 6.1 can be constructed as:\n\n34\n\n\nt = MakeTree(8, MakeTree(3,Leaf(1) ,MakeTree(6,EmptyTree,Leaf(7))),\nMakeTree(11,MakeTree(9,EmptyTree, Leaf (10)) ,MakeTree(14,Leaf (12) ,Leaf(15))))\n\nwhich is much simpler than the construction using the primitive operators:\n\nt = MakeTree(8, MakeTree(3,MakeTree(1,EmptyTree,EmptyTree) ,\nMakeTree (6, EmptyTree ,MakeTree(7,EmptyTree,EmptyTree))),\nMakeTree(11,MakeTree(9,EmptyTree ,MakeTree(10,EmptyTree,EmptyTree)),\nMakeTree(14,MakeTree(12,EmptyTree,EmptyTree) ,\nMakeTree(15,EmptyTree ,EmptyTree) )))\n\nNote that the selectors can only operate on non-empty trees. For example, for the tree t\ndefined above we have\n\nroot(left(left(t)) = 1,\nbut the expression\nroot (left (left (left (t))))\n\ndoes not make sense because\n\neft (left (left (t))) = EmptyTree\n\nand the empty tree does not have a root. In a language such as Java, this would typically\nraise an exception. In a language such as C’, this would cause an unpredictable behaviour,\nbut if you are lucky, a core dump will be produced and the program will be aborted with\nno further harm. When writing algorithms, we need to check the selector arguments using\nisEmpty(t) before allowing their use.\n\nThe following equations should be obvious from the primitive operator definitions:\n\nroot (MakeTree(v,l,r)) =v\nleft (MakeTree(v,l,r)) = 1\nright (MakeTree(v,l,r)) =r\nisEmpty (EmptyTree) = true\nisEmpty (MakeTree(v,l,r)) = false\n\nThe following makes sense only under the assumption that t is a non-empty tree:\nMakeTree(root(t) ,left(t),right(t)) = t\n\nIt just says that if we break apart a non-empty tree and use the pieces to build a new tree,\nthen we get an identical tree back.\n\nIt is worth emphasizing that the above specifications of quad-trees and binary trees are\nfurther examples of abstract data types: Data types for which we exhibit the constructors\nand destructors and describe their behaviour (using equations such as defined above for lists,\nstacks, queues, quad-trees and binary trees), but for which we explicitly hide the implemen-\ntational details. The concrete data type used in an implementation is called a data structure.\nFor example, the usual data structures used to implement the list and tree data types are\nrecords and pointers — but other implementations are possible.\n\nThe important advantage of abstract data types is that we can develop algorithms without\nhaving to worry about the details of the representation of the data or the implementation. Of\ncourse, everything will ultimately be represented as sequences of bits in a computer, but we\nclearly do not generally want to have to think in such low level terms.\n\n35\n\n\n6.5 The height of a binary tree\n\nBinary trees don’t have a simple relation between their size n and height h. The maximum\nheight of a binary tree with n nodes is (n — 1), which happens when all non-leaf nodes have\nprecisely one child, forming something that looks like a chain. On the other hand, suppose we\nhave n nodes and want to build from them a binary tree with minimal height. We can achieve\nthis by ‘filling’ each successive level in turn, starting from the root. It does not matter where\nwe place the nodes on the last (bottom) level of the tree, as long as we don’t start adding to\nthe next level before the previous level is full. Terminology varies, but we shall say that such\ntrees are perfectly balanced or height balanced, and we shall see later why they are optimal for\nmany of our purposes. Basically, if done appropriately, many important tree-based operations\n(such as searching) take as many steps as the height of the tree, so minimizing the height\nminimizes the time needed to perform those operations.\n\nWe can easily determine the maximum number of nodes that can fit into a binary tree of\na given height h. Calling this size function s(h), we obtain:\n\nIn fact, it seems fairly obvious that s(h) =1+2+4+---+2\"=2'+1_ 1. This hypothesis\ncan be proved by induction using the definition of a binary tree as follows:\n\n(a) The base case applies to the empty tree that has height h = —1, which is consistent\nwith s(—1) = 274! —1= 2° —1=1-—1=0 nodes being stored.\n\n(b) Then for the induction step, a tree of height h + 1 has a root node plus two subtrees of\nheight h. By the induction hypothesis, each subtree can store s(h) = 2’+! — 1 nodes,\nso the total number of nodes that can fit in a height h +1 tree is 1+ 2 x (2’+1—1) =\n14 2+? 9 — a(h+I)+l _ 1 = s(h +1). It follows that if s(h) is correct for the empty\ntree, which it was shown to be in the base case above, then it is correct for all h.\n\nAn obvious potential problem with any proof by induction like this, however, is the need to\nidentify an induction hypothesis to start with, and that is not always easy.\n\nAnother way to proceed here would be to simply sum the series s(h) = 1+2+4+---+2\"\nalgebraically to get the answer. Sometimes, however, the relevant series is too complicated\nto sum easily. An alternative is to try to identify two different expressions for s(h +1) asa\nfunction of s(h), and solve them for s(h). Here, since level h of a tree clearly has 2” nodes,\nwe can explicitly add in the 2’+! nodes of the last level of the height h +1 tree to give\n\ns(h +1) = s(h) + 2'+1\n\nAlso, since a height h + 1 tree is made up of a root node plus two trees of height h\n\ns(h+1) =14 2s(h)\n\nThen subtracting the second equation from the first gives\n\ns(h) =2htt_ 1\n\n36\n\n\nwhich is the required answer. From this we can get an expression for h\nh = loge (s +1) —1* logy s\n\nin which the approximation is valid for large s.\nHence a perfectly balanced tree consisting of n nodes has height approximately logan.\nThis is good, because logy n is very small, even for relatively large n:\n\n1,024 10\n1,048, 576 20\n\nWe shall see later how we can use binary trees to hold data in such a way that any search has\nat most as many steps as the height of the tree. Therefore, for perfectly balanced trees we\ncan reduce the search time considerably as the table demonstrates. However, it is not always\neasy to create perfectly balanced trees, as we shall also see later.\n\n6.6 The size of a binary tree\n\nUsually a binary tree will not be perfectly balanced, so we will need an algorithm to determine\nits size, i.e. the number of nodes it contains.\n\nThis is easy if we use recursion. The terminating case is very simple: the empty tree has\nsize 0. Otherwise, any binary tree will always be assembled from a root node, a left sub-tree\n1, and a right sub-tree r, and its size will be the sum of the sizes of its components, i.e. 1\nfor the root, plus the size of 1, plus the size of r. We have already defined the primitive\noperator isEmpty(t) to check whether a binary tree t is empty, and the selectors Left(t) and\nright(t) which return the left and right sub-trees of binary tree t. Thus we can easily define\nthe procedure size(t), which takes a binary tree t and returns its size, as follows:\n\nsize(t) f\nif ( isEmpty(t) )\nreturn 0\nelse return (1 + size(left(t)) + size(right(t)))\n}\n\nThis recursively processes the whole tree, and we know it will terminate because the trees\nbeing processed get smaller with each call, and will eventually reach an empty tree which\nreturns a simple value.\n\n6.7 Implementation of trees\n\nThe natural way to implement trees is in terms of records and pointers, in a similar way to how\nlinked lists were represented as two-cells consisting of a pointer to a list element and a pointer\nto the next two-cell. Obviously, the details will depend on how many children each node can\nhave, but trees can generally be represented as data structures consisting of a pointer to the\nroot-node content (if any) and pointers to the children sub-trees. The inductive definition\n\n37\n\n\nof trees then allows recursive algorithms on trees to operate efficiently by simply passing the\npointer to the relevant root-node, rather than having to pass complete copies of whole trees.\nHow data structures and pointers are implemented in different programming languages will\nvary, of course, but the general idea is the same.\n\nA binary tree can be implemented as a data record for each node consisting simply of the\nnode value and two pointers to the children nodes. Then MakeTree simply creates a new data\nrecord of that form, and root, left and right simply read out the relevant contents of the\nrecord. The absence of a child node can be simply represented by a Null Pointer.\n\n6.8 Recursive algorithms\n\nSome people have difficulties with recursion. A source of confusion is that it appears that\n“the algorithm calls itself” and it might therefore get confused about what it is operating on.\nThis way of putting things, although suggestive, can be misleading. The algorithm itself is a\npassive entity, which actually cannot do anything at all, let alone call itself. What happens is\nthat a processor (which can be a machine or a person) executes the algorithm. So what goes\n\non when a processor executes a recursive algorithm such as the size(t) algorithm above? An\neasy way of understanding this is to imagine that whenever a recursive call is encountered,\nnew processors are given the task with a copy of the same algorithm.\n\nFor example, suppose that John (the first processor in this task) wants to compute the\nsize of a given tree t using the above recursive algorithm. Then, according to the above\nalgorithm, John first checks whether it is empty. If it is, he simply returns zero and finishes\nhis computation. If it isn’t empty, then his tree t must have left and right subtrees 1 and\nxr (which may, or may not, be empty) and he can extract them using the selectors left(t)\nand right(t). He can then ask two of his students, say Steve and Mary, to execute the\nsame algorithm, but for the trees 1 and r. When they finish, say returning results m and n\nrespectively, he computes and returns 1+m-+n, because his tree has a root node in addition to\nthe left and right sub-trees. If Steve and Mary aren’t given empty trees, they will themselves\nhave to delegate executions of the same algorithm, with their sub-trees, to other people. Thus,\nthe algorithm is not calling itself. What happens, is that there are many people running their\nown copies of the same algorithm on different trees.\n\nIn this example, in order to make things understandable, we assumed that each person\nexecutes a single copy of the algorithm. However, the same processor, with some difficulty,\ncan impersonate several processors, in such a way that it achieves the same result as the\nexecution involving many processors. This is achieved via the use of a stack that keeps track\nof the various positions of the same algorithm that are currently being executed — but this\nknowledge is not needed for our purposes.\n\nNote that there is nothing to stop us keeping count of the recursions by passing integers\nalong with any data structures being operated on, for example:\n\nfunction(int n, tree t) {\n// terminating condition and return\n\n// procedure details\n\nreturn function(n-1, t2)\n\n38\n\n\nso we can do something n times, or look for the nth item, etc. The classic example is the\nrecursive factorial function:\n\nfactorial(int n) {\nif ( n == 0) return 1\nreturn n*factorial (n-1)\n\n}\n\nAnother example, with two termination or base-case conditions, is a direct implementation\nof the recursive definition of Fibonacci numbers (see Appendix A.5):\n\nF(int n) f{\nif ( n == 0 ) return 0\nif ( n == 1) return 1\n\nreturn F(n-1) + F(n-2)\n}\n\nthough this is an extremely inefficient algorithm for computing these numbers. Exercise: Show\nthat the time complexity of this algorithm is O(2”), and that there exists a straightforward\niterative algorithm that has only O(n) time complexity. Is it possible to create an O(n)\nrecursive algorithm to compute these numbers?\n\nIn most cases, however, we won’t need to worry about counters, because the relevant data\nstructure has a natural end point condition, such as isEmpty(x), that will bring the recursion\nto an end.\n\n39\n\n\nChapter 7\n\nBinary Search Trees\n\nWe now look at Binary Search Trees, which are a particular type of binary tree that provide\nan efficient way of storing data that allows particular items to be found as quickly as possible.\nThen we consider further elaborations of these trees, namely AVL trees and B-trees, which\noperate more efficiently at the expense of requiring more sophisticated algorithms.\n\n7.1 Searching with arrays or lists\n\nAs we have already seen in Chapter 4, many computer science applications involve searching\nfor a particular item in a collection of data. If the data is stored as an unsorted array or\nlist, then to find the item in question, one obviously has to check each entry in turn until the\ncorrect one is found, or the collection is exhausted. On average, if there are n items, this will\ntake n/2 checks, and in the worst case, all n items will have to be checked. If the collection is\nlarge, such as all items accessible via the internet, that will take too much time. We also saw\nthat if the items are sorted before storing in an array, one can perform binary search which\nonly requires logs n checks in the average and worst cases. However, that involves an overhead\nof sorting the array in the first place, or maintaining a sorted array if items are inserted or\ndeleted over time. The idea here is that, with the help of binary trees, we can speed up the\nstoring and search process without needing to maintain a sorted array.\n\n7.2 Search keys\n\nIf the items to be searched are labelled by comparable keys, one can order them and store\nhem in such a way that they are sorted already. Being ‘sorted’ may mean different things\nor different keys, and which key to choose is an important design decision.\n\nIn our examples, the search keys will, for simplicity, usually be integer numbers (such\nas student ID numbers), but other choices occur in practice. For example, the comparable\nkeys could be words. In that case, comparability usually refers to the alphabetical order. If\nw and t are words, we write w < t to mean that w precedes t in the alphabetical order. If\nw = bed and t = sky then the relation w < t holds, but this is not the case if w = bed and\nt = abacus. A classic example of a collection to be searched is a dictionary. Each entry of\nhe dictionary is a pair consisting of a word and a definition. The definition is a sequence\nof words and punctuation symbols. The search key, in this example, is the word (to which a\ndefinition is attached in the dictionary entry). Thus, abstractly, a dictionary is a sequence of\n\n40\n\n\nentries, where an entry is a pair consisting of a word and its definition. This is what matters\nfrom the point of view of the search algorithms we are going to consider. In what follows, we\nshall concentrate on the search keys, but should always bear in mind that there is usually a\nmore substantial data entry associated with it.\n\nNotice the use of the word “abstract” here. What we mean is that we abstract or remove\nany details that are irrelevant from the point of view of the algorithms. For example, a\ndictionary usually comes in the form of a book, which is a sequence of pages — but for us,\nhe distribution of dictionary entries into pages is an accidental feature of the dictionary. All\nhat matters for us is that the dictionary is a sequence of entries. So “abstraction” means\n“eetting rid of irrelevant details”. For our purposes, only the search key is important, so we\nwill ignore the fact that the entries of the collection will typically be more complex objects\nas in the example of a dictionary or a phone book).\n\nNote that we should always employ the data structure to hold the items which performs\nest for the typical application. There is no easy answer as to what the best choice is — the\nparticular circumstances have to be inspected, and a decision has to be made based on that.\nHowever, for many applications, the kind of binary trees we studied in the last chapter are\nparticularly useful here.\n\n7.3 Binary search trees\n\nThe solution to our search problem is to store the collection of data to be searched using\na binary tree in such a way that searching for a particular item takes minimal effort. The\nunderlying idea is simple: At each tree node, we want the value of that node to either tell us\nthat we have found the required item, or tell us which of its two subtrees we should search for\nit in. For the moment, we shall assume that all the items in the data collection are distinct,\nwith different search keys, so each possible node value occurs at most once, but we shall see\nlater that it is easy to relax this assumption. Hence we define:\n\nDefinition. A binary search tree is a binary tree that is either empty or satisfies the following\nconditions:\n\ne All values occurring in the left subtree are smaller than that of the root.\ne All values occurring in the right subtree are larger than that of the root.\ne The left and right subtrees are themselves binary search trees.\n\nSo this is just a particular type of binary tree, with node values that are the search keys. This\nmeans we can inherit many of the operators and algorithms we defined for general binary\ntrees. In particular, the primitive operators MakeTree(v, 1,r), root(t), left(t), right(t) and\nisEmpty(t) are the same — we just have to maintain the additional node value ordering.\n\n7.4 Building binary search trees\n\nWhen building a binary search tree, one naturally starts with the root and then adds further\nnew nodes as needed. So, to insert a new value v, the following cases arise:\n\ne If the given tree is empty, then simply assign the new value v to the root, and leave the\nleft and right subtrees empty.\n\n41\n\n\ne If the given tree is non-empty, then insert a node with value v as follows:\n\n— If v is smaller than the value of the root: insert v into the left sub-tree.\n— If v is larger than the value of the root: insert v into the right sub-tree.\n\n— If v is equal to the value of the root: report a violated assumption.\nThus, using the primitive binary tree operators, we have the procedure:\n\ninsert(v,bst) {\nif ( isEmpty(bst) )\nreturn MakeTree(v, EmptyTree, EmptyTree)\nelseif ( v < root(bst) )\nreturn MakeTree(root(bst), insert(v,left(bst)), right (bst))\nelseif ( v > root(bst) )\nreturn MakeTree(root(bst), left(bst), insert(v,right (bst)))\nelse error(‘Error: violated assumption in procedure insert.’)\n\n}\n\nwhich inserts a node with value v into an existing binary search tree bst. Note that the node\nadded is always a leaf. The resulting tree is once again a binary search tree. This can be\nproved rigorously via an inductive argument.\n\nNote that this procedure creates a new tree out of a given tree bst and new value v,\nwith the new value inserted at the right position. The original tree bst is not modified, it\nis merely inspected. However, when the tree represents a large database, it would clearly be\nmore efficient to modify the given tree, rather than to construct a whole new tree. That can\neasily be done by using pointers, similar to the way we set up linked lists. For the moment,\nthough, we shall not concern ourselves with such implementational details.\n\n7.5 Searching a binary search tree\n\nSearching a binary search tree is not dissimilar to the process performed when inserting a\nnew item. We simply have to compare the item being looked for with the root, and then keep\n‘pushing’ the comparison down into the left or right subtree depending on the result of each\nroot comparison, until a match is found or a leaf is reached.\n\nAlgorithms can be expressed in many ways. Here is a concise description in words of the\nsearch algorithm that we have just outlined:\n\nIn order to search for a value v in a binary search tree t, proceed as follows. If t\nis empty, then v does not occur in t, and hence we stop with false. Otherwise,\nif v is equal to the root of t, then v does occur in t, and hence we stop returning\ntrue. If, on the other hand, v is smaller than the root, then, by definition of a\nbinary search tree, it is enough to search the left sub-tree of t. Hence replace t\nby its left sub-tree and carry on in the same way. Similarly, if v is bigger than the\nroot, replace t by its right sub-tree and carry on in the same way.\n\nNotice that such a description of an algorithm embodies both the steps that need to be carried\nout and the reason why this gives a correct solution to the problem. This way of describing\nalgorithms is very common when we do not intend to run them on a computer.\n\n42\n\n\nWhen we do want to run them, we need to provide a more precise specification, and would\nnormally write the algorithm in pseudocode, such as the following recursive procedure:\n\nisIn(value v, tree t) {\n\nif ( isEmpty(t) )\nreturn false\n\nelseif ( v == root(t) )\nreturn true\n\nelseif ( v < root(t) )\nreturn isIn(v, left(t))\n\nelse\n\nreturn isIn(v, right(t))\n}\n\nEach recursion restricts the search to either the left or right subtree as appropriate, reducing\nthe search tree height by one, so the algorithm is guaranteed to terminate eventually.\nIn this case, the recursion can easily be transformed into a while-loop:\n\nisIn(value v, tree t) {\nwhile ( (not isEmpty(t)) and (v != root(t)) )\nif (v < root(t) )\nt = left(t)\nelse\nt = right (t)\nreturn ( not isEmpty(t) )\n}\n\nHere, each iteration of the while-loop restricts the search to either the left or right subtree as\nappropriate. The only way to leave the loop is to have found the required value, or to only\nhave an empty tree remaining, so the procedure only needs to return whether or not the final\ntree is empty.\n\nIn practice, we often want to have more than a simple true/false returned. For example,\nif we are searching for a student ID, we usually want a pointer to the full record for that\nstudent, not just a confirmation that they exist. In that case, we could store a record pointer\nassociated with the search key (ID) at each tree node, and return the record pointer or a null\npointer, rather than a simple true or false, when an item is found or not found. Clearly,\nthe basic tree structures we have been discussing can be elaborated in many different ways\nlike this to form whatever data-structure is most appropriate for the problem at hand, but,\nas noted above, we can abstract out such details for current purposes.\n\n7.6 Time complexity of insertion and search\n\nAs always, it is important to understand the time complexity of our algorithms. Both item\ninsertion and search in a binary search tree will take at most as many comparisons as the\nheight of the tree plus one. At worst, this will be the number of nodes in the tree. But how\nmany comparisons are required on average? To answer this question, we need to know the\naverage height of a binary search tree. This can be calculated by taking all possible binary\nsearch trees of a given size n and measuring each of their heights, which is by no means an\n\n43,\n\n\neasy task. The trouble is that there are many ways of building the same binary search tree\nby successive insertions.\n\nAs we have seen above, perfectly balanced trees achieve minimal height for a given number\nof nodes, and it turns out that the more balanced a tree, the more ways there are of building\nit. This is demonstrated in the figure below:\n\nThe only way of getting the tree on the left hand side is by inserting 3, 2, 1 into the empty\ntree in that order. The tree on the right, however, can be reached in two ways: Inserting in\nthe order 2, 1, 3 or in the order 2, 3, 1. Ideally, of course, one would only use well-balanced\ntrees to keep the height minimal, but they do not have to be perfectly balanced to perform\nbetter than binary search trees without restrictions.\n\nCarrying out exact tree height calculations is not straightforward, so we will not do that\nhere. However, if we assume that all the possible orders in which a set of n nodes might be\ninserted into a binary search tree are equally likely, then the average height of a binary search\ntree turns out to be O(loggn). It follows that the average number of comparisons needed to\nsearch a binary search tree is O(loggn), which is the same complexity we found for binary\nsearch of a sorted array. However, inserting a new node into a binary search tree also depends\non the tree height and requires O(log2 n) steps, which is better than the O(n) complexity of\ninserting an item into the appropriate point of a sorted array.\n\nInterestingly, the average height of a binary search tree is quite a bit better than the\naverage height of a general binary tree consisting of the same n nodes that have not been\nbuilt into a binary search tree. The average height of a general binary tree is actually O(/n).\nThe reason for that is that there is a relatively large proportion of high binary trees that are\nnot valid binary search trees.\n\n7.7 Deleting nodes from a binary search tree\n\nSuppose, for some reason, an item needs to be removed or deleted from a binary search tree.\nIt would obviously be rather inefficient if we had to rebuild the remaining search tree again\nfrom scratch. For n items that would require n steps of O(logz n) complexity, and hence have\noverall time complexity of O(nlogyn). By comparison, deleting an item from a sorted array\nwould only have time complexity O(n), and we certainly want to do better than that. Instead,\nwe need an algorithm that produces an updated binary search tree more efficiently. This is\nmore complicated than one might assume at first sight, but it turns out that the following\nalgorithm works as desired:\n\ne If the node in question is a leaf, just remove it.\ne If only one of the node’s subtrees is non-empty, ‘move up’ the remaining subtree.\n\ne If the node has two non-empty sub-trees, find the ‘left-most’ node occurring in the right\nsub-tree (this is the smallest item in the right subtree). Use this node to overwrite the\n\n44\n\n\none that is to be deleted. Replace the left-most node by its right subtree, if this exists;\notherwise just delete it.\n\nThe last part works because the left-most node in the right sub-tree is guaranteed to be bigger\nthan all nodes in the left sub-tree, smaller than all the other nodes in the right sub-tree, and\nhave no left sub-tree itself. For instance, if we delete the node with value 11 from the tree in\nFigure 6.1, we get the tree displayed in Figure 7.1.\n\nUS\n\n© i) ©\n\\ \\\n\nFigure 7.1: Example of node deletion in a binary search tree.\n\nIn practice, we need to turn the above algorithm (specified in words) into a more detailed\nalgorithm specified using the primitive binary tree operators:\n\ndelete(value v, tree t) {\nif ( isEmpty(t) )\nerror(‘Error: given item is not in given tree’)\nelse\nif ( v < root(t) ) // delete from left sub-tree\nreturn MakeTree(root(t), delete(v,left(t)), right (t));\nelse if ( v > root(t) ) // delete from right sub-tree\nreturn MakeTree(root(t), left(t), delete(v,right(t)));\nelse // the item v to be deleted is root(t)\nif ( isEmpty(left(t)) )\nreturn right(t)\nelseif ( isEmpty(right(t)) )\nreturn left (t)\nelse // difficult case with both subtrees non-empty\nreturn MakeTree(smallestNode(right(t)), left(t),\nremoveSmallestNode (right (t) )\n}\n\nIf the empty tree condition is met, it means the search item is not in the tree, and an\nappropriate error message should be returned.\n\nThe delete procedure uses two sub-algorithms to find and remove the smallest item of a\ngiven sub-tree. Since the relevant sub-trees will always be non-empty, these sub-algorithms can\nbe written with that precondition. However, it is always the responsibility of the programmer\nto ensure that any preconditions are met whenever a given procedure is used, so it is important\nto say explicitly what the preconditions are. It is often safest to start each procedure with a\n\n45\n\n\ncheck to determine whether the preconditions are satisfied, with an appropriate error message\nproduced when they are not, but that may have a significant time cost if the procedure is\ncalled many times. First, to find the smallest node, we have:\n\nsmallestNode(tree t) {\n// Precondition: t is a non-empty binary search tree\nif ( isEmpty(left(t) )\nreturn root(t)\nelse\nreturn smallestNode(left(t));\n\n}\n\nwhich uses the fact that, by the definition of a binary search tree, the smallest node of t is\nthe left-most node. It recursively looks in the left sub-tree till it reaches an empty tree, at\nwhich point it can return the root. The second sub-algorithm uses the same idea:\n\nremoveSmallestNode(tree t) {\n// Precondition: t is a non-empty binary search tree\nif ( isEmpty(left(t) )\nreturn right(t)\nelse\nreturn MakeTree(root(t), removeSmallestNode(left(t)), right(t))\n}\n\nexcept that the remaining tree is returned rather than the smallest node.\n\nThese procedures are further examples of recursive algorithms. In each case, the recursion\nis guaranteed to terminate, because every recursive call involves a smaller tree, which means\nthat we will eventually find what we are looking for or reach an empty tree.\n\nIt is clear from the algorithm that the deletion of a node requires the same number of\nsteps as searching for a node, or inserting a new node, i.e. the average height of the binary\nsearch tree, or O(log2n) where n is the total number of nodes on the tree.\n\n7.8 Checking whether a binary tree is a binary search tree\n\nBuilding and using binary search trees as discussed above is usually enough. However, another\nthing we sometimes need to do is check whether or not a given binary tree is a binary search\ntree, so we need an algorithm to do that. We know that an empty tree is a (trivial) binary\nsearch tree, and also that all nodes in the left sub-tree must be smaller than the root and\nthemselves form a binary search tree, and all nodes in the right sub-tree must be greater than\nthe root and themselves form a binary search tree. Thus the obvious algorithm is:\n\nisbst(tree t) {\nif ( isEmpty(t) )\nreturn true\nelse\nreturn ( allsmaller(left(t),root(t)) and isbst(left(t))\nand allbigger(right(t),root(t)) and isbst(right(t)) )\n\n46\n\n\nallsmaller(tree t, value v) {\nif ( isEmpty(t) )\nreturn true\nelse\nreturn ( (root(t) < v) and allsmaller(left(t) ,v)\nand allsmaller(right(t),v) )\n\nallbigger(tree t, value v) {\nif ( isEmpty(t) )\nreturn true\nelse\nreturn ( (root(t) > v) and allbigger(left(t) ,v)\nand allbigger(right(t),v) )\n\n}\n\nHowever, the simplest or most obvious algorithm is not always the most efficient. Exercise:\nidentify what is inefficient about this algorithm, and formulate a more efficient algorithm.\n\n7.9 Sorting using binary search trees\n\nSorting is the process of putting a collection of items in order. We shall formulate and discuss\nmany sorting algorithms later, but we are already able to present one of them.\n\nThe node values stored in a binary search tree can be printed in ascending order by\nrecursively printing each left sub-tree, root, and right sub-tree in the right order as follows:\n\nprintInOrder(tree t) {\nif ( not isEmpty(t) ) {\nprintInOrder (left (t))\nprint (root (t))\nprintInOrder (right (t))\n\n}\n\nThen, if the collection of items to be sorted is given as an array a of known size n, they can\nbe printed in sorted order by the algorithm:\n\nsort(array a of size n) {\nt = EmptyTree\nfor i= 0,1,...,n-1\nt = insert (a[i] ,t)\nprintInOrder (t)\n}\n\nwhich starts with an empty tree, inserts all the items into it using insert(v,t) to give a\nbinary search tree, and then prints them in order using printInOrder(t). Exercise: modify\nthis algorithm so that instead of printing the sorted values, they are put back into the original\narray in ascending order.\n\n47\n\n\n7.10 Balancing binary search trees\n\nIf the items are added to a binary search tree in random order, the tree tends to be fairly\nwell balanced with height not much more than logan. However, there are many situations\nwhere the added items are not in random order, such as when adding new student IDs. In\nhe extreme case of the new items being added in ascending order, the tree will be one long\nranch off to the right, with height n > logan.\n\nIf all the items to be inserted into a binary search tree are already sorted, it is straight-\norward to build a perfectly balanced binary tree from them. One simply has to recursively\nyuild a binary tree with the middle (i.e., median) item as the root, the left subtree made up\nof the smaller items, and the right subtree made up of the larger items. This idea can be used\no rebalance any existing binary search tree, because the existing tree can easily be output\ninto a sorted array as discussed in Section 7.9. Exercise: Write an algorithm that rebalances\na binary search tree in this way, and work out its time complexity.\n\nAnother way to avoid unbalanced binary search trees is to rebalance them from time to\nime using tree rotations. Such tree rotations are best understood as follows: Any binary\nsearch tree containing at least two nodes can clearly be drawn in one of the two forms:\n\nRight rotation\n—\n\n(B) [E A] (D)\n\nLeft rotation\n\nAl [c C| |E\n\nwhere B and D are the required two nodes to be rotated, and A, C and E are binary search\nsub-trees (any of which may be empty). The two forms are related by left and right tree\nrotations which clearly preserve the binary search tree property. In this case, any nodes in\nsub-tree A would be shifted up the tree by a right rotation, and any nodes in sub-tree E would\noe shifted up the tree by a left rotation. For example, if the left form had A consisting of two\nnodes, and C and E consisting of one node, the height of the tree would be reduced by one\nand become perfectly balanced by a right tree rotation.\n\nTypically, such tree rotations would need to be applied to many different sub-trees of a\n‘ull tree to make it perfectly balanced. For example, if the left form had C consisting of two\nnodes, and A and E consisting of one node, the tree would be balanced by first performing\na left rotation of the A-B-C sub-tree, followed by a right rotation of the whole tree. In\npractice, finding suitable sequences of appropriate tree rotations to rebalance an arbitrary\ninary search tree is not straightforward, but it is possible to formulate systematic balancing\nalgorithms that are more efficient than outputting the whole tree and rebuilding it.\n\n7.11 Self-balancing AVL trees\nSelf-balancing binary search trees avoid the problem of unbalanced trees by automatically\n\nrebalancing the tree throughout the insertion process to keep the height close to logy n at\neach stage. Obviously, there will be a cost involved in such rebalancing, and there will be a\n\n48\n\n\ntrade-off between the time involved in rebalancing and the time saved by the reduced height\nof the tree, but generally it is worthwhile.\n\nThe earliest type of self-balancing binary search tree was the AVL tree (named after its\ninventors G.M. Adelson-Velskii and E.M. Landis). These maintain the difference in heights\nof the two sub-trees of all nodes to be at most one. This requires the tree to be periodically\nrebalanced by performing one or more tree rotations as discussed above, but the complexity\nof insertion, deletion and search remain at O(log n).\n\nThe general idea is to keep track of the balance factor for each node, which is the height\nof the left sub-tree minus the height of the right sub-tree. By definition, all the nodes in an\nAVL-tree will have a balance factor in the integer range [—1, 1]. However, insertion or deletion\nof a node could leave that in the wider range {[—2, 2] requiring a tree-rotation to bring it back\ninto AVL form. Exercise: Find some suitable algorithms for performing efficient AVL tree\nrotations. Compare them with other self-balancing approaches such as red-black trees.\n\n7.12 B-trees\n\nA B-tree is a generalization of a self-balancing binary search tree in which each node can hold\nmore than one search key and have more than two children. The structure is designed to\nallow more efficient self-balancing, and offers particular advantages when the node data needs\nto be kept in external storage such as disk drives. The standard (Knuth) definition is:\n\nDefinition. A B-tree of order m is a tree which satisfies the following conditions:\ne Every node has at most m children.\ne Every non-leaf node (except the root node) has at least m/2 children.\ne The root node, if it is not a leaf node, has at least two children.\n\ne A non-leaf node with ¢ children contains c— 1 search keys which act as separation values\nto divide its sub-trees.\n\ne All leaf nodes appear in the same level, and carry information.\n\nThere appears to be no definitive answer to the question of what the “B” in “B-Tree” stands\nfor. It is certainly not “Binary”, but it could equally well be “balanced”, “broad” or “bushy”,\nor even “Boeing” because they were invented by people at Boeing Research Labs.\n\nThe standard representation of simple order 4 example with 9 search keys would be:\n\nPs ||\n\n1} 2]5 9 | 12 [19 21\n\nThe search keys held in each node are ordered (e.g., 1, 2, 5 in the example), and the non-leaf\nnode’s search keys (i.e., the items 8 and 17 in the example) act as separation values to divide\n\n49\n\n\nhe contents of its sub-trees in much the same way that a node’s value in a binary search\nree separates the values held in its two sub-trees. For example, if a node has 3 child nodes\nor sub-trees) then it must have 2 separation values s1 and s2. All values in the leftmost\nsubtree will be less than s1, all values in the middle subtree will be between s1 and s2, and all\nvalues in the rightmost subtree will be greater than s2. That allows insertion and searching\no proceed from the root down in a similar way to binary search trees.\n\nThe restriction on the number of children to lie between m/2 and m means that the best\ncase height of an order m B-tree containing n search keys is logmn and the worst case height\nis logm/gn. Clearly the costs of insertion, deletion and searching will all be proportional to\nhe tree height, as in a binary search tree, which makes them very efficient. The requirement\nhat all the leaf nodes are at the same level means that B-trees are always balanced and thus\nhave minimal height, though rebalancing will often be required to restore that property after\n\ninsertions and deletions.\n\nThe order of a B-tree is typically chosen to optimize a particular application and imple-\nmentation. To maintain the conditions of the B-tree definition, non-leaf nodes often have to\nde split or joined when new items are inserted into or deleted from the tree (which is why\nhere is a factor of two between the minimum and maximum number of children), and rebal-\nancing is often required. This renders the insertion and deletion algorithms somewhat more\ncomplicated than for binary search trees. An advantage of B-trees over self balancing binary\nsearch trees, however, is that the range of child nodes means that rebalancing is required less\nrequently. A disadvantage is that there may be more space wastage because nodes will rarely\nye completely full. There is also the cost of keeping the items within each node ordered, and\nhaving to search among them, but for reasonably small orders m, that cost is low. Exercise:\nfind some suitable insertion, deletion and rebalancing algorithms for B-trees.\n\n\n\nChapter 8\n\nPriority Queues and Heap Trees\n\n8.1 Trees stored in arrays\n\nIt was noted earlier that binary trees can be stored with the help of pointer-like structures,\nin which each item contains references to its children. If the tree in question is a complete\nbinary tree, there is a useful array based alternative.\n\nDefinition. A binary tree is complete if every level, except possibly the last, is completely\nfilled, and all the leaves on the last level are placed as far to the left as possible.\n\nIntuitively, a complete binary tree is one that can be obtained by filling the nodes starting\nwith the root, and then each next level in turn, always from the left, until one runs out of\nnodes. Complete binary trees always have minimal height for their size n, namely logan, and\nare always perfectly balanced (but not every perfectly balanced tree is complete in the sense\nof the above definition). Moreover, and more importantly, it is possible for them to be stored\nstraightforwardly in arrays, top-to-bottom left-to-right, as in the following example:\n\nFor complete binary trees, such arrays provide very tight representations.\n\nNotice that this time we have chosen to start the array with index 1 rather than 0. This\nhas several computational advantages. The nodes on level i then have indices 2’,--- ,2’+1—1.\nThe level of a node with index i is |loggi|, that is, logg rounded down. The children of a\nnode with index i, if they exist, have indices 21 and 21+ 1. The parent of a child with index 7\n\nhas index i/2 (using integer division). This allows the following simple algorithms:\n\nboolean isRoot(int i) {\nreturn i ==\n\n\nint level(int i) {\nreturn log(i)\n\nint parent(int i) {\nreturn i / 2\n\n}\n\nint left(int i) {\nreturn 2 * i\n\nint right(int i) {\nreturn2*i+1\n\n}\n\nwhich make the processing of these trees much easier.\n\nThis way of storing a binary tree as an array, however, will not be efficient if the tree\nis not complete, because it involves reserving space in the array for every possible node in\nthe tree. Since keeping binary search trees balanced is a difficult problem, it is therefore not\nreally a viable option to adapt the algorithms for binary search trees to work with them stored\nas arrays. Array-based representations will also be inefficient for binary search trees because\nnode insertion or deletion will usually involve shifting large portions of the array. However, we\nshall now see that there is another kind of binary tree for which array-based representations\nallow very efficient processing.\n\n8.2 Priority queues and binary heap trees\n\nWhile most queues in every-day life operate on a first come, first served basis, it is sometimes\nimportant to be able to assign a priority to the items in the queue, and always serve the item\nwith the highest priority next. An example of this would be in a hospital casualty department,\nwhere life-threatening injuries need to be treated first. The structure of a complete binary\nree in array form is particularly useful for representing such priority queues.\n\nIt turns out that these queues can be implemented efficiently by a particular type of\ncomplete binary tree known as a binary heap tree. The idea is that the node labels, which\nwere the search keys when talking about binary search trees, are now numbers representing\nhe priority of each item in question (with higher numbers meaning a higher priority in our\nexamples). With heap trees, it is possible to insert and delete elements efficiently without\nhaving to keep the whole tree sorted like a binary search tree. This is because we only ever\nwant to remove one element at a time, namely the one with the highest priority present, and\nhe idea is that the highest priority item will always be found at the root of the tree.\n\nDefinition. A binary heap tree is a complete binary tree which is either empty or satisfies\nhe following conditions:\n\ne The priority of the root is higher than (or equal to) that of its children.\n\ne The left and right subtrees of the root are heap trees.\n\n52\n\n\nAlternatively, one could define a heap tree as a complete binary tree such that the priority of\nevery node is higher than (or equal to) that of all its descendants. Or, as a complete binary\ntree for which the priorities become smaller along every path down through the tree.\n\nThe most obvious difference between a binary heap tree and a binary search trees is that\nthe biggest number now occurs at the root rather than at the right-most node. Secondly,\nwhereas with binary search trees, the left and right sub-trees connected to a given parent\nnode play very different roles, they are interchangeable in binary heap trees.\n\nThree examples of binary trees that are valid heap trees are:\n\nSob e O°\n\nand three which are not valid heap trees are:\n\nthe first because 5 > 4 violates the required priority ordering, the second because it is not\nperfectly balanced and hence not complete, and the third because it is not complete due to\nthe node on the last level not being as far to the left as possible.\n\n8.3 Basic operations on binary heap trees\n\nIn order to develop algorithms using an array representation, we need to allocate memory and\nkeep track of the largest position that has been filled so far, which is the same as the current\nnumber of nodes in the heap tree. This will involve something like:\n\nint MAX = 100 // Maximum number of nodes allowed\nint heap[MAX+1] // Stores priority values of nodes of heap tree\nint n=0 // Largest position that has been filled so far\n\nFor heap trees to be a useful representation of priority queues, we must be able to insert new\nnodes (or customers) with a given priority, delete unwanted nodes, and identify and remove\nthe top-priority node, i.e. the root (that is, ‘serve’ the highest priority customer). We also\nneed to be able to determine when the queue/tree is empty. Thus, assuming the priorities are\ngiven by integers, we need a constructor, mutators/selectors, and a condition:\n\ninsert(int p, array heap, int n)\ndelete(int i, array heap, int n)\n\nint root(array heap, int n)\nboolean heapEmpty(array heap, int n)\n\nIdentifying whether the heap tree is empty, and getting the root and last leaf, is easy:\n\n53\n\n\nboolean heapEmpty(array heap, int n) {\nreturn n ==\n\n}\n\nint root(array heap, int n) {\nif ( heapEmpty(heap,n) )\nerror(‘Heap is empty’)\nelse return heap[1]\n\nint lastLeaf(array heap, int n) f\nif ( heapEmpty(heap,n) )\nerror(‘Heap is empty’)\nelse return heap[n]\n\n}\n\nInserting and deleting heap tree nodes is also straightforward, but not quite so easy.\n\n8.4 Inserting a new heap tree node\n\nSince we always keep track of the last position n in the tree which has been filled so far, we\ncan easily insert a new element at position n+ 1, provided there is still room in the array,\nand increment n. The tree that results will still be a complete binary tree, but the heap tree\npriority ordering property might have been violated. Hence we may need to ‘bubble up’ the\nnew element into a valid position. This can be done easily by comparing its priority with\nthat of its parent, and if the new element has higher priority, then it is exchanged with its\nparent. We may have to repeat this process, but once we reach a parent that has higher or\nequal priority, we can stop because we know there can be no lower priority items further up\nthe tree. Hence an algorithm which inserts a new heap tree node with priority p is:\n\ninsert(int p, array heap, int n) f\nif ( n == MAX) {\nerror(‘Heap is full’)\nelse {\nheap[nt1] = p\nbubbleUp(n+1,heap,nt1)\n3\n3\n\nbubbleUp(int i, array heap, int n) {\nif ( isRoot(i) )\nreturn\nelseif ( heap[i] > heap[parent(i)] ) {\nswap heap[i] and heap[parent(i)]\nbubbleUp (parent (i) ,heap,n)\n\n\nNote that this insert algorithm does not increment the heap size n — that has to be done\nseparately by whatever algorithm calls it.\n\nInserting a node takes at most O(logy n) steps, because the maximum number of times we\nmay have to ‘bubble up’ the new element is the height of the tree which is logan.\n\n8.5 Deleting a heap tree node\n\nTo use a binary heap tree as a priority queue, we will regularly need to delete the root, i.e.\nremove the node with the highest priority. We will then be left with something which is not a\nbinary tree at all. However, we can easily make it into a complete binary tree again by taking\nthe node at the ‘last’ position and using that to fill the new vacancy at the root. However, as\nwith insertion of a new item, the heap tree (priority ordering) property might be violated. In\nthat case, we will need to ‘bubble down’ the new root by comparing it with both its children\nand exchanging it with the largest. This process is then repeated until the new root element\nhas found a valid place. Thus, a suitable algorithm is:\n\ndeleteRoot (array heap, int n) {\nif (n<1)\nerror(‘Node does not exist’)\nelse {\nheap[1] = heap[n]\nbubbleDown(1,heap,n-1)\n\n}\n\nA similar process can also be applied if we need to delete any other node from the heap tree,\nbut in that case we may need to ‘bubble up’ the shifted last node rather than bubble it down.\nSince the original heap tree is ordered, items will only ever need to be bubbled up or down,\nnever both, so we can simply call both, because neither procedure changes anything if it is\nnot required. Thus, an algorithm which deletes any node i from a heap tree is:\n\ndelete(int i, array heap, int n) {\nif (n<i)\nerror(‘Node does not exist’)\nelse {\nheap[i] = heap[n]\nbubbleUp(i,heap,n-1)\nbubbleDown(i,heap,n-1)\n}\n}\n\nThe bubble down process is more difficult to implement than bubble up, because a node\nmay have none, one or two children, and those three cases have to be handled differently. In\nthe case of two children, it is crucial that when both children have higher priority than the\ngiven node, it is the highest priority one that is swapped up, or their priority ordering will be\nviolated. Thus we have:\n\n\nbubbleDown(int i, array heap, int n) {\n\nif ( left(i) > n ) // no children\nreturn\nelseif ( right(i) > n ) // only left child\n\nif ( heap[i] < heap[left(i)] )\n\nswap heap[i] and heap[left(i)]\nelse // two children\n\nif ( heap[left(i)] > heap[right(i)] and heap[i] < heap[left(i)] ) {\nswap heap[i] and heap[left(i)]\nbubbleDown (left (i) ,heap,n)\n\n}\n\nelseif ( heap[i] < heap[right(i)] ) f{\nswap heap[i] and heap[right(i)]\nbubbleDown (right (i) ,heap,n)\n\n}\n\nIn the same way that the insert algorithm does not increment the heap size, this delete\nalgorithm does not decrement the heap size n — that has to be done separately by whatever\nalgorithm calls it. Note also that this algorithm does not attempt to be fair in the sense that\nif two or more nodes have the same priority, it is not necessarily the one that has been waiting\nlongest that will be removed first. However, this factor could easily be fixed, if required, by\nkeeping track of arrival times and using that in cases of equal priority.\n\nAs with insertion, deletion takes at most O(logg n) steps, because the maximum number\nof times it may have to bubble down or bubble up the replacement element is the height of\nthe tree which is loge n.\n\n8.6 Building a new heap tree from scratch\n\nSometimes one is given a whole set of n new items in one go, and there is a need to build\na binary heap tree containing them. In other words, we have a set of items that we wish to\nheapify. One obvious possibility would be to insert the n items one by one into a heap tree,\nstarting from an empty tree, using the O(log n) ‘bubble up’ based insert algorithm discussed\nearlier. That would clearly have overall time complexity of O(nlogg n).\n\nIt turns out, however, that rearranging an array of items into heap tree form can be done\nmore efficiently using ‘bubble down’. First note that, if we have the n items in an array a in\npositions 1,...,n, then all the items with an index greater than n/2 will be leaves, and not\nneed bubbling down. Therefore, if we just bubble down all the non-leaf items a[n/2],...,\na[1] by exchanging them with the larger of their children until they either are positioned at\na leaf, or until their children are both smaller, we obtain a valid heap tree.\n\nConsider a simple example array if items from which a heap tree must be built:\n\n5}/8)/3]/9]1)/4)7)6 | 2\n\nWe can start by simply drawing the array as a tree, and see that the last 5 entries (those with\nindices greater than 9/2 = 4) are leaves of the tree, as follows:\n\n56\n\n\n©)\n\nCre\n\nThen the rearrangement algorithm starts by bubbling down a{n/2] = a[4] = 9, which turns\nout not to be necessary, so the array remains the same. Next a[3] = 3 is bubbled down,\n\nswapping with a[7] = 7, giving:\n\n5/8) 7)9 4/3} 6| 2\n\nNext a[{2] = 8 is bubbled down, swapping with a[4] = 9, giving:\n\n5/9/7]8 4/3} 6] 2\n\nFinally, a{1] = 5 is bubbled down, swapping with a[2] = 9, to give first:\n\n9/5/7]8 4/3} 6] 2\n\nthen swapping with a[4] = 8 to give:\n\nand finally swapping with a[8] = 6 to give:\n\n9/8)/7)|6 4}3}5]2\n\nwhich has the array rearranged as the required heap tree.\n\nThus, using the above bubbleDown procedure, the algorithm to build a complete binary\n\nheap tree from any given array a of size n is simply:\n\nheapify(array a, int n) {\nfor( i=n/2;i>0; i-- )\nbubbleDown(i,a,n)\n}\n\nThe time complexity of this heap tree creation algorithm might be computed as follows: It\n\npotentially bubbles down |n/2| items, namely those with indices 1,..., |n\nnumber of bubble down steps for each of those items is the height of the t:\nand each step involves two comparisons — one to find the highest priority\nto compare the item with that child node. So the total number of com:\nat most (n/2).logyn.2 = nloggn, which is the same as we would have by\nitems one at a time into an initially empty tree.\n\nIn fact, this is a good example of a situation in which a naive countin\n\n/2|. The maximum\nree, which is logy n,\nchild node, and one\noarisons involved is\ninserting the array\n\nig of loops and tree\n\nheights over-estimates the time complexity. This is because the number o\n\n57\n\nbubble down steps\n\n\nwill usually be less than the full height of the tree. In fact, at each level as you go down the\ntree, there are more nodes, and fewer potential bubble down steps, so the total number of\noperations will actually be much less than nloggn. To be sure of the complexity class, we\nneed to perform a more accurate calculation. At each level i of a tree of height h there will be\n2' nodes, with at most h—i bubble down steps, each with 2 comparisons, so the total number\nof comparisons for a tree of height h will on average be\n\nh . h .\n. h—i j\nC(h) = $2 2\"(h- i) = 2\" > st gh\n\ni=0 i=0 j=0\n\nThe final sum converges to 2 as h increases (see Appendix A.4), so for large h we have\n\nand the worst case will be no more than twice that. Thus, the total number of operations\nis O(2'*+!) = O(n), meaning that the complexity class of heapify is actually O(n), which is\nbetter than the O(nlogg n) complexity of inserting the items one at a time.\n\n8.7 Merging binary heap trees\n\nFrequently one needs to merge two existing priority queues based on binary heap trees into\na single priority queue. To achieve this, there are three obvious ways of merging two binary\nheap trees s and t of a similar size n into a single binary heap tree:\n\n1. Move all the items from the smaller heap tree one at a time into the larger heap tree\nusing the standard insert algorithm. This will involve moving O(n) items, and each of\nthem will need to be bubbled up at cost O(logg n), giving an overall time complexity of\nO(nlog2 n).\n\n2. Repeatedly move the last items from one heap tree to the other using the standard\ninsert algorithm, until the new binary tree makeTree(0,t,s) is complete. Then move\nthe last item of the new tree to replace the dummy root “0”, and bubble down that new\nroot. How this is best done will depend on the sizes of the two trees, so this algorithm\nis not totally straightforward. On average, around half the items in the last level of one\ntree will need moving and bubbling, so that will be O(n) moves, each with a cost of\nO(loge n), again giving an overall time complexity of O(nloggn). However, the actual\nnumber of operations required will, on average, be a lot less than the previous approach,\nby something like a factor of four, so this approach is more efficient, even though the\nalgorithm is more complex.\n\n3. Simply concatenate the array forms of the heap trees s and t and use the standard\nheapify algorithm to convert that array into a new binary heap tree. The heapify\nalgorithm has time complexity O(n), and the concatenation need be no more than\nthat, so this approach has O(n) overall time complexity, making it in the best general\napproach of all three.\n\nThus, the merging of binary heap trees generally has O(n) time complexity.\n\n58\n\n\nIf the two binary heap trees are such that very few moves are required for the second\napproach, then that may look like a better choice of approach than the third approach.\nHowever, makeTree will itself generally be an O(n) procedure if the trees are array-based,\nrather than pointer-based, which they usually are for binary heap trees. So, for array-based\nsimilarly-sized binary heaps, the third approach is usually best.\n\nIf the heap trees to be merged have very different sizes n and m < n, the first approach\nwill have overall time complexity O(mlogn), which could be more efficient than an O(n)\napproach ifm <n. In practice, a good general purpose merge algorithm would check the\nsizes of the two trees and use them to determine the best approach to apply.\n\n8.8 Binomial heaps\n\nA Binomial heap is similar to a binary heap as described above, but has the advantage of\nmore efficient procedures for insertion and merging. Unlike a binary heap, which consists of\na single binary tree, a binomial heap is implemented as a collection of binomial trees.\n\nDefinition. A binomial tree is defined recursively as follows:\ne A binomial tree of order 0 is a single node.\n\ne A binomial tree of order k has a root node with children that are roots of binomial trees\nof orders k — 1, k — 2, ..., 2, 1, 0 (in that order).\n\nThus, a binomial tree of order k has height k, contains 2\" nodes, and is trivially constructed\nby attaching one order k—1 binomial tree as the left-most child of another order k—1 binomial\ntree. Binomial trees of order 0, 1, 2 and 3 take the form:\n\n0 2 3\n\nO OC\nO\n\nOO\nO\n\nand it is clear from these what higher order trees will look like.\nA Binomial heap is constructed as a collection of binomial trees with a particular structure\nand node ordering properties:\n\ne There can only be zero or one binomial tree of each order.\n\ne Each constituent binomial tree must satisfy the priority ordering property, i.e. each node\nmust have priority less than or equal to its parent.\n\n59\n\n\nThe structure of such a heap is easily understood by noting that a binomial tree of order k\ncontains exactly 2 nodes, and a binomial heap can only contain zero or one binomial tree of\neach order, so the total number of nodes in a Binomial Heap must be\n\nwhere b, specifies the number of trees of orc\n\nnumber of trees in a\n\nist of root nodes orc\nThe most import\na sub-process for mo\n\nin binary without leading zeros, i.e. logan 4\n\nfoe)\nn= > by2*\nk=0\n\nered\nant operation for binomia\nst other operations. Unde:\n\nbp € [0,1]\n\nheaps is merge, becau\n\ner k. Thus there is a one-to-one mapping between\nhe binomial heap structure and the standard binary representation of the number n, and since\nhe binary representation is clearly unique, so is the binomial heap structure. The maximum\n1eap with n nodes therefore equals the number of digits when n is written\n+ 1. The heap can be stored efficien\n\noy increasing tree order.\n\nly as a linked\n\nse that can be used as\n\nbinomial trees\n\nhose trees as the left mos\nordering simply requires t.\nroot of the combined tree.\n\nbinomial heaps is achieved\n\nsame order, in a sequentia'\ncase, the O(1) insert comp.\n\nof order j into a binomial tree of order j +\n\n1at it is the tree with the highest priority root tha\n\nexity will be multip\n\nrlying that is the merge of two\n. By definition, that is achieved by\nsub-tree of the roo\n\nadding one of\nof the other, and preservation of the priority\nprovides the\nThis clearly has O(1) time complexity. Then merging two whole\nby merging the constituent trees whenever there are two of the\nmanner analogous to the addition of two binary numbers. In this\nied by the number of trees, which is O(log n),\n\nso the overall time complexity of merge is O(logy n). This is better than the O(n) complexity\n\nof merging binary heaps that can be achieved\n\nO(n) heapify algorithm.\n\nInsertion of a new element into an existing binomial heap can easi\nthe new element as a binomial heap consisting of a single node (i.e.,\n\nand merging that using the\n\nvy concatenating the heap arrays and using the\n\ny be done by treating\nan order zero tree),\n\nstandard merge algorithm. The average time complexity of that\n\ninsert is given by computing the average number of O(1) tree combinations required. The\n\nprobability of needing the order zero combination is 0.5, the probabili\ncombination is 0.52, and the third is 0.5, and so\nO(1) overall time complexity. That is better\n\na standard binary heap.\n\nCreating a whole new binomial heap from scratch can be achieved b\nprocess for each of the n items, giving an overall time complexity of O\nis no better process, so heapify here has the same\n\nfor binary heaps.\n\ny of needing a second\none. So insertion has\nexity of insertion into\n\non, which sum to\nhe O(log2 n) comp.\n\nhan\n\ny using the O(1) insert\nn). In this case, there\nthe heapify algorithm\n\nime complexity as\n\nAnother important heap operation in practice is that of updating the heap after increas-\n\ning a node priori\nbubble-up process with O(\nmial heaps, and that will a\nThe highest priority noc\n\nand a pointer to that can be\n\ncomplexity of the operation\n\ny. For standard binary heaps, that simply requires a)\n\npplication of the usual\nogg n) complexity. Clearly, a similar process can be used in bino-\nso be of O(logs n) complexity.\n\ne in a binomial heap will clearly be the highest priority root node,\nmaintained by each heap update operation without increasing the\n. Serving the highest priority item requires deleting the highest\n\npriority node from the order j tree it appears in, and that will break it up into another\n\nbinomial heap consisting o\neasily be merged back into\n\nf trees of all orders from 0 to 7 — 1. However, those trees can\nthe original heap using the standard merge algorithm, with the\n\n60\n\n\nstandard merge complexity of O(logy). Deleting non-root nodes can also be achieved with\nthe existing operations by increasing the relevant node priority to infinity, bubbling-up, and\nusing the root delete operation, again with O(logzn) complexity overall. So, the complexity\nof delete is always O(logs n).\n\nExercise: Find pseudocode versions of the merge, insert and delete algorithms for binomial\nheaps, and see exactly how their time complexities arise.\n\n8.9 Fibonacci heaps\n\nA Fibonacci heap is another collection of trees that satisfy the standard priority-ordering\nproperty. It can be used to implement a priority queue in a similar way to binary or binomial\nheaps, but the structure of Fibonacci heaps are more flexible and efficient, which allows them\no have better time complexities. They are named after the Fibonacci numbers that restrict\nhe tree sizes and appear in their time complexity analysis.\n\nThe flexibility and efficiency of Fibonacci heaps comes at the cost of more complexity: the\nrees do not have a fixed shape, and in the extreme cases every element in the heap can be in\na separate tree. Normally, the roots of all the trees are stored using a circular doubly linked\nlist, and the children of each node are handled in the same way. A pointer to the highest\npriority root node is maintained, making it trivial to find the highest priority node in the\nheap. The efficiency is achieved by performing many operations in a lazy manner, with much\nof the work postponed for later operations to deal with.\n\nFibonacci heaps can easily be merged with O(1) complexity by simply concatenating the\nwo lists of root nodes, and then insertion can be done by merging the existing heap with a\nnew heap consisting only of the new node. By inserting n items one at a time, a whole heap\ncan be created from scratch with O(n) complexity.\n\nObviously, at some point, order needs to be introduced into the heap to achieve the overall\nefficiency. This is done by keeping the number of children of all nodes to be at most O(log2 n),\nand the size of a subtree rooted in a node with k children is at least Fy42, where F), is the\nkth Fibonacci number. The number of trees in the heap is decreased as part of the delete\noperation that is used to remove the highest priority node and update the pointer to the\nhighest priority root. This delete algorithm is quite complex. First it removes the highest\npriority root, leaving its children to become roots of new trees within the heap, the processing\nof which will be O(logg n). Then the number of trees is reduced by linking together trees that\nhave roots with the same number of children, similar to a Binomial heap, until every root\nhas a different number of children, leaving at most O(log2 n) trees. Finally the roots of those\ntrees are checked to reset the pointer to the highest priority. It can be shown that all the\nrequired processes can be completed with O(logg n) average time complexity.\n\nFor each node, a record is kept of its number of children and whether it is marked. The\n\nmark indicates that at least one of its children has been separated since the node was made\na child of another node, so all roots are unmarked. The mark is used by the algorithm for\nincreasing a node priority, which is also complex, but can be achieved with O(1) complexity.\nThis gives Fibonacci heaps an important advantage over both binary and binomial heaps for\nwhich this operation has O(logz n) time complexity.\n\nFinally, an arbitrary node can be deleted from the heap by increasing its node priority\nto infinity and applying the delete highest priority algorithm, resulting in an overall time\ncomplexity of O(logs n).\n\n61\n\n\nExercise: Find pseudocode versions of the various Fibonacci heap operations, and work\nout how Fibonacci numbers are involved in computing their time complexities.\n\n8.10 Comparison of heap time complexities\n\nIt is clear that the more complex Binomial and Fibonacci Heaps offer average time complexity\nadvantages over simple Binary Heap Trees. The following table summarizes the average time\ncomplexities of the crucial heap operations:\n\nHeap type Insert Delete Merge Heapify | Up priority\nBinary O(logz n) | O(logz n) O(n) O(n) O(log2 n)\nBinomial O(1) O(loggn) | O(loge n) O(n) O(log2 n)\nFibonacci O(1) O(log2 n) O(1) O(n) O(1)\n\nObviously it will depend on the application in question whether using a more complicated heap\nis worth the effort. We shall see later that Fibonacci heaps are important in practice because\nthey are used in the most efficient versions of many algorithms that can be implemented using\npriority queues, such as Dijkstra’s algorithm for finding shortest routes, and Prim’s algorithm\nfor finding minimal spanning trees.\n\n62\n\n\nChapter 9\n\nSorting\n\n9.1 The problem of sorting\n\nIn computer science, ‘sorting’ usually refers to bringing a set of items into some well-defined\norder. To be able to do this, we first need to specify the notion of order on the items we are\nconsidering. For example, for numbers we can use the usual numerical order (that is, defined\nby the mathematical ‘less than’ or ‘<’ relation) and for strings the so-called lexicographic or\nalphabetic order, which is the one dictionaries and encyclopedias use.\n\nUsually, what is meant by sorting is that once the sorting process is finished, there is\na simple way of ‘visiting’ all the items in order, for example to print out the contents of a\ndatabase. This may well mean different things depending on how the data is being stored.\nFor example, if all the objects are sorted and stored in an array a of size n, then\n\nfor i= 0,...,n-1\nprint (a[i])\n\nwould print the items in ascending order. If the objects are stored in a linked list, we would\nexpect that the first entry is the smallest, the next the second-smallest, and so on. Often,\nmore complicated structures such as binary search trees or heap trees are used to sort the\n\nmo\n\nems, which can then be printed, or written into an array or linked list, as desired.\n\nSorting is important because having the items in order makes it much easier to find a given\nitem, such as the cheapest item or the file corresponding to a particular student. It is thus\nclosely related to the problem of search, as we saw with the discussion of binary search tress.\nIf the sorting can be done beforehand (off-line), this enables faster access to the required\nitem, which is important because that often has to be done on the fly (on-line). We have\nalready seen that, by having the data items stored in a sorted array or binary search tree,\n\nwe can reduce the average (and worst case) complexity of searching for a particular item to\nO(logg n) steps, whereas it would be O(n) steps without sorting. So, if we often have to look\nup items, it is worth the effort to sort the whole collection first. Imagine using a dictionary\n\nor phone book in which the entries do not appear in some known logical order.\n\nIt follows that sorting algorithms are important tools for program designers. Different\nalgorithms are suited to different situations, and we shall see that there is no ‘best’ sorting\nalgorithm for everything, and therefore a number of them will be introduced in these notes. It\nis worth noting that we will be far from covering all existing sorting algorithms — in fact, the\nfield is still very much alive, and new developments are taking place all the time. However,\n\n63\n\n\nthe general strategies can now be considered to be well-understood, and most of the latest\nnew algorithms tend to be derived by simply tweaking existing principles, although we still\ndo not have accurate measures of performance for some sorting algorithms.\n\n9.2 Common sorting strategies\n\nOne way of organizing the various sorting algorithms is by classifying the underlying idea, or\n‘strategy’. Some of the key strategies are:\n\nenumeration sorting Consider all items. If we know that there are N items which are\nsmaller than the one we are currently considering, then its final\nposition will be at number N + 1.\n\nexchange sorting If two items are found to be out of order, exchange them. Repeat\ntill all items are in order.\n\nselection sorting Find the smallest item, put it in the first position, find the smallest\nof the remaining items, put it in the second position ...\n\ninsertion sorting Take the items one at a time and insert them into an initially\nempty data structure such that the data structure continues to be\nsorted at each stage.\n\ndivide and conquer Recursively split the problem into smaller sub-problems till you\njust have single items that are trivial to sort. Then put the sorted\n‘parts’ back together in a way that preserves the sorting.\n\nAll these strategies are based on comparing items and then rearranging them accordingly.\nThese are known as comparison-based sorting algorithms. We will later consider other non-\ncomparison-based algorithms which are possible when we have specific prior knowledge about\nthe items that can occur, or restrictions on the range of items that can occur.\n\nThe ideas above are based on the assumption that all the items to be sorted will fit into\nthe computer’s internal memory, which is why they are often referred to as being internal\nsorting algorithms. If the whole set of items cannot be stored in the internal memory at one\ntime, different techniques have to be used. These days, given the growing power and memory\nof computers, external storage is becoming much less commonly needed when sorting, so we\nwill not consider external sorting algorithms in detail. Suffice to say, they generally work by\nsplitting the set of items into subsets containing as many items as can be handled at one time,\nsorting each subset in turn, and then carefully merging the results.\n\n9.3. How many comparisons must it take?\n\nAn obvious way to compute the time complexity of sorting algorithms is to count the number of\ncomparisons they need to carry out, as a function of the number of items to be sorted. There\nis clearly no general upper bound on the number of comparisons used, since a particularly\nstupid algorithm might compare the same two items indefinitely. We are more interested in\nhaving a lower bound for the number of comparisons needed for the best algorithm in the\nworst case. In other words, we want to know the minimum number of comparisons required\n\n64\n\n\nto have all the information needed to sort an arbitrary collection of items. Then we can see\nhow well particular sorting algorithms compare against that theoretical lower bound.\n\nIn general, questions of this kind are rather hard, because of the need to consider all pos-\nsible algorithms. In fact, for some problems, optimal lower bounds are not yet known. One\nimportant example is the so-called Travelling Salesman Problem (TSP), for which all algo-\nrithms, which are known to give the correct shortest route solution, are extremely inefficient in\nhe worst case (many to the extent of being useless in practice). In these cases, one generally\nhas to relax the problem to find solutions which are probably approximately correct. For the\nTSP, it is still an open problem whether there exists a feasible algorithm that is guaranteed\n0 give the exact shortest route.\n\nFor sorting algorithms based on comparisons, however, it turns out that a tight lower\nsound does exist. Clearly, even if the given collection of items is already sorted, we must still\ncheck all the items one at a time to see whether they are in the correct order. Thus, the lower\nsound must be at least n, the number of items to be sorted, since we need at least n steps to\nexamine every element. If we already knew a sorting algorithm that works in n steps, then\nwe could stop looking for a better algorithm: n would be both a lower bound and an upper\nsound to the minimum number of steps, and hence an exact bound. However, as we shall\nshortly see, no algorithm can actually take fewer than O(nloggn) comparisons in the worst\ncase. If, in addition, we can design an algorithm that works in O(nlogg n) steps, then we will\nhave obtained an exact bound. We shall start by demonstrating that every algorithm needs\nat least O(nlogg n) comparisons.\n\nTo begin with, let us assume that we only have three items\nthat i < j and j < k, then we know that the sorted order is\n\nj, and k. If we have found\n, Jj, k. So it took us two\ncomparisons to find this out. In some cases, however, it is clear that we will need as many as\nthree comparisons. For example, if the first two comparisons tell us that i > j and j < k, then\nwe know that j is the smallest of the three items, but we cannot say from this information\nhow # and k relate. A third comparison is needed. So what is the average and worst number\nof comparisons that are needed? This can best be determined from the so-called decision tree,\nwhere we keep track of the information gathered so far and count the number of comparisons\nneeded. The decision tree for the three item example we were discussing is:\n\ni<=k<=j k<si<=j j<Hk<=i k<=j<si\n\nSo what can we deduce from this about the general case? The decision tree will obviously\nalways be a binary tree. It is also clear that its height will tell us how many comparisons will\nbe needed in the worst case, and that the average length of a path from the root to a leaf\nwill give us the average number of comparisons required. The leaves of the decision tree are\n\n65\n\n\nall the possible outcomes. These are given by the different possible orders we can have on n\nitems, so we are asking how many ways there are of arranging n items. The first item can\nbe any of the n items, the second can be any of the remaining n — 1 items, and so forth, so\ntheir total number is n(n — 1)(n — 2)---3-2-1=n!. Thus we want to know the height h of\na binary tree that can accommodate as many as n! leaves. The number of leaves of a tree of\nheight h is at most 2”, so we want to find h such that\n\n2°>n! or h > logs (n!)\n\nThere are numerous approximate expressions that have been derived for logs (n!) for large n,\nout they all have the same dominant term, namely nloggn. (Remember that, when talking\nabout time complexity, we ignore any sub-dominant terns and constant factors.) Hence,\nno sorting algorithm based on comparing items can have a better average or worst case\nperformance than using a number of comparisons that is approximately nlogg n for large n. It\nremains to be seen whether this O(nlogg n) complexity can actually be achieved in practice.\nTo do this, we would have to exhibit at least one algorithm with this performance behaviour\nand convince ourselves that it really does have this behaviour). In fact, we shall shortly see\nhat there are several algorithms with this behaviour.\n\nWe shall proceed now by looking in turn at a number of sorting algorithms of increasing\nsophistication, that involve the various strategies listed above. The way they work depends\non what kind of data structure contains the items we wish to sort. We start with approaches\nhat work with simple arrays, and then move on to using more complex data structures that\nead to more efficient algorithms.\n\n9.4 Bubble Sort\n\nBubble Sort follows the exchange sort approach. It is very easy to implement, but tends to\nbe particularly slow to run. Assume we have array a of size n that we wish to sort. Bubble\nSort starts by comparing a[n-1] with a[n-2] and swaps them if they are in the wrong order.\nIt then compares a[n-2] and a[n-3] and swaps those if need be, and so on. This means\nthat once it reaches a[0], the smallest entry will be in the correct place. It then starts from\nthe back again, comparing pairs of ‘neighbours’, but leaving the zeroth entry alone (which is\nknown to be correct). After it has reached the front again, the second-smallest entry will be\nin place. It keeps making ‘passes’ over the array until it is sorted. More generally, at the ith\nstage Bubble Sort compares neighbouring entries ‘from the back’, swapping them as needed.\nThe item with the lowest index that is compared to its right neighbour is afi-1]. After the\nith stage, the entries a[0],...,a[i-1] are in their final position.\n\nAt this point it is worth introducing a simple ‘test-case’ of size n = 4 to demonstrate how\nthe various sorting algorithms work:\n\nBubble Sort starts by comparing a[3]=2 with a[2]=3. Since they are not in order, it swaps\nthem, giving) 4} 1 | 2 | 3 | It then compares a[2]=2 with a[1]=1. Since those are in order,\nit leaves them where they are. Then it compares a[1]=1 with a[0]=4, and those are not in\norder once again, so they have to be swapped. We get | 1 | 4 | 2 | 3 |. Note that the smallest\nentry has reached its final place. This will always happen after Bubble Sort has done its first\n\n‘pass’ over the array.\n\n66\n\n\nNow that the algorithm has reached the zeroth entry, it starts at the back again, comparing\na[3]=3 with a[2]=2. These entries are in order, so nothing happens. (Note that these\nnumbers have been compared before — there is nothing in Bubble Sort that prevents it from\nrepeating comparisons, which is why it tends to be pretty slow!) Then it compares a[2]=2\nand a[1]=4. These are not in order, so they have to be swapped, giving] 1 | 2 | 4 | 3 F Since\nwe already know that a[0] contains the smallest item, we leave it alone, and the second pass\nis finished. Note that now the second-smallest entry is in place, too.\n\nThe algorithm now starts the third and final pass, comparing a[3]=3 and a[2]=4. Again\nthese are out of order and have to be swapped, giving] 1 | 2 | 3 | 4 |. Since it is known that\na[0] and a[1] contain the correct items already, they are not touched. Furthermore, the\nthird-smallest item is in place now, which means that the fourth-smallest has to be correct,\ntoo. Thus the whole array is sorted.\n\nIt is now clear that Bubble Sort can be implemented as follows:\n\nfor (i=1; i<n 3; i++ )\nfor (j =n-1; j >= i; j--)\nif ( alj] < alj-1] )\nswap a[j] and a[j-1]\n\nThe outer loop goes over all n— 1 positions that may still need to be swapped to the left, and\nthe inner loop goes from the end of the array back to that position.\n\nAs is usual for comparison-based sorting algorithms, the time complexity will be measured\nby counting the number of comparisons that are being made. The outer loop is carried out\nn — 1 times. The inner loop is carried out (n — 1) — (i — 1) = n —i times. So the number of\ncomparisons is the same in each case, namely\n\nn—-In-1 n-1\n\nevi = Yi(n- 3)\n\ni=l j=i i=l\n(n—1)+(n—2)+---4+1\nn(n —1)\na\n\nThus the worst case and average case number of comparisons are both proportional to n?,\nand hence the average and worst case time complexities are O(n”).\n\n9.5 Insertion Sort\n\nInsertion Sort is (not surprisingly) a form of insertion sorting. It starts by treating the first\nentry a[0] as an already sorted array, then checks the second entry a[1] and compares it with\nthe first. If they are in the wrong order, it swaps the two. That leaves a[0],a[1] sorted.\nThen it takes the third entry and positions it in the right place, leaving a[0],a[1],a[2]\nsorted, and so on. More generally, at the beginning of the ith stage, Insertion Sort has the\nentries a[0],..., ali-1] sorted and inserts a[i], giving sorted entries a[0],...,a[i].\nFor the example starting array| 4 | 1 | 3 | 2 |, Insertion Sort starts by considering a[0]=4\nas sorted, then picks up a[1] and ‘inserts it’ into the already sorted array, increasing the size\nof it by 1. Since a[1]=1 is smaller than a[0]=4, it has to be inserted in the zeroth slot,\n\n67\n\n\nout that slot is holding a value already. So we first move a[0] ‘up’ one slot into a[1] (care\nyeing taken to remember a[1] first!), and then we can move the old a[1] to a0], giving\n1)4)3)2)\n\nAt the next step, the algorithm treats a[0] ,a[1] as an already sorted array and tries to\ninsert a[2]=3. This value obviously has to fit between a[0]=1 and a[1]=4. This is achieved\nby moving a[1] ‘up’ one slot to a[2] (the value of which we assume we have remembered),\nallowing us to move the current value into a[1], giving | 1/3)4)2 ,\n\nFinally, a[3]=2 has to be inserted into the sorted array a[0],...,a[2]. Since a[2]=4 is\nnigger than 2, it is moved ‘up’ one slot, and the same happens for a[1]=3. Comparison with\na[0]=1 shows that a[1] was the slot we were looking for, giving | 2/3/44\nThe general algorithm for Insertion Sort can therefore be written:\n\nfor (i=1;i<n; i++) {\nfor( j =i; j>0; j--)\nif ( alj] < alj-1] )\nswap a[j] and a[j-1]\nelse break\n}\n\nThe outer loop goes over the n — 1 items to be inserted, and the inner loop takes each next\nitem and swaps it back through the currently sorted portion till it reaches its correct position.\nHowever, this typically involves swapping each next item many times to get it into its right\nposition, so it is more efficient to store each next item in a temporary variable t and only\ninsert it into its correct position when that has been found and its content moved:\n\nfor (i=1;i<n; i++) {\n\njai\nt = aljl\nwhile ( j > 0 && t < alj-1] ) {\nalj] = alj-1]\nj--\n}\nalj] =t\n\n}\n\nThe outer loop again goes over n—1 items, and the inner loop goes back through the currently\nsorted portion till it finds the correct position for the next item to be inserted.\n\nThe time complexity is again taken to be the number of comparisons performed. The\nouter loop is always carried out n — 1 times. How many times the inner loop is carried out\ndepends on the items being sorted. In the worst case, it will be carried out 7 times; on average,\nit will be half that often. Hence the number of comparison in the worst case is:\n\nn-1 7 n-1\nLyi = »!\ni=\n\ni=1 j=l\n\n\nand in the average case it is half that, namely n(n — 1)/4. Thus average and worst case\nnumber of steps for of Insertion Sort are both proportional to n?, and hence the average and\nworst case time complexities are both O(n?).\n\n9.6 Selection Sort\n\nSelection Sort is (not surprisingly) a form of selection sorting. It first finds the smallest item\nand puts it into a[0] by exchanging it with whichever item is in that position already. Then\nit finds the second-smallest item and exchanges it with the item in a[1]. It continues this\nway until the whole array is sorted. More generally, at the ith stage, Selection Sort finds the\nith-smallest item and swaps it with the item in a[i-1]. Obviously there is no need to check\nfor the ith-smallest item in the first i — 1 elements of the array.\n\nFor the example starting array | 4/1)3]2 , Selection Sort first finds the smallest item in\nthe whole array, which is a[1]=1, and swaps this value with that in a[0] giving| 1} 4)|34)2 F\nThen, for the second step, it finds the smallest item in the reduced array a[1] ,a[2],a[3],\nthat is a[3]=2, and swaps that into a[1], giving] 1 | 2 | 3 | 4 |. Finally, it finds the smallest\nof the reduced array a[2] ,a[3], that is a[2]=3, and swaps that into a[2], or recognizes that\na swap is not needed, giving] 1 | 2 | 3 | 4 |.\n\nThe general algorithm for Selection Sort can be written:\n\nfor (i=0; i<n-1; itt) f{\nk=i\nfor ( j = itt 5; j <n 5 j++)\nif ( alj] < alk] )\nk= j\nswap a[i] and a[k]\n}\n\nThe outer loop goes over the first n — 1 positions to be filled, and the inner loop goes through\nthe currently unsorted portion to find the next smallest item to fill the next position. Note\nthat, unlike with Bubble Sort and Insertion Sort, there is exactly one swap for each iteration\nof the outer loop,\n\nThe time complexity is again the number of comparisons carried out. The outer loop is\ncarried out n — 1 times. In the inner loop, which is carried out (n — 1) —i =n— 1 —i times,\none comparison occurs. Hence the total number of comparisons is:\n\nn—-2 n-1 n—-2\n\nYY 1 = VMmr-1-4\n\ni=0 j=it1 i=0\n= (n—-1)4+---4+241\n— n(n—1)\n=:\n\nTherefore the number of comparisons for Selection Sort is proportional to n?, in the worst\ncase as well as in the average case, and hence the average and worst case time complexities\nare both O(n?).\n\nNote that Bubblesort, Insertion Sort and Selection Sort all involve two nested for loops\nover O(n) items, so it is easy to see that their overall complexities will be O(n?) without\nhaving to compute the exact number of comparisons.\n\n69\n\n\n9.7 Comparison of O(n) sorting algorithms\n\nWe have now seen three different array based sorting algorithms, all based on different sorting\nstrategies, and all with O(n?) time complexity. So one might imagine that it does not make\n\nmuch difference which of these algorithms is usec\na big difference which algorithm is chosen. The\ntimes of the three algorithms app\n\nollowing\nied to arrays 0:\n\ntable shows the\n\n. However, in practice, it can actually make\n\nmeasured running\n\nintegers of the size given in the top row:\n\nAlgorithm 128 256 512 | 1024 | 01024 | R1024 | 2048\nBubble Sort 54 221 881 | 3621 1285 5627 | 14497\nInsertion Sort 15 69 276 | 1137 6 2200 | 4536\nSelection Sort 12 45 164 634 643 833 | 2497\n\nHere 01024 denotes an array with 1024\narray which is sorted in the reverse order, that is, from biggest to smal\narrays were filled randomly. Warning: ta\n\nentries which are already sorted, and R1024 is an\n\nest. All the other\n\ndles of measurements like this are always dependent\n\non the random ordering used, the implementation of the programming language involved, and\n\non the machine it was run on, and so wil\nSo where exactly do\n\nhese differences come from?\n\nnever be exactly the same.\nFor a start, Selection Sort always\n\nmakes n(n —1)/2 comparisons, but carries out at most n—1 swaps. Each swap requires three\n\nassignments and\n\ndoes a lot of swaps. Inser\n\ntakes, in\n\nfact, more time than a comparison. Bubble Sort, on the other hand,\nion Sort does particularly well on data which is sorted already\n\nin such a case, it only makes n — 1 comparisons. It is worth bearing this in mind for some\n\napplications, because if on\nThese comparisons serve\n\nrequire good juc\nsome experimen\n\nmade are realistic in prac\n\ns to test\n\nly a few entries are out of place, Insertion Sort can be very quick.\no show that complexity considerations can be rather delicate, and\ngement concerning what operations to count. It is often a good idea to run\nhe theoretical considerations and see whether any simplifications\nice. For instance, we have assumed here that a\n\n1 comparisons cost\n\nthe same, but that may not be true for big numbers or strings of characters.\n\nWhat exactly to count\n\na judgement cal\n\nsuch decisions yourself.\nalgorithm, you may want\n\nimproving that.\nnot a bad idea\n\n. You wi\nFurthermore, when you want\n\nSomething else to be aware of when ma\no keep track of any constant factors, in\n\ndominating sub-\n\nnamely n?, varies. It is 1/2 for the average case of Bubble Sort and Selec\n1/4 for Insertion Sort. It is certainly usefu\n\nerm. In the above examples, the factor a\n\nperform better than a quadratic one provided the size of\n\nyou know that your problem has a size of, s:\n\nay, at most\n\no determine the biggest user of computing reso\n\no improve the\n\nking these calcw\n\nwhen considering the complexity of a particular algorithm is always\nhave to gain experience before you feel comfortable with making\n\nperformance of an\nurces and focus on\nations is that it is\n\nparticular those that go with the\npplied to the dominating sub-term,\n\nion Sort, but only\n\nto know that an algorithm that is linear will\nthe problem is large enough, but if\n00, then a complexity of (1/20)n?\n\nwill be preferable to one of 20n. Or if you know that your program is only ever used on fairly\n\nsmall samples, then using the simplest algori\nis easier to program, and there is not a lot o:\n\nhm you can find might be beneficial overall\nf compute time to be saved.\n\nit\n\nFinally, the above numbers give you some idea why, for program designers, the general\nrule is to never use Bubble Sort. It is certainly easy to program, but that is about all it has\n\ngoing for it. You are better off avoiding it al\n\nogether.\n\n70\n\n\n9.8 Sorting algorithm stability\n\nOne often wants to sort items which might have identical keys (e.g., ages in years) in such a\nway that items with identical keys are kept in their original order, particularly if the items\nhave already been sorted according to a different criteria (e.g., alphabetical). So, if we denote\nthe original order of an array of items by subscripts, we want the subscripts to end up in\norder for each set of items with identical keys. For example, if we start out with the array\n(51, Ao, 63, 5a, 65, 76; 57, 28, 99], it should be sorted to [2s, Ao, 51, 5a, 57, 63, 65, 76, 99] and not to\n[28, 42, 54, 51, 57, 63, 65, 76, 99]. Sorting algorithms which satisfy this useful property are said\nto be stable.\n\nThe easiest way to determine whether a given algorithm is stable is to consider whether\n\nthe algorithm can ever swap identical items past each other. In this way, the stability of the\nsorting algorithms studied so far can easily be established:\n\nBubble Sort This is stable because no item is swapped past another unless they are\nin the wrong order. So items with identical keys will have their original\n\norder preserved.\n\nInsertion Sort This is stable because no item is swapped past another unless it has a\nsmaller key. So items with identical keys will have their original order\npreserved.\n\nSelection Sort This is not stable, because there is nothing to stop an item being swapped\n\npast another item that has an identical key. For example, the array\n[21, 22, 13] would be sorted to [13, 22,21] which has items 2) and 2; in the\nwrong order.\n\nThe issue of sorting stability needs to be considered when developing more complex sorting\nalgorithms. Often there are stable and non-stable versions of the algorithms, and one has to\nconsider whether the extra cost of maintaining stability is worth the effort.\n\n9.9 ‘Treesort\n\nLet us now consider a way of implementing an insertion sorting algorithm using a data\nstructure better suited to the problem. The idea here, which we have already seen before,\ninvolves inserting the items to be sorted into an initially empty binary search tree. Then,\nwhen all items have been inserted, we know that we can traverse the binary search tree to\nvisit all the items in the right order. This sorting algorithm is called Treesort, and for the\nbasic version, we require that all the search keys be different.\n\nObviously, the tree must be kept balanced in order to minimize the number of comparisons,\nsince that depends on the height of the tree. For a balanced tree that is O(log2 n). If the tree\nis not kept balanced, it will be more than that, and potentially O(n).\n\nTreesort can be difficult to compare with other sorting algorithms, since it returns a tree,\nrather than an array, as the sorted data structure. It should be chosen if it is desirable\n\nto have the items stored in a binary search tree anyway. This is usually the case if items\n\nare frequently deleted or inserted, since a binary search tree allows these operations to be\nimplemented efficiently, with time complexity O(logg n) per item. Moreover, as we have seen\nbefore, searching for items is also efficient, again with time complexity O(logs n).\n\n71\n\n\nEven if we have an array of items to start with, and want to finish with a sorted array,\nwe can still use Treesort. However, to output the sorted items into the original array, we will\nneed another procedure fillArray(tree t,array a, int j) to traverse the tree t and fill the\narray a. That is easiest done by passing and returning an index j that keeps track of the next\narray position to be filled. This results in the complete Treesort algorithm:\n\ntreeSort (array a) {\nt = EmptyTree\nfor (i=0; i < size(a) ; i++ )\nt = insert (a[i] ,t)\nfillarray(t,a,0)\n}\n\nfillArray(tree t, array a, int j) {\nif ( not isEmpty(t) ) {\nj = fillArray(left(t),a,j)\nalj+t] = root(t)\nj = fillArray(right(t),a,j)\n3\nreturn j\n\n}\n\nwhich assumes that a is a pointer to the array location and that its elements can be accessed\nand updated given that and the relevant array index.\n\nSince there are n items to insert into the tree, and each insertion has time complexity\nO(logg n), Treesort has an overall average time complexity of O(nlogy n). So, we already have\none algorithm that achieves the theoretical best average case time complexity of O(nlog n).\nNote, however, that if the tree is not kept balanced while the items are being inserted, and\nthe items are already sorted, the height of the tree and number of comparisons per insertion\nwill be O(n), leading to a worst case time complexity of O(n”), which is no better than the\nsimpler array-based algorithms we have already considered.\n\nExercise: We have assumed so far that the items stored in a Binary Search Tree must not\ncontain any duplicates. Find the simplest ways to relax that restriction and determine how\nthe choice of approach affects the stability of the associated Treesort algorithm.\n\n9.10 Heapsort\n\nWe now consider another way of implementing a selection sorting algorithm using a more\nefficient data structure we have already studied. The underlying idea here is that it would\nhelp if we could pre-arrange the data so that selecting the smallest/biggest entry becomes\neasier. For that, remember the idea of a priority queue discussed earlier. We can take the\nvalue of each item to be its priority and then queue the items accordingly. Then, if we remove\nthe item with the highest priority at each step we can fill an array in order ‘from the rear’,\nstarting with the biggest item.\n\nPriority queues can be implemented in a number of different ways, and we have already\nstudied a straightforward implementation using binary heap trees in Chapter 8. However,\nthere may be a better way, so it is worth considering the other possibilities.\n\n72\n\n\nAn obvious way of implementing them would be using a sorted array, so that the entry\n\nwith the highest priority appears in a[n].\ninserting a new item would always invo\nitems to the right to make room for it.\n\nem would be very simple, but\n\nhe right position and shifting a number of\n\nFor example, inserting a 3 into the queue [1, 2, 4]:\n\nThat kind of item insertion is effectively insertion sort and clearly inefficient in general, of\n\nO(n) complexity ra:\n\nAnother approach would be\ning it into a{n+1], but\ninvolve having to find it first. Then, after\n\ninserted by just put\n\nthe gap, or\nclearly inefficient in\nThus, of those\n\nthe array is\nTo make use of\nit so that i\n\nsatisfies t\n\nalways in a[1]. In the\nthe two, we will have\n\nwould be moved into t\n\nitem, we wi\n\nand bring them back int\nwhich we know to have\n\nNow the second largest\nso we now swap these two i\n\nusing the bub\n\nentries, and there will\n\nand therefore\n\nall items with a hig\ngeneral, of O(n) comp\nhree representations, only one is of use in carrying out the al\narted from, so that is not any help, anc\nwhat we are trying to achieve, so heaps are the way forward.\n\ninary heap trees, we first have to take the unsorted array\n1e heap tree priority ordering. We have already stud\nalgorithm which can do that with O(n) time complexity. Then we need to ex\narray from it. In the heap tree, the item with the highest priori\nsorted array, it should be in the last posi\n1at item at the right position of the array, and also have\n\nefficiently. An unsortec\n\narray is what we s\n\n1 never have\n\nle down\n\nthe height, o\n\nto\noa\ncomplexity O(log2 n).\n\nRemoving this i\nve finding\nn O0);1)2)3),4)]5\na(n] 2)4\nn O0);1)2)3),4)]5\na[n] |) 1 | 2 4\nn 0 2)3/4)5\nafm] |} 1/2/34\nher than O(logy n) with a binary hea:\n\nstandard procedure of removing the root of the heap-tree, since a[n] is precise\n1e root position at the next step. Since a\nook at it again. Instead, we just take\nheap-tree form using the bubble down procedure on the new root,\n\nitem is in position a[1], and its final\nems. Then we rearrange a[1],...\nprocedure on the new root. And so on.\nWhen the ith step has been completed, the items a[n-it+1],...\nye a heap tree for the items a[1],...\n‘ the heap tree decreases at each step. As a part of the ith step,\n\n> tree.\n\nract\ny, that is the\n\nto use an unsorted array. In this case, a new item would be\nto delete the entry with the highest priority would\nhat, the last item would have to be swapped into\nner index ‘shifted down’. Again, that kind of item c\nexity rather than O(logg n) with a heap\n\neletion is\n\ntree.\n\nove idea\nordering\n\nand re-arrange\nied the heapify\n\nhe sorted\n\nlargest item, is\nion a[n]. If we simply swap\n\ny the\n\nthe items a[1],..\n\nposition should b\n\nyegun the\nitem that\n\n[n] now contains the correct\n\n.,aln-1]\n\ne a[n-1],\n\n,a[n-2] back into a heap tree\n\n,a[n] will have the correct\n\n,atn-i]. Note that the size,\n\nwe have to bubble down the new root. This will take at most twice as many comparisons as\n\nthe height of\n\nmost 2logy mn comparisons,\n\nhe original heap tree, which is logy n. So overall there are n — 1 steps, with at\notalling 2(n — 1)logyn. The number of comparisons will actually\n\nbe less than that, because the number of bubble down steps will usually be less than the full\n\nheight of the\n\nree, but usually not much les\n\n30 the time complexity is still O(nlog2 n).\n\nThe full Heapsort algorithm can thus be written in a very simple form, using the bubble\ndown and heapify procedures we already have from Chapter 8. First heapify converts the\n\n73\n\n\narray into a binary heap tree, and then the for loop moves each successive root one item at a\ntime into the correct position in the sorted array:\n\nheapSort(array a, int n) {\nheapify(a,n)\nfor(jan;j>1; j--) f\nswap a[i] and al[j]\nbubbleDown(1,a, j-1)\n}\n}\n\nIt is clear from the swap step that the order of identical items can easily be reversed, so there\nis no way to render the Heapsort algorithm stable.\n\nThe average and worst-case time complexities of the entire Heapsort algorithm are given\nby the sum of two complexity functions, first that of heapify rearranging the original unsorted\narray into a heap tree which is O(n), and then that of making the sorted array out of the\nheap tree which is O(nlogz n) coming from the O(n) bubble-downs each of which has O(log n)\ncomplexity. Thus the overall average and worst-case complexities are both O(nlogz n), and\nwe now have a sorting algorithm that achieves the theoretical best worst-case time complex-\nity. Using more sophisticated priority queues, such as Binomial or Fibonacci heaps, cannot\nimprove on this because they have the same delete time complexity.\n\nA useful feature of Heapsort is that if only the largest m <n items need to be found and\nsorted, rather than all n, the complexity of the second stage is only O(mlog2n), which can\neasily be less than O(n) and thus render the whole algorithm only O(n).\n\n9.11 Divide and conquer algorithms\n\nAll the sorting algorithms considered so far work on the whole set of items together. Instead,\ndivide and conquer algorithms recursively split the sorting problem into more manageable\nsub-problems. The idea is that it will usually be easier to sort many smaller collections of\nitems than one big one, and sorting single items is trivial. So we repeatedly split the given\ncollection into two smaller parts until we reach the ‘base case’ of one-item collections, which\nrequire no effort to sort, and then merge them back together again. There are two main\napproaches for doing this:\n\nAssuming we are working on an array a of size n with entries a[0],...,a[n-1], then the\nobvious approach is to simply split the set of indices. That is, we split the array at item n/2\nand consider the two sub-arrays a[0],...,af(n-1)/2] and a[(n+1)/2],...,a[n-1]. This\nmethod has the advantage that the splitting of the collection into two collections of equal (or\nnearly equal) size at each stage is easy. However, the two sorted arrays that result from each\nsplit have to be merged together carefully to maintain the ordering. This is the underlying\nidea for a sorting algorithm called mergesort.\n\nAnother approach would be to split the array in such a way that, at each stage, all the items\nin the first collection are no bigger than all the items in the second collection. The splitting\nhere is obviously more complex, but all we have to do to put the pieces back together again\nat each stage is to take the first sorted array followed by the second sorted array. This is the\nunderlying idea for a sorting algorithm called Quicksort.\n\nWe shall now look in detail at how these two approaches work in practice.\n\n74\n\n\n9.12 Quicksort\n\nThe general idea here is to repeatedly split (or partition) the given array in such a way that\nall the items in the first sub-array are smaller than all the items in the second sub-array, and\nthen concatenate all the sub-arrays to give the sorted full array.\n\nHow to partition. The important question is how to perform this kind of splitting most\nefficiently. If the array is very simple, for example [4,3,7,8,1,6], then a good split would be to\nput all the items smaller than 5 into one part, giving [4,3,1], and all the items bigger than\nor equal to 5 into the other, that is [7,8,6]. Indeed, moving all items with a smaller key than\nsome given value into one sub-array, and all entries with a bigger or equal key into the other\nsub-array is the standard Quicksort strategy. The value that defines the split is called the\npivot. However, it is not obvious what is the best way to choose the pivot value.\n\nOne situation that we absolutely have to avoid is splitting the array into an empty sub-\narray and the whole array again. If we do this, the algorithm will not just perform badly, it\nwill not even terminate. However, if the pivot is chosen to be an item in the array, and the\npivot is kept in between and separate from both sub-arrays, then the sub-arrays being sorted\nat each recursion will always be at least one item shorter than the previous array, and the\nalgorithm is guaranteed to terminate.\n\nThus, it proves convenient to split the array at each stage into the sub-array of values\nsmaller than or equal to some chosen pivot item, followed by that chosen pivot item, followed\nby the sub-array of values greater than or equal to the chosen pivot item. Moreover, to save\n\nspace, we do not actually split the array into smaller arrays. Instead, we simply rearrange the\nwhole array to reflect the splitting. We say that we partition the array, and the Quicksort\nalgorithm is then applied to the sub-arrays of this partitioned array.\n\nIn order for the algorithm to be called recursively, to sort ever smaller parts of the original\narray, we need to te\nQuicksort is called giving the lowest index (left) and highest index (right) of the sub-array\nit must work on. Thus the algorithm takes the form:\n\nBH:\n\nwhich part of the array is currently under consideration. Therefore,\n\nquicksort(array a, int left, int right) {\nif ( left < right ) {\npivotindex = partition(a,left,right)\nquicksort (a, left ,pivotindex-1)\nquicksort (a, pivotindex+1,right)\n\n}\n\nor which the initial call would be quicksort (a,0,n-1) and the array a at the end is sorted.\nThe crucial part of this is clearly the partition(a,left,right) procedure that rearranges\nhe array so that it can be split around an appropriate pivot a[pivotindex].\n\nIf we were to split off only one item at a time, Quicksort would have n recursive calls,\nwhere n is the number of items in the array. If, on the other hand, we halve the array at each\nstage, it would only need logy n recursive calls. This can be made clear by drawing a binary\nree whose nodes are labelled by the sub-arrays that have been split off at each stage, and\nmeasuring its height. Ideally then, we would like to get two sub-arrays of roughly equal size\nnamely half of the given array) since that is the most efficient way of doing this. Of course,\nhat depends on choosing a good pivot.\n\n75\n\n\nChoosing the pivot. If we get the pivot ‘just right’ (e.g., choosing 5 in the above example),\nthen the split will be as even as possible. Unfortunately, there is no quick guaranteed way\nof finding the optimal pivot. If the keys are integers, one could take the average value of all\nthe keys, but that requires visiting all the entries to sample their key, adding considerable\noverhead to the algorithm, and if the keys are more complicated, such as strings, you cannot\ndo this at all. More importantly, it would not necessarily give a pivot that is a value in the\n\narray.\n\nSome sensible heuristic pivot choice strategies are:\n\ne Use a random number generator to produce an index k and then use a[k].\n\ne Take a key from ‘the middle’ of the array, that is a[(n-1)/2].\n\ne Take a small sample (e.g., 3 or 5 items) and take the ‘middle’ key of those.\n\nNote that one should never simply choose the first or last key in the array as the pivot, because\nif the array is almost sorted already, that will lead to the particularly bad choice mentioned\n\nabove,\n\nand this situation is actually quite common in practice.\n\nSince there are so many reasonable possibilities, and they are all fairly straightforward,\nwe will not give a specific implementation for any of these pivot choosing strategies, but just\nassume that we have a choosePivot(a,left,right) procedure that returns the index of the\n\npivot for a particular sub-array (rather than the pivot value itself).\n\nan exa\n\nhe rig\nIn the\n\nsepara\n\npascal,\n\nwe mo\nrom\n\ndasic,\n\nasic |\n\nis bigg\n\nproceeds to investigate the items in the array from two sides.\nWe begin by swapping the pivot value to the end of the array where it can easily be kep\n\nhaskell | fortran]. Now “ocaml” is greater than “fortran”, so we stop on the left and proceec\n\nThe partitioning. In order to carry out the partitioning within the given array, some\nhought is required as to how this may be best achieved. This is more easily demonstrated by\n\nmple than put into words. For a change, we will consider an array of strings, namely\n\nhe programming languages: [c, fortran, java, ada, pascal, basic, haskell, ocaml]. The ordering\nwe choose is the standard lexicographic one, and let the chosen pivot be “fortran”.\n\nWe will use markers | to denote a partition of the array. To the left of the left marker,\nhere will be items we know to have a key smaller than or equal to the pivot. To the right of\n\nat marker, there will be items we know to have a key bigger than or equal to the pivot.\nmiddle, there will be the items we have not yet considered. Note that this algorithm\n\ne from the sub-array creation process, so we have the array: [|c, ocaml, java, ada,\nbasic, haskell | fortran]. Starting from the left, we find “c” is less than “fortran”, so\nve the left marker one step to the right to give [c | ocaml, java, ada, pascal, basic,\n\n1e right instead, without moving the left marker. We then find “haskell” is bigger than\n\n“fortran”, so we move the right marker to the left by one, giving [c | ocaml, java, ada, pascal,\n\nhaskell, fortran]. Now “basic” is smaller than “fortran”, so we have two keys, “ocam”\n\nand “basic”, which are ‘on the wrong side’. We therefore swap them, which allows us to move\ndoth the left and the right marker one step further towards the middle. This brings us to [c,\n\njava, ada, pascal | ocaml, haskell, fortran]. Now we proceed from the left once again,\n\nout “java” is bigger than “fortran”, so we stop there and switch to the right. Then “pascal”\n\ner than “fortran”, so we move the right marker again. We then find “ada”, which is\n\nsmaller than the pivot, so we stop. We have now got [c, basic | java, ada, | pascal, ocaml,\nhaskell, fortran]. As before, we want to swap “java” and “ada”, which leaves the left and the\nright markers in the same place: [c, basic, ada, java | | pascal, ocaml, haskell, fortran], so we\n\n76\n\n\nstop. Finally, we swap the pivot back from the last position into the position immediately\nafter the markers to give [c, basic, ada, java | | fortran, ocaml, haskell, pascal].\n\nSince we obviously cannot have the marker indices\n\n‘between’ array entries, we will as-\n\nsume the left marker is on the left of afleftmark] and the right marker is to the right\nof alrightmark]. The markers are therefore ‘in the same place’ once rightmark becomes\nsmaller than leftmark, which is when we stop. If we assume that the keys are integers, we\ncan write the partitioning procedure, that needs to return the final pivot position, as:\n\npartition(array a, int left, int right) {\npivotindex = choosePivot(a, left, right)\npivot = alpivotindex]\nswap a[pivotindex] and alright]\nleftmark = left\nrightmark = right - 1\nwhile (leftmark <= rightmark) {\n\nwhile (leftmark <= rightmark and al[leftmark] <= pivot)\n\neftmarkt++\n\nwhile (leftmark <= rightmark and al[rightmark] >= pivot)\n\nrightmark--\n\nif (leftmark < rightmark)\n\nswap a[leftmark++] and a[rightmark--\n}\nswap a[leftmark] and a[right]\n\nreturn leftmark\n\n}\n\nThis achieves a partitioning that ends with the same i\n\n]\n\nems in the array, but in a different\n\norder, with all items to the left of the returned pivot position smaller or equal to the pivot\n\nvalue, and all items to the right greater or equal to the\n\ndivot value.\n\nNote that this algorithm doesn’t require any extra memory — it just swaps the items in the\noriginal array. However, the swapping of items means the algorithm is not stable. To render\n\nquicksort stable, the partitioning must be done in such a\ncan never be reversed. A conceptually simple approac!\n\nway that the order of identical items\n1 that does this, but requires more\n\nmemory and copying, is to simply go systematically through the whole array, re-filling the\n\narray a with items less than or equal to the pivot, anc\n\nfilling a second array b with items\n\ngreater or equal to the pivot, and finally copying the array b into the end of a:\n\npartition2(array a, int left, int right) f{\ncreate new array b of size right-left+1\npivotindex = choosePivot(a, left, right)\npivot = alpivotindex]\nacount = left\n\nbeount = 1\nfor ( i = left ; i <= right ; i++) f{\nif ( i == pivotindex )\npO] = ali]\nelse if ( ali] < pivot || (ali] == pivot && i < pivotindex) )\n\n77\n\n\naLlacount++] = a[i]\nelse\nb[bcount++] = ali]\n\n}\n\nfor (i=0; i < bcount ; it+ )\nalacountt++] = b[i]\n\nreturn right-bcount+1\n\n}\n\nLike the first partition procedure, this also achieves a partitioning with the same items in the\narray, but in a different order, with all items to the left of the returned pivot position smaller\nor equal to the pivot value, and all items to the right greater or equal to the pivot value.\n\nComplexity of Quicksort. Once again we shall determine complexity based on the number\nof comparisons performed. The partitioning step compares each of n items against the pivot,\nand therefore has complexity O(n). Clearly, some partition and pivot choice algorithms are\nless efficient than others, like partition2 involving more copying of items than partition,\nbut that does not generally affect the overall complexity class.\n\nIn the worst case, when an array is partitioned, we have one empty sub-array. If this\nhappens at each step, we apply the partitioning method to arrays of size n, then n — 1, then\nn — 2, until we reach 1. Those complexity functions then add up to\n\nn+(n—1)+(n—2)+---24+1=n(n+1)/2\n\nIgnoring the constant factor and the non-dominant term n/2, this shows that, in the worst\ncase, the number of comparisons performed by Quicksort is O(n?).\n\nIn the best case, whenever we partition the array, the resulting sub-arrays will differ in size\nby at most one. Then we have n comparisons in the first case, two lots of |n/2] comparisons\nfor the two sub-arrays, four times |n/4], eight times [n/8|, and so on, down to 2'°82\"—! times\n[n/2les2-1| = |2|. That gives the total number of comparisons as\n\nn+2!|n/2!| + 2?|n/2?| + 23|n/23| +--. 4+ lesz! | n/gles2\"—1| ~ nlogy n\n\nwhich matches the theoretical best possible time complexity of O(nlog2 n).\nMore interesting and important is how well Quicksort does in the average case. However,\nthat is much harder to analyze exactly. The strategy for choosing a pivot at each stage\naffects that, though as long as it avoids the problems outlined above, that does not change\nthe complexity class. It also makes a difference whether there can be duplicate values, but\nagain that doesn’t change the complexity class. In the end, all reasonable variations involve\ncomparing O(n) items against a pivot, for each of O(loggn) recursions, so the total number\nof comparisons, and hence the overall time complexity, in the average case is O(nlogg n).\nLike Heapsort, when only the largest m < n items need to be found and sorted, rather\nthan all n, Quicksort can be modified to result in reduced time complexity. In this case, only\nthe first sub-array needs to be processed at each stage, until the sub-array sizes exceed m. In\nthat situation, for the best case, the total number of comparisons is reduced to\n\nn+1|n/2\"] + 1[n/2?| + 1[n/2?| +---+ mlogem = 2n.\n\nrendering the time complexity of the whole modified algorithm only O(n). For the average\ncase, the computation is again more difficult, but as long as the key problems outlined above\nare avoided, the average-case complexity of this special case is also O(n).\n\n78\n\n\nImproving Quicksort. It is always worthwhile spending some time optimizing the strategy\nfor defining the pivot, since the particular problem in question might well allow for a more\nrefined approach. Generally, the pivot will be better if more items are sampled before it\nis being chosen. For example, one could check several randomly chosen items and take the\n‘middle’ one of those, the so called median. Note that in order to find the median of all the\nitems, without sorting them first, we would end up having to make n? comparisons, so we\ncannot do that without making Quicksort unattractively slow.\n\nQuicksort is rarely the most suitable algorithm if the problem size is small. The reason for\nthis is all the overheads from the recursion (e.g., storing all the return addresses and formal\nparameters). Hence once the sub-problem become ‘small’ (a size of 16 is often suggested in\nthe literature), Quicksort should stop calling itself and instead sort the remaining sub-arrays\nusing a simpler algorithm such as Selection Sort.\n\n9.13  Mergesort\n\nThe other divide and conquer sorting strategy based on repeatedly splitting the array of items\ninto two sub-arrays, mentioned in Section 9.11, is called mergesort. This simply splits the\narray at each stage into its first and last half, without any reordering of the items in it.\nHowever, that will obviously not result in a set of sorted sub-arrays that we can just append\nto each other at the end. So mergesort needs another procedure merge that merges two sorted\nsub-arrays into another sorted array. As with binary search in Section 4.4, integer variables\nleft and right can be used to refer to the lower and upper index of the relevant array, and\nmid refers to the end of its left sub-array. Thus a suitable mergesort algorithm is:\n\nmergesort(array a, int left, int right) {\nif ( left < right ) {\nmid = (left + right) / 2\nmergesort(a, left, mid)\nmergesort(a, midti, right)\nmerge(a, left, mid, right)\n\n}\n\nNote that it would be relatively simple to modify this mergesort algorithm to operate on\nlinked lists (of known length) rather than arrays. To ‘split’ such a list into two, all one has to\ndo is set the pointer of the |n/2|th list entry to null, and use the previously-pointed-to next\nentry as the head of the new second list. Of course, care needs to be taken to keep the list\nsize information intact, and effort is required to find the crucial pointer for each split.\n\nThe merge algorithm. The principle of merging two sorted collections (whether they be\nlists, arrays, or something else) is quite simple: Since they are sorted, it is clear that the\nsmallest item overall must be either the smallest item in the first collection or the smallest\nitem in the second collection. Let us assume it is the smallest key in the first collection. Now\nthe second smallest item overall must be either the second-smallest item in the first collection,\nor the smallest item in the second collection, and so on. In other words, we just work through\nboth collections and at each stage, the ‘next’ item is the current item in either the first or the\n\nsecond collection.\n\n79\n\n\nThe implementation will be quite different, however, depending on which data structure\nwe are using. When arrays are used, it is actually necessary for the merge algorithm to create\na new array to hold the result of the operation at least temporarily. In contrast, when using\nlinked lists, it would be possible for merge to work by just changing the reference to the next\nnode. This does make for somewhat more confusing code, however.\n\nFor arrays, a suitable merge algorithm would start by creating a new array b to store the\nresults, then repeatedly add the next smallest item into it until one sub-array is finished, then\ncopy the remainder of the unfinished sub-array, and finally copy b back into a:\n\nmerge(array a, int left, int mid, int right) {\ncreate new array b of size right-left+1\n\nbcount = 0\nlcount = left\nrecount = mid+1\n\nwhile ( (lcount <= mid) and (rcount <= right) ) f{\nif ( a[lcount] <= a[rcount] )\n\np[bcountt++] = a[lcountt++]\nelse\np[bcountt++] = a[rcountt+]\n\n}\nif ( lcount > mid )\nwhile ( rcount <= right )\n\np[bcountt++] = a[rcountt+]\nelse\nwhile ( lcount <= mid )\np[bcountt++] = a[lcountt++]\n\nfor ( bcount = 0 ; bcount < right-left+1 ; bcount++ )\na[left+bcount] = b[bcount]\n\n}\n\nIt is instructive to compare this with the partition2 algorithm for Quicksort to see exactly\nwhere the two sort algorithms differ. As with partition2, the merge algorithm never swaps\nidentical items past each other, and the splitting does not change the ordering at all, so the\nwhole Mergesort algorithm is stable.\n\nComplexity of Mergesort. The total number of comparisons needed at each recursion\nlevel of mergesort is the number of items needing merging which is O(n), and the number\nof recursions needed to get to the single item level is O(log n), so the total number of com-\nparisons and its time complexity are O(nlogz n). This holds for the worst case as well as the\naverage case. Like Quicksort, it is possible to speed up mergesort by abandoning the recursive\nalgorithm when the sizes of the sub-collections become small. For arrays, 16 would once again\nbe a suitable size to switch to an algorithm like Selection Sort.\n\nNote that, with Mergesort, for the special case when only the largest/smallest m << n\nitems need to be found and sorted, rather than all n, there is no way to reduce the time\ncomplexity in the way it was possible with Heapsort and Quicksort. This is because the\nordering of the required items only emerges at the very last stage after the large majority of\n\nthe comparisons have already been carried out.\n\n80\n\n\n9.14 Summary of comparison-based sorting algorithms\n\nThe following table summarizes the key properties of all the comparison-based sorting algo-\nrithms we have considered:\nSorting Strategy Objects Worst case | Average case | Stable\nAlgorithm employed manipulated | complexity | complexity\nBubble Sort Exchange arrays O(n”) O(n?) Yes\nSelection Sort || Selection arrays O(n?) O(n?) No\nInsertion Sort || Insertion arrays/lists O(n?) O(n?) Yes\nTreesort Insertion trees/lists O(n?) O(nlog2 n) Yes\nHeapsort Selection arrays O(nlogs n) O(nlog2 n) No\nQuicksort D&C arrays O(n?) O(nlogzn) | Maybe\nMergesort D&C arrays/lists | O(nlogyn) O(nlog2 n) Yes\nTo see what the time complexities mean in practice, the following table compares the typical\nrun times of those of the above algorithms that operate directly on arrays:\nAlgorithm 128 256 512 | 1024 | 01024 | R1024 | 2048\nBubble Sort 54} 221} 881} 3621 1285 5627 | 14497\nSelection Sort 12 45 164 634 643 833 | 2497\nInsertion Sort 15 69 276 | 1137 6 2200 | 4536\nHeapsort 21 45 103 236 215 249 527\nQuicksort 12 27 55 2 1131 1200 230\nQuicksort2 6 12 24 57 1115 1191 134\nMergesort 18 36 88 88 166 170 409\nMergesort2 6 22 48 2 94 93 254\n\nAs before, arrays of the stated sizes are filled randomly, except 01024 that denotes an array\nwith 1024 entries which are already sorted, and R1024 that denotes an array which is sorted in\nthe reverse order. Quicksort2 and Mergesort2 are algorithms where the recursive procedure is\nabandoned in favour of Selection Sort once the size of the array falls to 16 or below. It should\nbe emphasized again that these numbers are of limited accuracy, since they vary somewhat\ndepending on machine and language implementation.\n\nWhat has to be stressed here is that there is no ‘best sorting algorithm’ in general, but\nthat there are usually good and bad choices of sorting algorithms for particular circumstances.\nIt is up to the program designer to make sure that an appropriate one is picked, depending\non the properties of the data to be sorted, how it is best stored, whether all the sorted items\nare required rather than some sub-set, and so on.\n\n9.15 Non-comparison-based sorts\n\nAll the above sorting algorithms have been based on comparisons of the items to be sorted,\nand we have seen that we can’t get time complexity better than O(nlogg n) with comparison\nbased algorithms. However, in some circumstances it is possible to do better than that with\nsorting algorithms that are not based on comparisons.\n\n81\n\n\nIt is always worth thinking about the data that needs to be sorted, and whether com-\nparisons really are required. For example, suppose you know the items to be sorted are the\nnumbers from 0 to n — 1. How would you sort those? The answer is surprisingly simple. We\nknow that we have n entries in the array and we know exactly which items should go there\nand in which order. This is a very unusual situation as far as general sorting is concerned, yet\nthis kind of thing often comes up in every-day life. For example, when a hotel needs to sort\nthe room keys for its 100 rooms. Rather than employing one of the comparison-based sorting\nalgorithms, in this situation we can do something much simpler. We can simply put the items\ndirectly in the appropriate places, using an algorithm such as that as shown in Figure 9.1:\n\n3| o| 4} 72} 2 create array b of size n\n\nfor (i=0;i<n; i++)\nbfali]] = afi]\n\nO| 1| 2| 3) 4 copy array b into array a\n\nFigure 9.1: Simply put the items in the right order using their values.\n\nThis algorithm uses a second array b to hold the results, which is clearly not very memory\nefficient, but it is possible to do without that. One can use a series of swaps within array a\nto get the items in the right positions as shown in Figure 9.2:\n\n1) o| 4) 3) 2 for (i=0; i<n; i++) {\nwhile ( afi] != i )\n\nYY swap a[a[i]] and a[i]\n\ni=]\ni=2\n\nO| 1| 2) 3) 4\n\nFigure 9.2: Swapping the items into the right order without using a new array.\n\nAs far as time complexity is concerned, it is obviously not appropriate here to count the\nnumber of comparisons. Instead, it is the number of swaps or copies that is important. The\nalgorithm of Figure 9.1 performs n copies to fill array b and then another n to return the result\nto array a, so the overall time complexity is O(n). The time complexity of the algorithm of\nFigure 9.2 looks worse than it really is. This algorithm performs at most n — 1 swaps, since\none item, namely a[a[i]] is always swapped into its final position. So at worst, this has time\ncomplexity O(n) too.\n\nThis example should make it clear that in particular situations, sorting might be per-\nformed by much simpler (and quicker) means than the standard comparison sorts, though\nmost realistic situations will not be quite as simple as the case here. Once again, it is the\nresponsibility of the program designer to take this possibility into account.\n\n82\n\n\n9.16 Bin, Bucket, Radix Sorts\n\nBin, Bucket, and Radix Sorts are all names for essentially the same non-comparison-based\nsorting algorithm that works well when\nexample, suppose you are given a number of dates, by day and month, and need to sort them\n\ninto order. One way of doing this woulc\nitems (dates) one at a time into the right\nfurther). Then form one big queue out of\nwith day 1 and continuing up to day 31.\nmonth, and place the dates into the right\n\nhe items are labelled by small sets of values. For\n\nbe to create a queue for each day, and place the\nqueue according to their day (without sorting them\nthese, by concatenating all the day queues starting\nThen for the second phase, create a queue for each\nqueues in the order that they appear in the queue\n\ncreated by the first phase. Again form a big queue by concatenating these month queues in\norder. This final queue is sorted in the intended order.\nThis may seem surprising at first sight, so let us consider a simple example:\n\n[25/12, 28/08, 29/05, 01/05, 24/04, 03/01, 04/01, 25/04, 26/12, 26/04, 05/01, 20/04].\n\nWe first create and fill queues for the days as follows:\n\nOl:\n03:\n04:\n05:\n20:\n24:\n25:\n26:\n28:\n29:\n\nThe empty queues are not shown\n\n01/05\n03/01\n04/01\n05/01\n20/04\n24/04\n25/12, 25/04]\n26/12, 26/04]\n28/08\n29/05\n\nthere is no need to create queues before we hit an item\n\nthat belongs to them. Then concatenation of the queues gives:\n\n(01/05, 03/01, 04/01, 05/01, 20/04, 24/04, 25/12, 25/04, 26/12, 26/04,28/08, 29/05].\n\nNext we create and fill queues for the months that are present, giving:\n\n01\n\n: (03/01, 04/01,\n: (20/04, 24/04,\n\n: (01/05, 29/05]\n: [28/08]\n: [25/12, 26/12]\n\n05/01]\n25/04, 26/04]\n\nFinally, concatenating all these queues gives the items in the required order:\n\n[03/01, 04/01, 05/01, 20/04, 24/04, 25/04, 26/04, 01/05, 29/05, 28/08, 25/12, 26/12].\n\nThis is called Two-phase Radix Sorting, since there are clearly two phases to it.\n\n83\n\n\nThe extension of this idea to give a general sorting algorithm should be obvious: For each\nphase, create an ordered set of queues corresponding to the possible values, then add each\nitem in the order they appear to the end of the relevant queue, and finally concatenate the\nthe queues in order. Repeat this process for each sorting criterion. The crucial additional\ndetail is that the queuing phases must be performed in the order of the significance of each\ncriteria, with the least significant criteria first.\n\nFor example, if you know that your items to be sorted are all (at most) two-digit integers,\nyou can use Radix Sort to sort them. First create and fill queues for the last digit, concatenate,\nthen create and fill queues for the first digit, and concatenate to leave the items in sorted\norder. Similarly, if you know that your keys are all strings consisting of three characters, you\ncan again apply Radix Sort. You would first queue according to the third character, then the\nsecond, and finally the first, giving a Three phase Radix Sort.\n\nNote that at no point, does the the algorithm actually compare any items at all. This\nkind of algorithm makes use of the fact that for each phase the items are from a strictly\nrestricted set, or, in other words, the items are of a particular form which is known a priori.\nThe complexity class of this algorithm is O(n), since at every phase, each item is dealt with\nprecisely once, and the number of phases is assumed to be small and constant. If the restricted\nsets are small, the number of operations involved in finding the right queue for each item and\nplacing it at the end of it will be small, but this could become significant if the sets are\narge. The concatenation of the queues will involve some overheads, of course, but these will\nye small if the sets are small and linked lists, rather than arrays, are used. One has to be\ncareful, however, because if the total number of operations for each item exceeds logs n, then\nhe overall complexity is likely to be greater than the O(nloggn) complexity of the more\nefficient comparison-based algorithms. Also, if the restricted sets are not known in advance,\nand potentially large, the overheads of finding and sorting them could render Radix sort worse\nhan using a comparison-based approach. Once again, it is the responsibility of the program\ndesigner to decide whether a given problem can be solved more efficiently with Radix Sort\nrather than a comparison-based sort.\n\n84\n\n\nChapter 10\n\nHash Tables\n\n10.1 Storing data\n\nWe have already seen a number of different ways of storing items in a computer: arrays and\nvariants thereof (e.g., sorted and unsorted arrays, heap trees), linked lists (e.g., queues, stacks),\nand trees (e.g., binary search trees, heap trees). We have also seen that these approaches can\nperform quite differently when it comes to the particular tasks we expect to carry out on the\nitems, such as insertion, deletion and searching, and that the best way of storing data does\nnot exist in general, but depends on the particular application.\n\nThis chapter looks at another way of storing data, that is quite different from the ones\nwe have seen so far. The idea is to simply put each item in an easily determined location, so\nwe never need to search for it, and have no ordering to maintain when inserting or deleting\nitems. This has impressive performance as far as time is concerned, but that advantage is\npayed for by needing more space (i.e., memory), as well as by being more complicated and\ntherefore harder to describe and implement.\n\nWe first need to specify what we expect to be able to do with this way of storing data,\nwithout considering how it is actually implemented. In other words, we need to outline an\nabstract data type. This is similar to what you will generally do when first trying to implement\na class in Java: You should think about the operations you wish to perform on the objects\nof that class. You may also want to specify a few variables that you know will definitely be\nneeded for that class, but this does not usually come into defining an abstract data type.\nThe approach we have been following for defining abstract data types in these notes is by\ndescribing the crucial operations in plain English, trusting that they are simple enough to not\nneed further explanations. In general, what is needed is a specification for the abstract data\ntype in question. An important aspect of studying software engineering is to learn about and\nuse more formal approaches to this way of operating.\n\nAfter we have decided what our specification is, we then need to choose a data structure\nin order to implement the abstract data type. The data structure to be considered in this\nchapter is a particular type of table known as a hash table.\n\n10.2 The Table abstract data type\n\nThe specification of the table abstract data type is as follows:\n\n1. A table can be used to store objects, for example\n\n85\n\n\n012 | Johnny | English | Spy\n\n007 | James Bond Spy\n\n583 | Alex Rider Spy\n\n721 | Sherlock | Holmes Detective\n722 | James Moriarty | Villain\n\n2. The objects can be arbitrarily complicated. However, for our purposes, the only relevant\ndetail is that each object has a unique key, and that their keys can be compared for\nequality. The keys are used in order to identify objects in much the way we have done\nfor searching and sorting.\n\n3. We assume that there are methods or procedures for:\n\n(a) determining whether the table is empty or full;\n(b) inserting a new object into the table, provided the table is not already full;\n(c) given a key, retrieving the object with that key;\n\n(d) given a key, updating the item with that key (usually by replacing the item with a\nnew one with the same key, which is what we will assume here, or by overwriting\nsome of the item’s variables);\n\n(e) given a key, deleting the object with that key, provided that object is already stored\nin the table;\n\n(f) listing or traversing all the items in the table (if there is an order on the keys then\nwe would expect this to occur in increasing order).\n\nNotice that we are assuming that each object is uniquely identified by its key.\n\nIn a programming language such as Java, we could write an interface for this abstract\ndata type as follows, where we assume here that keys are objects of a class we call Key and\nwe have records of a class called Record:\n\ninterface Table {\n\nBoolean isEmpty ();\nBoolean isFull();\nvoid Insert (Record) ;\nRecord Retrieve (Key) ;\nvoid Update (Record) ;\nvoid Delete{Key};\nvoid Traverse();\n\n}\n\nNote that we have not fixed how exactly the storage of records should work — that is some-\nthing that comes with the implementation. Also note that you could give an interface to\nsomebody else, who could then write a program which performs operations on tables without\never knowing how they are implemented. You could certainly carry out all those operations\nwith binary search trees and sorted or unsorted arrays if you wished. The former even has\nthe advantage that a binary search tree never becomes full as such, because it is only limited\nby the size of the memory.\n\nThis general approach follows the sensible and commonly used way to go about defining a\nJava class: First think about what it is you want to do with the class, and only then wonder\n\n86\n\n\nabout how exactly you might implement the methods. Thus, languages such as Java support\nmechanisms for defining abstract data types. But notice that, as opposed to a specification\nin plain English, such as the above, a definition of an interface is only a partial specification\nof an abstract data type, because it does not explain what the methods are supposed to do;\nit only explains how they are called.\n\n10.3. Implementations of the table data structure\n\nThere are three key approaches for implementing the table data structure. The first two we\nhave studied already, and the third is the topic of this chapter:\n\nImplementation via sorted arrays. Let us assume that we want to implement the table\ndata structure using a sorted array. Whether it is full or empty can easily be determined in\nconstant time if we have a variable for the size. Then to insert an element we first have to find\nits proper position, which will take on average the same time as finding an element. To find an\nelement (which is necessary for all other operations apart from traversal), we can use binary\nsearch as described in in Section 4.4, so this takes O(loggn). This is also the complexity for\nretrieval and update. However, if we wish to delete or insert an item, we will have to shift\nwhat is ‘to the right’ of the location in question by one, either to the left (for deletion) or to\nthe right (for insertion). This will take on average n/2 steps, so these operations have O(n)\ncomplexity. Traversal in order is simple, and is of O(n) complexity as well.\n\nImplementation via binary search trees A possible alternative implementation would\ninvolve using binary search trees. However, we know already that in the worst case, the tree\ncan be very deep and narrow, and that these trees will have linear complexity when it comes\nto looking up an entry. We have seen that there is a variant of binary search trees which keeps\nthe worst case the same as the average case, the so-called self-balancing binary search tree,\nbut that is more complicated to both understand and program. For those trees, insertion,\ndeletion, search, retrieval and update, can all be done with time complexity O(logy n), and\ntraversal has O(n) complexity.\n\nImplementation via Hash tables The idea here is that, at the expense of using more\nspace than strictly needed, we can speed up the table operations. The remainder of this\nchapter will describe how this is done, and what the various computational costs are.\n\n10.4 Hash Tables\n\nThe underlying idea of a hash table is very simple, and quite appealing: Assume that, given\na key, there was a way of jumping straight to the entry for that key. Then we would never\nhave to search at all, we could just go there! Of course, we still have to work out a way for\nthat to be achieved. Assume that we have an array data to hold our entries. Now if we had a\nfunction h(k) that maps each key k to the index (an integer) where the associated entry will\nbe stored, then we could just look up data[h(k)] to find the entry with the key k.\n\nIt would be easiest if we could just make the data array big enough to hold all the keys\nthat might appear. For example, if we knew that the keys were the numbers from 0 to 99,\nthen we could just create an array of size 100 and store the entry with key 67 in data[67],\n\n87\n\n\nand so on. In this case, the function h would be the identity function h(k) = k. However,\nthis idea is not very practical if we are dealing with a relatively small number of keys out of a\nhuge collection of possible keys. For example, many American companies use their employees’\n9-digit social security number as a key, even though they have nowhere near 10° employees.\nBritish National Insurance Numbers are even worse, because they are just as long and usually\ncontain a mixture of letters and numbers. Clearly it would be very inefficient, if not impossible,\n0 reserve space for all 10° social security numbers which might occur.\n\nInstead, we use a non-trivial function h, the so-called hash function, to map the space\n\nof possible keys to the set of indices of our array. For example, if we had to store entries\nabout 500 employees, we might create an array with 1000 entries and use three digits from\nheir social security number (maybe the first or last three) to determine the place in the array\nwhere the records for each particular employee should be stored.\nThis approach sounds like a good idea, but there is a pretty obvious problem with it: What\nhappens if two employees happen to have the same three digits? This is called a collision\nyetween the two keys. Much of the remainder of this chapter will be spent on the various\nstrategies for dealing with such collisions.\n\nFirst of all, of course, one should try to avoid collisions. If the keys that are likely to\nactually occur are not evenly spread throughout the space of all possible keys, particular\nattention should be paid to choosing the hash function h in such a way that collisions among\nthem are less likely to occur. If, for example, the first three digits of a social security number\nhad geographical meaning, then employees are particularly likely to have the three digits\nsignifying the region where the company resides, and so choosing the first three digits as a\nhash function might result in many collisions. However, that problem might easily be avoided\nby a more prudent choice, such as the last three digits.\n\n10.5 Collision likelihoods and load factors for hash tables\n\nOne might be tempted to assume that collisions do not occur very often when only a small\nsubset of the set of possible keys is chosen, but this assumption is mistaken.\n\nThe von Mises birthday paradox. As an example, consider a collection of people, and a\nhash function that gives their birthdays as the number of the day in the year, i.e. 1st January\nis 1, 2nd January is 2, ..., 31st December is 365. One might think that if all we want to do\nis store a modest number of 24 people in this way in an array with 365 locations, collisions\nwill be rather unlikely. However, it turns out that the probability of collision is bigger than\n50%. This is so surprising at first sight that this phenomenon has become known as the von\nMises birthday paradox, although it is not really a paradox in the strict sense.\n\nIt is easy to understand what is happening. Suppose we have a group of n people and\nwant to find out how likely it is that two of them have the same birthday, assuming that the\nbirthdays are uniformly distributed over the 365 days of the year. Let us call this probability\np(n). It is actually easier to first compute the probability q(n) that no two of them share a\nbirthday, and then p(n) = 1— q(n). For n = 1 this probability is clearly g(1) = 1. For n = 2\nwe get q(2) = 364/365 because, for the added second person, 364 of the 365 days are not the\nbirthday of the first person. For n = 3 we get\n\n_ 365 - 364 - 363\n\n(3) 3653\n\n= 1—p(3)\n\n88\n\n\nand for the general n > 1 case we have\n\n365 - 364. 363--- (365 —n +1) 365!\n365\" 365\"(365 — n)!\n\np(n)\n\nq(n)\n\nIt may be surprising that p(22) = 0.476 and p(23) = 0.507, which means that as soon as there\nare more than 22 people in a group, it is more likely that two of them share a birthday than\nnot. Note that in the real world, the distribution of birthdays over the year is not precisely\nuniform, but this only increases the probability that two people have the same birthday. In\nother words, birthday collisions are much more likely than one might think at first.\n\nImplications for hash tables. If 23 random locations in a table of size 365 have more\nthan a 50% chance of overlapping, it seems inevitable that collisions will occur in any hash\ntable that does not waste an enormous amount of memory. And collisions will be even more\nlikely if the hash function does not distribute the items randomly throughout the table.\n\nTo compute the computational efficiency of a hash table, we need some way of quantifying\nhow full the table is, so we can compute the probability of collisions, and hence determine\nhow much effort will be required to deal with them.\n\nThe load factor of a hash table. Suppose we have a hash table of size m, and it currently\nhas n entries. Then we call ) = n/m the load factor of the hash table. This load factor is\nthe obvious way of describing how full the table currently is: A hash table with load factor\n0.25 is 25% full, one with load factor 0.50 is 50% full, and so on. Then if we have a hash\ntable with load factor A, the probability that a collision occurs for the next key we wish to\ninsert is A. This assumes that each key from the key space is equally likely, and that the hash\nfunction h spreads the key space evenly over the set of indices of our array. If these optimistic\nassumptions fail, then the probability may be even higher.\n\nTherefore, to minimize collisions, it is prudent to keep the load factor low. Fifty percent is\nan often quoted good maximum figure, while beyond an eighty percent load the performance\ndeteriorates considerably. We shall see later exactly what effect the table’s load factor has on\nthe speed of the operations we are interested in.\n\n10.6 A simple Hash Table in operation\n\nLet us assume that we have a small data array we wish to use, of size 11, and that our set\nof possible keys is the set of 3-character strings, where each character is in the range from\nA to Z. Obviously, this example is designed to illustrate the principle — typical real-world\nhash tables are usually very much bigger, involving arrays that may have a size of thousands,\nmillions, or tens of millions, depending on the problem.\n\nWe now have to define a hash function which maps each string to an integer in the range\n0 to 10. Let us consider one of the many possibilities. We first map each string to a number\nas follows: each character is mapped to an integer from 0 to 25 using its place in the alphabet\n(A is the first letter, so it goes to 0, B the second so it goes to 1, and so on, with Z getting\nvalue 25). The string X;X»2X3 therefore gives us three numbers from 0 to 25, say ky, ko,\nand k3. We can then map the whole string to the number calculated as\n\nk = ky «26? + ky * 26! + ky « 269 = ky * 26? + ky «26 + ky.\n\n89\n\n\nThat is, we think of the strings as coding numbers in base 26.\n\nNow it is quite easy to go from any number k (rather than a string) to a number from 0\nto 10. For example, we can take the remainder the number leaves when divided by 11. This\nis the C or Java modulus operation k % 11. So our hash function is\n\nh(X,X_X3) = (ky * 267 + ky * 26+ kg)%11 = k%11.\n\nThis modulo operation, and modular arithmetic more generally, are widely used when con-\n\nstructing good hash functions.\n\nAs a simple example of a hash table in operation, assume that we now wish to insert the\n\nfollowing three-letter airport acronyms as keys (in this order) into our hash table: PHL, ORY,\nGCM, HKG, GLA, AKL, FRA, LAX, DCA. To make this easier, it is a good idea to start by\nlisting the values the hash function takes for each of the keys:\n\nCode PHL | ORY | GCM | HKG | GLA | AKL | FRA | LAX | DCA\n\nh(XiX2X3) | 4 8 6 4 8\n\n7 5 1 1\n\nIt is clear already that we will have hash collisions to deal with.\nWe naturally start off with an empty table of the required size, i.e. 11:\n\nClearly we have to be able to tell whether a particular location in the array is still empty, or\nwhether it has already been filled. We can assume that there is a wnique key or entry (which\n\nis never associated with a record) which denotes that the\n\nosition has not been filled yet.\n\nHowever, for clarity, this key will not appear in the pictures we use.\nNow we can begin inserting the keys in order. The number associated with the first item\n\nPHL is 4, so we place it at index 4, giving:\n\nPHL\n\nNext is ORY, which gives us the number 8, so we get:\n\nPHL ORY\nThen we have GCM, with value 6, giving:\n| PHL GCM ORY\n\nThen HKG, which also has value 4, results in our first collision since the corresponding position\nhas already been filled with PHL. Now we could, of course, try to deal with this by simply\nsaying the table is full, but this gives such poor performance (due to the frequency with which\n\ncollisions occur) that it is unacceptable.\n\n10.7 Strategies for dealing with collisions\n\nWe now look at three standard approaches, of increasing complexity, for dealing with hash\n\ncollisions:\n\n90\n\n\nBuckets. One obvious option is to reserve a two-dimensional array from the start. We can\nthink of each column as a bucket in which we throw all the elements which give a particular\nresult when the hash function is supplied, so the fifth column contains all the keys for which\nthe hash function evaluates to 4. Then we could put HKG into the slot ‘beneath’ PHL, and\nGLA in the one beneath ORY, and continue filling the table in the order given until we reach:\n\n0 1 2 3 4 5 6 7 8 9 | 10\nLAX PHL | FRA | GCM |} AKL | ORY\nDCA HKG GLA\n\nThe disadvantage of this approach is that it has to reserve quite a bit more space than will be\neventually required, since it must take into account the likely maximal number of collisions.\nEven while the table is still quite empty overall, collisions will become increasingly likely.\nMoreover, when searching for a particular key, it will be necessary to search the entire column\nassociated with its expected position, at least until an empty slot is reached. If there is an\norder on the keys, they can be stored in ascending order, which means we can use the more\nefficient binary search rather than linear search, but the ordering will have an overhead of its\nown. The average complexity of searching for a particular item depends on how many entries\nin the array have been filled already. This approach turns out to be slower than the other\ntechniques we shall consider, so we shall not spend any more time on it, apart from noting\nthat it does prove useful when the entries are held in slow external storage.\n\nDirect chaining. Rather than reserving entire sub-arrays (the columns above) for keys\nthat collide, one can instead create a linked list for the set of entries corresponding to each\nkey. The result for the above example can be pictured something like this:\n\n0 1 2 3 4 5 6 7 8 9 10\nr++\nLAX PHL | FRA | GCM| AKL | ORY\n° ° ° e e °\nY Y Y\nDCA HKG GLA\ne e °\n\nThis approach does not reserve any space that will not be taken up, but has the disadvantage\nthat in order to find a particular item, lists will have to be traversed. However, adding the\nhashing step still speeds up retrieval considerably.\n\nWe can compute the size of the average non-empty list occurring in the hash table as\nfollows. With n items in an array of size m, the probability than no items land in a particular\nslot is g(n,m) = (myn. So the number of slots with at least one item falling in it is\n\nN(n,m) m.(1 g(n,m)) m.(1 (2=*))\n\nm\n\n91\n\n\nand since there are n items altogether, the average number of items in a non-empty list is:\n\nn n\n\nN(n,m) m.(1 7 ep) .\n\nk(n,m)\n\nm\nThen a linear search for an item in a list of size k takes on average\n\n1 k(k+1) k+l\n“(1424--4k\n( +) Qk 2\n\ncomparisons. It is difficult to visualize what these formulae mean in practice, but if we assume\nthe hash table is large but not overloaded, i.e. n and m are both large with n < m, we can\n\nperform a Taylor approximation for small loading factor \\ = n/m. T\n\nk+1 » » 3\na 1454 54003\n2 +7 tag F 00)\n\ncomparisons on average for a successful search, i.e. that this has O(1\n\n1at shows there are\n\ncomplexity.\n\nFor an unsuccessful search, we need the average list size including the empty slots. That\n\nwill clearly be n/m = 4, and so in an unsuccessful search the average\nmade to decide the item in question is not present will be A, which is\n\nnumber of comparisons\nagain O(1).\n\nThus, neither the successful nor unsuccessful search times depenc\n\non the number of keys\n\nin the table, but only on the load factor, which can be kept low by choosing the size of the\nhash table to be big enough. Note also that insertion is done even more speedily, since all we\n\nhave to do is to insert a new element at the front of the appropriate\ntraversal, the complexity class of all operations is constant, i.e. O(1).\n\nto sort the keys, which can be done in O(nloggn), as we know from Chapter 9. A varian'\nwould be to make each linked list sorted, which will speed up finding an item, as well as speec\nup traversal slightly, although this will not put either operation into a different complexity\nclass. This speed-up would be paid for by making the insertion operation more expensive, i.e.\n\ntake slightly longer, but it will still have constant complexity.\n\nOverall, all the time complexities for this approach are clearly very impressive comparec\n\nto those for sorted arrays or (balanced) binary search trees.\n\nOpen addressing. The last fundamentally different approach to collision avoidance is callec\n\nlist. Hence, apart from\nFor traversal, we neec\n\nopen addressing, and that involves finding another open location for any entry which cannot be\nplaced where its hash function points. We refer to that position as a key’s primary position (so\nin the earlier example, ORY and GLA have the same primary position). The easiest strategy\nfor achieving this is to search for open locations by simply decreasing the index considered by\none until we find an empty space. If this reaches the beginning of the array, i.e. index 0, we\nstart again at the end. This process is called linear probing. A better approach is to search\nfor an empty location using a secondary hash function. This process is called double hashing.\n\nWe will now look at both of these approaches in some detail.\n\n10.8 Linear Probing\n\nWe now proceed with the earlier example using linear probing. We had reached the stage:\n\n| PHL GCM ORY\n\n92\n\n\nand then wanted to put HKG at inc\nLinear probing reduces the index\nso we put HKG there giving:\n\nex 4, where we found PHL.\nby one to 3, and finds an empty location in that position,\n\nHKG\n\nPHL\n\nGCM ORY\n\nNext we wish to insert GLA, with hash value 8, but the location with that index is already\nprobing reduces the index by one, and since that slot one to the\n\nfilled by ORY. Again\n\nleft is free, we insert GLA\n\nlinear\n\nhere:\n\nHKG\n\nPHL\n\nGCM | GLA | ORY |\n\nThen we have AKL, and although we have not had the value 7 before, the corresponding\nlocation is filled by GLA. So we try the next index down, but that contains GCM, so we\n\ncontinue to the next one at index 5 which s empty, so we put AKL there:\n\n| HKG | PHL | AKL | GCM | GLA ORY | | |\n\nWe now continue in t:\n\n1e same way with the remaining keys, eventually reaching:\n\n[DCA | LAX\n\nFRA\n\nHKG\n\nPHL\n\nAKL | GCM | GLA | ORY |\n\nThis looks quite conv\ngood use of the space\n\nincing\n\n- all the keys have been inserted in a way that seems to make\n\nwe have reserved.\n\nHowever, what happens now if we wish to find a particular key? It will no longer be good\n\nenough to simply apply the hash\n\n‘unction to it and check there. Instead, we will have to\n\nfollow its possible insertion locations until we hit an empty one, which tells us that the key\n\nwe were looking for is\nis why every hash tab.\ntime, and be declarec\nhash tables lose much\n\ne that\n\nof policy, many more\n\nuntil we find an empty space which tells us\npretty bad at first sight, but bear in mind\n\nnot present,\n\nuses open adc\nfull when only one\nof their speec\nocations should be kept empty.\n\nSo, to find the key AKL, we would firs\nsuccessful. Searching for JFK, on\n\nafter al\n\nadvant\n\n, because it would have been inserted there. This\nressing should have at least one empty slot at any\nempty location is left. However, as we shall see,\nage if they have a high load factor, so as a matter\n\ncheck at index 7, then at 6, and 5, where we are\n\nhe other hand, we would start with its proper position,\ngiven by the hash function value 8, so we would check indices 8, 7, 6, ..., 1, 0, 10 in that order\n\nthat JFK is, in fact, not present at all. This looks\n\nhat we said that we will aim towards keeping the\n\nload factor at around 50 percent, so there would be many more empty slots which effectively\n\nstop any further searc\n\nh.\n\nBut this idea brings another problem with it. Suppose we now delete GCM from the table\n\nand then search for AKL again. We would find the array empty at index 6 and stop searching,\nand therefore wrongly conclude that AKL is not present. This is clearly not acceptable, but\nequally, we do not wish to have to search through the entire array to be sure that an entry is\n\nnot there. The solution is tha\n\nwe reserve another key to mean that a position is empty, but\n\nthat it did hold a key at some point. Let us assume that we use the character ‘!’ for that.\n\nThen after deleting GCM, the\n\narray would be:\n\n[DCA | LAX\n\nFRA\n\nHKG\n\nPHL | AKL | ! | GLA | ORY\n\n93\n\n\nand when searching for AKL we would know to continue beyond the exclamation mark. If,\non the other hand, we are trying to insert a key, then we can ignore any exclamation marks\nand fill the position once again. This now does take care of all our problems, although if we\ndo a lot of deleting and inserting, we will end up with a table which is a bit of a mess. A large\nnumber of exclamation marks means that we have to keep looking for a long time to find a\nparticular entry despite the fact that the load factor may not be all that high. This happens\nif deletion is a frequent operation. In such cases, it may be better to re-fill a new hash table\nagain from scratch, or use another implementation.\n\nSearch complexity. The complexity of open addressing with linear probing is rather dif-\nficult to compute, so we will not attempt to present a full account of it here. If A is once\nagain the load factor of the table, then a successful search can be shown to take 5(1 + 4)\n\ncomparisons on average, while an unsuccessful search takes approximately 3(1 + ay): For\n\nrelatively small load factors, this is quite impressive, and even for larger ones, it is not bad.\nThus, the hash table time complexity for search is again constant, i.e. O(1).\n\nClustering. There is a particular problem with linear probing, namely what is known as\nprimary and secondary clustering. Consider what happens if we try to insert two keys that\nhave the same result when the hash function is applied to them. Take the above example\nwith hash table at the stage where we just inserted GLA:\n\nHKG | PHL GCM | GLA | ORY |\n\nIf we next try to insert JFK we note that the hash function evaluates to 8 once again. So we\nkeep checking the same locations we only just checked in order to insert GLA. This seems a\nrather inefficient way of doing this. This effect is known as primary clustering because the\nnew key JFK will be inserted close to the previous key with the same primary position, GLA.\nIt means that we get a continuous ‘block’ of filled slots, and whenever we try to insert any key\nwhich is sent into the block by the hash function, we will have to test all locations until we\nhit the end of the block, and then make such block even bigger by appending another entry\nat its end. So these blocks, or clusters, keep growing, not only if we hit the same primary\nlocation repeatedly, but also if we hit anything that is part of the same cluster. The last effect\nis called secondary clustering. Note that searching for keys is also adversely affected by these\nclustering effects.\n\n10.9 Double Hashing\n\nThe obvious way to avoid the clustering problems of linear probing is to do something slightly\nmore sophisticated than trying every position to the left until we find an empty one. This is\nknown as double hashing. We apply a secondary hash function to tell us how many slots to\njump to look for an empty slot if a key’s primary position has been filled already.\n\nLike the primary hash function, there are many possible choices of the secondary hash\nfunction. In the above example, one thing we could do is take the same number k associated\nwith the three-character code, and use the result of integer division by 11, instead of the\nremainder, as the secondary hash function. However, the resulting value might be bigger than\n10, so to prevent the jump looping round back to, or beyond, the starting point, we first take\n\n94\n\n\nthe result of integer division by 11, and then take the remainder this result leaves when again\n\ndivided by 11. Thus we would like to use as our secondary hash function h2(n)\n\n(k/11)%11.\n\nHowever, this has yet another problem: it might give zero at some point, and we obviously\ncannot test ‘every zeroth location’. An easy solution is to simply make the secondary hash\nfunction one if the above would evaluate to zero, that is:\n\nha(n) = {\n\n(k/11)%11\n1\n\nif (k/11)%11 4 0,\n\notherwise.\n\nThe values of this for our example set of keys are given in the following table:\n\nCode\n\nPHL | ORY\n\nGCM\n\nHKG\n\nGLA\n\nAKL | FRA\n\nLAX\n\nDCA | BHX\n\nho(X1X2X3)\n\n4 1 1\n\n3\n\n9 2 6\n\n7\n\n2\n\nWe can then proceed from the situation we were in when the first co\n\nwith HKG\nnow try eve\n\nNo\n\nCov\n\nfirs\n\nNo\n\nOur example is too small to show convincingly t\nering, but in general it does.\nIt is clear that the trivial secondary hash func\nof linear probing. It is also worth noting that, in both cases, proceeding to seconc\nto the left is merely a convention\no be made clear which direction has been chosen for a particular hash\n\nlision occurred:\n\nPHL\n\nGCM ORY\n\nhe\n\nnext key\n\no insert, which gives a collision wit:\nry third location to the left in order to find a free s\n\not:\n\n12 PHL. Since h2(HKG) = 3 we\n\nHKG\n\nPHL\n\nGCM ORY\n\ne that this\n\nits primary location blocked\n\nunting to th\nslot) to th\n\ndid not create a block. When we now try to insert GLA, we once again find\n\noy O\ne left from\ne last loca\n\nion\n\nRY. Since ho\n\noverall:\n\nGLA) = 9, we now\nORY, that gets us (starting again from the\n\nry every nin\nback when we reach the\n\nh location.\n\nHKG\n\nPHL\n\nGCM ORY\n\nGLA |.\n\nshare the same primary location with ORY and\no find an empty slot, thus avoiding primary c\nable with the remaining keys given:\n\nferent route when\n\ne that we still have not got any blocks, which is good. Further note that most keys which\nGLA will follow a dif\nustering. Here is the result when filling the\n\nrying\n\nHKG | DCA\n\nPHL | FRA\n\nGCM | AKL | ORY\n\nLAX | GLA\n\nit could equal.\n\n95\n\nion ha(n) = 1 reduces\n\ny well be to the right\n\nSearch complexity. The efficiency of double hashing is even more difficul\nhan that of linear probing, and therefore we shall just give the results without\nWith load factor \\, a successful search requires (1/) n(1/(1 — A)) comparisons on average,\nand an unsuccessful one requires 1/(1 — A). Note that it is the natural logarithm (to base\ne = 2.71828...) that occurs here, rather than the usual logarithm to base 2. T\nable time complexity for search is again constant, i.e. O(1).\n\n1aat this method also avoids secondary clus-\n\nhis approach to that\nary positions\nbut obviously it has\nable.\n\nto compute\na derivation.\n\naus, the hash\n\n\n10.10 Choosing good hash functions\n\nIn principle, any convenient function can be used as a primary hash function. However, what\nis important when choosing a good hash function is to make sure that it spreads the space of\npossible keys onto the set of hash table indices as evenly as possible, or more collisions than\nnecessary will occur. Secondly, it is advantageous if any potential clusters in the space of\npossible keys are broken up (something that the remainder in a division will not do), because\nin that case we could end up with a ‘continuous run’ and associated clustering problems in\nhe hash table. Therefore, when defining hash functions of strings of characters, it is never a\ngood idea to make the last (or even the first) few characters decisive.\n\nWhen choosing secondary hash functions, in order to avoid primary clustering, one has\no make sure that different keys with the same primary position give different results when\nhe secondary hash function is applied. Secondly, one has to be careful to ensure that the\nsecondary hash function cannot result in a number which has a common divisor with the\nsize of the hash table. For example, if the hash table has size 10, and we get a secondary\nhash function which gives 2 (or 4, 6 or 8) as a result, then only half of the locations will be\nchecked, which might result in failure (an endless loop, for example) while the table is still\nhalf empty. Even for large hash tables, this can still be a problem if the secondary hash keys\ncan be similarly large. A simple remedy for this is to always make the size of the hash table\n\na prime number.\n\n10.11 Complexity of hash tables\n\nWe have already seen that insert, search and delete all have O(1) time complexity if the load\nfactor of the hash table is kept reasonably low, e.g. below 0.5, but having higher load factors\ncan considerably slow down the operations.\n\nThe crucial search time complexity of a particular form of hash table is determined by\ncounting the average number of location checks that are needed when searching for items in\nthe table when it has a particular load factor, and that will depend on whether the item is\nfound. The following table shows the average number of locations that need to be checked to\nconduct successful and unsuccessful searches in hash tables with different collision handling\nstrategies, depending on the load factor given in the top row. It shows how the different\n\napproaches and cases vary differently as the table becomes closer to fully loaded.\nStrategy 0.10 0.25 0.50 0.75 0.90 0.99\nSuccessful Search\nDirect chaining 1.05 12 1.25 1.37 1.45 1.48\nLinear probing 1.06 7 1.50 2.50 5.50 50.50\nDouble hashing 1.05 15 1.39 1.85 2.56 4.65\nUnsuccessful search\nDirect chaining 0.10 0.25 0.50 0.75 0.90 0.99\nLinear probing 1.12 39 2.50 8.50 50.50 | 5000.00\nDouble hashing 1.11 33 2.00 4.00 10.00 | 100.00\nIt also shows the considerable advantage that double hashing has over linear probing, partic-\nularly when the load factors become large. Whether or not double hashing is preferable to\n\n96\n\n\ndirect chaining (which appears far superior, but is generally more complex to implement and\nmaintain) is dependent on the circumstances.\nThe following table shows a comparison of the average time complexities for the different\n\npossible implementations of the table interface:\nSearch Insert Delete Traverse\nSorted array O(log2 n) O(n) O(n) O(n)\nBalanced BST | O(loggn) | O(log2n) | O(logs n) O(n)\nHash table O(1) O(1) O(1) O(nlogs n)\nHash tables are seen to perform rather well: the complexity of searching, updating and\n\nretrieving are all independent of table size. In practice, however, when deciding what approach\nto use, it will depend on the mix of operations typically performed. For example, lots of\nrepeated deletions and insertions can cause efficiency problems with some hash table strategies,\nas explained above. To give a concrete example, if there are 4096 entries in a balanced binary\nsearch tree, it takes on average 12.25 comparisons to complete a successful search. On the\nother hand, we can need as few as 1.39 comparisons if we use a hash table, provided that we\nkeep its load factor below 50 percent. Of course, despite their time advantage, we should never\nforget that hash tables have a considerable disadvantage in terms of the memory required to\nimplement them efficiently.\n\n97\n\n\nChapter 11\n\nGraphs\n\nOften it is useful to represent information in a more general graphical form than considered\nso far, such as the following representation of the distances between towns:\n\n44\n\nGlasgow Edinburgh\n\nManchester\n\nBirminghai\n\nSwansea on /\n\nExeter\n\nWith similar structures (maybe leaving out the dis\n\nNewcastle\n\nLondon\n\nances, or replacing them by something\n\nelse), we could represent many other situations, like an underground tunnel network, or a\nnetwork of pipes (where the number label might give the pipe diameters), or a railway map,\n\nor an indication of which cities are linked by flights,\nwe assume it is a network of paths or roads, the num|\ndistances, they might be an indication of how long i\n\nor ferries, or political alliances. Even if\ners do not necessarily have to represent\ntakes to cover the distance in question\n\non foot, so a given distance up a steep hill would take longer than on even ground.\nThere is much more that can be done with such a picture of a situation than just reading\n\noff which place is directly connected with another p\n\n98\n\nace: For example, we can ask ourselves\n\n\nwhether there is a way of getting from A to B at all, or what is the shortest path, or what\nwould be the shortest set of pipes connecting all the locations. There is also the famous\nTravelling Salesman Problem which involves finding the shortest route through the structure\nthat visits each city precisely once.\n\n11.1 Graph terminology\n\nThe kind of structure in the above figure is known formally as a graph. A graph consists of\na series of nodes (also called vertices or points), displayed as nodes, and edges (also called\nlines, links or, in directed graphs, arcs), displayed as connections between the nodes. There\nexists quite a lot of terminology that allows us to specify graphs precisely:\n\nA graph is said to be simple if it has no self-loops (i.e., edges connected at both ends to\nthe same vertex) and no more than one edge connecting any pair of vertices. The remainder\nof this Chapter will assume that, which is sufficient for most practical applications.\n\nIf there are labels on the edges (usually non-negative real numbers), we say that the graph\nis weighted. We distinguish between directed and undirected graphs. In directed graphs (also\ncalled digraphs), each edge comes with one or two directions, which are usually indicated by\narrows. Think of them as representing roads, where some roads may be one-way only. Or\nthink of the associated numbers as applying to travel in one way only: such as going up a hill\nwhich takes longer than coming down. An example of an unweighted digraph is:\n\n9)\naa aN\n\nand an example of a weighted digraph, because it has labels on its edges, is:\n\n4\n\nIn undirected graphs, we assume that every edge can be viewed as going both ways, that is,\nan edge between A and B goes from A to B as well as from B to A. The first graph given at\nthe beginning of this chapter is weighted and undirected.\n\nA path is a sequence of nodes or vertices V1, V2, .-., Un such that v; and vj41 are connected\nby an edge for all 1 < i < n—1. Note that in a directed graph, the edge from v; to vj41\nis the one which has the corresponding direction. A circle is a non-empty path whose first\nvertex is the same as its last vertex. A path is simple if no vertex appears on it twice (with\nthe exception of a circle, where the first and last vertex may be the same — this is because we\nhave to ‘cut open’ the circle at some point to get a path, so this is inevitable).\n\n99\n\n\nAn undirected graph is connected if every pair of vertices has a path connecting them. For\ndirected graphs, the notion of connectedness has two distinct versions: We say that a digraph\nis weakly connected if for every two vertices A and B there is either a path from A to B ora\npath from B to A. We say it is strongly connected if there are paths leading both ways. So,\nin a weakly connected digraph, there may be two vertices i and j such that there exists no\npath from i to j.\n\nA graph clearly has many properties similar to a tree. In fact, any tree can be viewed\nas a simple graph of a particular kind, namely one that is connected and contains no circles.\nBecause a graph, unlike a tree, does not come with a natural ‘starting point’ from which there\nis a unique path to each vertex, it does not make sense to speak of parents and children in\na graph. Instead, if two vertices A and B are connected by an edge e, we say that they are\nneighbours, and the edge connecting them is said to be incident to A and B. Two edges that\nhave a vertex in common (for example, one connecting A and B and one connecting B and C)\nare said to be adjacent.\n\n11.2 Implementing graphs\n\nAll the data structures we have considered so far were designed to hold certain information,\nand we wanted to perform certain actions on them which mostly centred around inserting new\nitems, deleting particular items, searching for particular items, and sorting the collection. At\nno time was there ever a connection between all the items represented, apart from the order in\nwhich their keys appeared. Moreover, that connection was never something that was inherent\nin the structure and that we therefore tried to represent somehow ~— it was just a property\nthat we used to store the items in a way which made sorting and searching quicker. Now, on\nthe other hand, it is the connections that are the crucial information we need to encode in\nthe data structure. We are given a structure which comes with specified connections, and we\nneed to design an implementation that efficiently keeps track of them.\n\nArray-based implementation. The first underlying idea for array-based implementations\nis that we can conveniently rename the vertices of the graph so that they are labelled by\nnon-negative integer indices, say from 0 to n — 1, if they do not have these labels already.\nHowever, this only works if the graph is given explicitly, that is, if we know in advance how\nmany vertices there will be, and which pairs will have edges between them. Then we only\nneed to keep track of which vertex has an edge to which other vertex, and, for weighted\ngraphs, what the weights on the edges are. For unweighted graphs, we can do this quite easily\n\nin an n x n two-dimensional binary array adj, also called a matrix, the so-called adjacency\nmatrix. In the case of weighted graphs, we instead have an n x n weight matrix weights.\nThe array/matrix representations for the two example graphs shown above are then:\n\nA|B|C/D/E A/B{|C/]D|E\n\nO;1),2)3)4 0 1 2|/3)4\nA/O};0]}/1]0)1)0 A/O] 0} 1 | o] 4c\nByj/1)0/;0]/1]0)0 B/ 1 2 0 2|/2)| 6\nC}2)1)/0;0;/0)]1 C}/2} oo}; 3)0),2) 1\nD/}3)0/;0]/1]0)1 D|3] ~o]o}o}0] 1\nE/4]/0/0/0/0]0 E|4] co }]o)] 3 }2)0\n\n100\n\n\nIn the first case, for the unweighted graph, a ‘0’ in position adj [i] [j] reads as false, tha\nis, there is no edge from the vertex 7 to the vertex j. A ‘1’, on the other hand, reads as true,\nindicating that there is an edge. It is often useful to use boolean values here, rather than the\nnumbers 0 and 1, because it allows us to carry out operations on the booleans. In the seconc\ncase, we have a weighted graph, and we have the real-valued weights in the matrix instead,\nusing the infinity symbol oo to indicate when there is no edge.\n\nFor an undirected graph, if there is a 1 in the ith column and the jth row, we know thai\nthere is an edge from vertex i to the vertex with the number j, which means there is also an\nedge from vertex j to vertex i. This means that adj[i][j] == adj[j] [il] will hold for al\ni and j from 0 to n — 1, so there is some redundant information here. We say that such a\n\nmatrix is symmetric — it equals its mirror image along the main diagonal.\n\nMixed implementation. There is a potential problem with the adjacency/weight matrix\n\nrepresentation: If the graph has very many vertices, the associated array will be extremely\nlarge (e.g., 10,000 entries are needed if the graph has just 100 vertices). Then, if the graph is\nsparse (i.e., has relatively few edges), the adjacency matrix contains many 0s and only a few\n1s, and it is a waste of space to reserve so much memory for so little information.\n\nA solution to this problem is to number all the vertices as before, but, rather than using a\ntwo-dimensional array, use a one-dimensional array that points to a linked list of neighbours\nfor each vertex. For example, the above weighted graph can be represented as follows, with\neach triple consisting of a vertex name, connection weight and pointer to the next triple:\n\nO0\\|1|2|3 |4\nIf there are very few edges, we will have very\nshort lists at each entry of the array, thus sav-\neieieliele ing space over the adjacency/weight matrix\nt t t t t representation. This implementation is using\n1|/0/1\\)4|2 so-called adjacency lists. Note that if we are\n1i2i/3/2/3 considering undirected graphs, there is still\na certain amount of redundancy in this rep-\nelelelel¢ resentation, since every edge is represented\n{ { { { twice, once in each list corresponding to the\n3/2 |3 3 two vertices it connects. In Java, this could\n41212 2 be accomplished with something like:\nelele e class Graph {\n{ { Vertex[] heads;\nprivate class Vertex {\n3 |4 .\nint name;\n2(/ double weight;\nele Vertex next;\n| ...//methods for vertices\n}\n4 ...//methods for graphs\n6 +\ne\n\n101\n\n\nPointer-based implementation. The standard pointer-based implementation of binary\ntrees, which is essentially a generalization of linked lists, can be generalized for graphs. In a\nlanguage such as Java, a class Graph might have the following as an internal class:\n\nclass Vertex {\nstring name;\nVertex[] neighbours;\ndouble[] weights;\n\n}\n\nWhen each vertex is created, an array neighbours big enough to accommodate (pointers to)\nall its neighbours is allocated, with (for weighted graphs) an equal sized array weights to\naccommodate the associated weights. We then place the neighbours of each vertex into those\narrays in some arbitrary order. Any entries in the neighbours array that are not needed will\nhold a null pointer as usual. For example, the above weighted graph would be represented\nas follows, with each weight shown following the associated pointer:\n\nol4 [biz ]e's\n\n11.3 Relations between graphs\n\nMany important theorems about graphs rely on formal definitions of the relations between\nthem, so we now define the main relevant concepts. Two graphs are said to be isomorphic\nif they contain the same number of vertices with the same pattern of adjacency, i.e. there is\na bijection between their vertices which preserves the adjacency relations. A subgraph of a\ngraph G is defined as any graph that has a vertex set which is a subset of that of G, with\nadjacency relations which are a subset of those of G. Conversely, a supergraph of a graph G\nis defined as any graph which has G as a subgraph. Finally, a graph G is said to contain\nanother graph H if there exists a subgraph of G that is either H or isomorphic to H.\n\nA subdivision of an edge e with endpoints u and v is simply the pair of edges e1, with\nendpoints u and w, and e2, with endpoints w and v, for some new vertex w. The reverse\noperation of smoothing removes a vertex w with exactly two edges e; and e2, leaving an edge\ne connecting the two adjacent vertices u and v:\n\nsubdivision\nee\nsmoothing\n\n102\n\n\nA subdivision of a graph G can be defined as a graph resulting from the subdivision of edges\nin G. Two graphs G and H can then be defined as being homeomorphic if there is a graph\nisomorphism from some subdivision of G to some subdivision of H.\n\nAn edge contraction removes an edge from a graph and merges the two vertices previously\nconnected by it. This can lead to multiple edges between a pair of vertices, or self-loops\nconnecting a vertex to itself. These are not allowed in simple graphs, in which case some\nedges may need to be deleted. Then an undirected graph H is said to be a minor of another\nundirected graph G if a graph isomorphic to H can be obtained from G by contracting some\nedges, deleting some edges, and deleting some isolated vertices.\n\n11.4 Planarity\n\nA planar graph is a graph that can be embeded in a plane. In other words, it can be drawn on a\nsheet of paper in such a way that no edges cross each other. This is important in applications\nsuch as printed circuit design.\n\nNote that it is clearly possible for planar graphs to be drawn in such a way that their\nedges do cross each other, but the crucial thing is that they can be transformed (by moving\nvertices and/or deforming the edges) into a form without any edges crossing. For example,\nthe following three diagrams all represent the same planar graph:\n\nXM 7 bs\n\nThis graph is the fully connected graph with four vertices, known as Ky. Clearly all sub-graphs\nof this will also be planar.\n\nIt is actually quite difficult to formulate general algorithms for determining whether a\ngiven graph is planar. For small graphs, it is easy to check systematically that there are no\npossible vertex repositionings or edge deformations that will bring the graph into explicitly\nplanar form. Two slightly larger graphs than Ky that can be shown to be non-planar in this\nway are the fully connected graph with five vertices, known as Ks, and the graph with three\nvertices fully connected to three other vertices, known as 13,3:\n\na Dx\n\nClearly, any larger graph that contains one of these two non-planar graphs as a subgraph\nmust also be non-planar iteslf, and any subdivision or smoothing of edges will have no effect\n\n103\n\n\non the planarity. In fact, it can be proved that these two graphs form the basis of some useful\n\ntheorems about planarity. The most well.\nthat “a finite graph is planar if and only i\n\nto, or a subdivision of, Ks or K33”. An\n\ntheorem which states that “a finite grap\nas a minor”.\n\nA good general approach for testing\ngiven graph that can be transformed int\nbut algorithms do exist which allow a g\ntime complexity O(n). Exercise: find ow\n\n-known of these is Kuratowski’s theorem which states\nit does not contain a subgraph that is homeomorphic\nother, based on the concept of minors, is Wagner’s\n1 is planar if and only if it does not have Ks or K33\n\nplanarity is therefore to search for subgraphs of the\no Ks or K33. This is not entirely straightforward,\nraph with n vertices to be tested for planarity with\nexactly how these algorithms work.\n\n11.5 Traversals — systematically visiting all vertices\n\nIn order to traverse a graph, i.e. systematica\nfor exploring graphs which guarantees that\nunlike trees, graphs do not have a root vertex, there is no natural place to start a traversal,\nand therefore we assume that we are given, or randomly pick, a starting vertex 7. There are\ntwo strategies for performing graph traversal.\n\nThe first is known as breadth first traversal: We start wit\n\nly visit all its vertices, we clearly need a strategy\nwe do not miss any edges or vertices. Because,\n\n1 the given vertex i. Then we\nbe possible no matter which implement\nuse), placing them in an initially empty qu We then remove the first vertex\nqueue and one by one put its neighbours at the end of the q\nvertex in the queue and again put its neighbours at the end of the queue. We do\nthe queue is empty.\n\nHowever, there is no reason why this basic algorithm shou\ncircle in the graph, like A, B, C in the first unweighted graph al\nwe have already visited, and thus we would run into an infinite loop (visiting A’s neighbours\nputs B onto the queue, visiting that (eventually) gives us C, anc\nwe get A again). To avoid this we create a second array done\ntrue if we have already visited the vertex with number j, anc\nabove algorithm, we only add a vertex j to the queue if done[j] is false. T\ntrue. This way, we will not visit any vertex more than once,\nare discussing,\n\nation we\nfrom the\nthe next\n\nvisit its neighbours one by one (which must\neue.\n\nueue. We then visit\nhis until\nd ever terminate. If there is a\n\n90ve, we would revisit a vertex\n\nonce we reach C in the queue,\nof booleans, where done [j] is\nit is false ot\n\n1erwise. In the\n1en we mark it\n\nas done by setting done [j]\nand for a finite graph, our algorithm is bound to terminate. In the example we\noreadth first search starting at A might yield: A, B, D, C, E.\n\nTo see why this is called breadth first search, we can imagine a tree being\nway, where the starting vertex is the root, and the children of each vertex are its neighbours\nthat haven’t already been visited). We would then first follow all the edges emanating from\nhe root, leading to all the vertices on level 1, then 1e level below,\nand so on, until we find all the vertices on the ‘lowest’ level.\n\nThe second traversal strategy is known as depth first traversal: Given a vertex i to start\nrom, we now put it on a stack rather than a queue (recall that in a stack, the next item to\n\nguilt up in this\n\nfind all the vertices on t.\n\noe removed at any time is the last one that was pu\nhe stack, mark it as done as for breadth first travers:\nother, and put them onto the stack. We then repeate\nmark it as done, and put its neighbours on the stack,\ndone, just as we did for breadth first traversal. For\n\n104\n\non the stack). Then we take it from\nal, look up its neighbours one after the\ndly pop the next vertex from the stack,\nprovided they have not been marked as\nhe example discussed above, we might\n\nstarting from A) get: A, B, C, E, D. Again, we can see why this is called depth first by\n\n\nformulating the traversal as a search tree and looking at the order in which the items are\nadded and processed.\n\nNote that with both breadth first and depth first, the order of the vertices depends on the\nimplementation. There is no reason why A’s neighbour B should be visited before D in the\nexample. So it is better to speak of a result of depth first or breadth first traversal, rather\nthan of the result. Note also that the only vertices that will be listed are those in the same\nconnected component as A. If we have to ensure that all vertices are visited, we may need\nto start the traversal process with a number of different starting vertices, each time choosing\none that has not been marked as done when the previous traversal terminated.\n\nExercises: Write algorithms, in pseudocode, to (1) visit all nodes of a graph, and (2) decide\nwhether a given graph is connected or not. For (2) you will actually need two algorithms, one\nfor the strong notion of connectedness, and another for the weak notion.\n\n11.6 Shortest paths — Dijkstra’s algorithm\n\nA common graph based problem is that we have some situation represented as a weighted di-\ngraph with edges labelled by non-negative numbers and need to answer the following question:\nFor two particular vertices, what is the shortest route from one to the other?\n\nHere, by “shortest route” we mean a path which, when we add up the weights along its\nedges, gives the smallest overall weight for the path. This number is called the length of the\npath. Thus, a shortest path is one with minimal length. Note that there need not be a unique\nshortest path, since several paths might have the same length. In a disconnected graph there\nwill not be a path between vertices in different components, but we can take care of this by\nusing co once again to stand for “no path at all”.\n\nNote that the weights do not necessarily have to correspond to distances; they could, for\nexample, be time (in which case we could speak of “quickest paths”) or money (in which case\nwe could speak of “cheapest paths”), among other possibilities. By considering “abstract”\ngraphs in which the numerical weights are left uninterpreted, we can take care of all such\nsituations and others. But notice that we do need to restrict the edge weights to be non-\nnegative numbers, because if there are negative numbers and cycles, we can have increasingly\nlong paths with lower and lower costs, and no path with minimal cost.\n\nApplications of shortest-path algorithms include internet packet routing (because, if you\nsend an email message from your computer to someone else, it has to go through various\nemail routers, until it reaches its final destination), train-ticket reservation systems (that\nneed to figure out the best connecting stations), and driving route finders (that need to find\n\nan optimum route in some sense).\n\nDijkstra’s algorithm. In turns out that, if we want to compute the shortest path from\na given start node s to a given end node z, it is actually most convenient to compute the\nshortest paths from s to all other nodes, not just the given node z that we are interested\nin. Given the start node, Dijkstra’s algorithm computes shortest paths starting from s and\nending at each possible node. It maintains all the information it needs in simple arrays, which\nare iteratively updated until the solution is reached. Because the algorithm, although elegant\nand short, is fairly complicated, we shall consider it one component at a time.\n\nOverestimation of shortest paths. We keep an array D of distances indexed by the\nvertices. The idea is that D[z] will hold the distance of the shortest path from s to z when\n\n105\n\n\nthe algorithm finishes. However, before the algorithm finishes, D[z] is the best overestimate\nwe currently have of the distance from s to z. We initially have D[s] = 0, and set D[z] = co\nfor all vertices z other than the start node s. Then the algorithm repeatedly decreases the\noverestimates until it is no longer possible to decrease them further. When this happens, the\nalgorithm terminates, with each estimate fully constrained and said to be tight.\n\nImproving estimates. The general idea is to look systematically for shortcuts. Suppose\nthat, for two given vertices u and z, it happens that D[u] + weight[u]{z] < D[z]. Then there\nis a way of going from s to u and then to z whose total length is smaller than the current\noverestimate D[z] of the distance from s to z, and hence we can replace D[z] by this better\nestimate. This corresponds to the code fragment\n\nif ( D[u] + weight[u] [z] < D[z] )\nDiz] = D[u] + weight [u] [z]\n\nof the full algorithm given below. The problem is thus reduced to developing an algorithm that\nwill systematically apply this improvement so that (1) we eventually get the tight estimates\npromised above, and (2) that is done as efficiently as possible.\n\nDijkstra’s algorithm, Version 1. The first version of such an algorithm is not as efficient\nas it could be, but it is relatively simple and certainly correct. (It is always a good idea to\nstart with an inefficient simple algorithm, so that the results from it can be used to check\nthe operation of a more complex efficient algorithm.) The general idea is that, at each stage\nof the algorithm’s operation, if an entry D[u] of the array D has the minimal value among\nall the values recorded in D, then the overestimate D[u] must actually be tight, because the\nimprovement algorithm discussed above cannot possibly find a shortcut.\nThe following algorithm implements that idea:\n\n// Input: A directed graph with weight matrix ‘weight’ and\n// a start vertex ‘s’.\n// Output: An array ‘D’ of distances as explained above.\n\n// We begin by buiding the distance overestimates.\nD[s] = 0 // The shortest path from s to itself has length zero.\n\nfor ( each vertex z of the graph ) {\nif ( z is not the start vertex s )\nD(z] = infinity // This is certainly an overestimate.\n\n}\n\n// We use an auxiliary array ‘tight’ indexed by the vertices,\n// that records for which nodes the shortest path estimates\n// are ‘‘known’’ to be tight by the algorithm.\n\nfor ( each vertex z of the graph ) {\n\ntight[z] = false\n}\n\n106\n\n\n// We now repeatedly update the arrays ‘D’ and ‘tight’ until\n// all entries in the array ‘tight’ hold the value true.\n\nrepeat as many times as there are vertices in the graph {\nfind a vertex u with tight[u] false and minimal estimate D[u]\n\ntight [u] true\n\nfor ( each vertex z adjacent to u )\nif ( D[u] + weight[u][z] < Dz] )\n\nD[z] = D[{u] + weight [u] [z]\n\n}\n\n// Lower overestimate exists.\n\n// At this point, all entries of array ‘D’ hold tight estimates.\n\nIt is clear that when this algorithm\n\nhe lengths of the shortest pai\nare actually tight, i.e. are the\nhat an initial sub-path of as\nyou wish to navigate from a\nz happens to go through a ce\nwo paths, one going from s\nfinal sub-path). Given that th\nsub-path has to be a shortest\n\nminim\n\nvertex\n\n1ortes\n\nou (\n\ne who\n\npa\n\nrom s to z by replacing the initial sub-pa'\n\na shorter path from s to u bu\n\nhis tree starting from the roo\nrather than just its length, we\n\nhe end point to the start po\n\n‘or non-connected vertices.\n\nThe time complexity of this algorithm is clearly O(n?) where n\nsince there are operations of O(n) nested within the repeat of O(n).\n\nA simple example.\n\nIf, as tends to be the case in\n\nalso from\n\nhat is, t\n\npractice,\n\nt,\n\na\n\n‘predecessor’ or ‘previous vertex’ of each\n\nThe a\n\nint.\n\nh to u by a shorter pat.\n\nany start vertex, there is a tree of shortest paths from that vertex\nreason is that shortest paths cannot have cycles. Implicitly, Dij\n\n1e start vertex.\nwe also wish to comput\n\nvertex, so that the pat.\n\ns to the final destination z. Now it\n\ngorithm can clearly also\n\nfinishes, the entries of D cannot hold under-estimates of\nhs. What is perhaps not so clear is why the estimates it holds\nal path lengths. In order to understand why, first notice\npath is itself a shortest path. To see this, suppose that\ns to a vertex z, and that the shortest path from s to\nrtain vertex u. Then your path from s to z can be split into\nan initial sub-path) and the other going\ne, unsplit path is a shortest path from s\nh from s to u, for if not, then you could sho\n\nrom wu to z (a\no z, the initial\nrten your path\nd not only give\nfollows that for\nr vertices. The\n\n1, which wou\n\no all othe:\nkstra’s algorithm constructs\n\ne the route of shortest path,\n\nso need to introduce a third array pred to keep track of the\n\nan be followed back from\nye adapted to work with\n\n1 Cc\n\nnon-weighted graphs by assigning a suitable weight matrix of 1s\n\n(node 4) in the weighted graph we looked at before:\n\n4\n\n107\n\nfor connected vertices and Os\n\nis the number of vertices,\n\nSuppose we want to compute the shortest path from A (node 0) to E\n\n\n\nA direct implementation of the above algorithm, with some code added to print out the status\nof the three arrays at each intermediate stage, gives the following output, in which “oo” is used\nto represent the infinity symbol “oo”:\n\nComputing shortest paths from A\n\nlA B c D E\n~-----+- 4o----- +--+ +--+ +--+\nD 10 00 00 00 00\ntight |no no no no no\npred. |none none none none none\n\nVertex A has minimal estimate, and so is tight.\n\nNeighbour B has estimate decreased from oo to 1 taking a shortcut via A.\nNeighbour D has estimate decreased from oo to 4 taking a shortcut via A.\n\nlA B c D E\n-------- $o-------------- == +--+ +--+\nD 10 1 00 4 00\ntight lyes no no no no\npred. Inone A none A none\n\nVertex B has minimal estimate, and so is tight.\n\nNeighbour A is already tight.\n\nNeighbour C has estimate decreased from oo to 3 taking a shortcut via B.\nNeighbour D has estimate decreased from 4 to 3 taking a shortcut via B.\nNeighbour E has estimate decreased from oo to 7 taking a shortcut via B.\n\nlA B c D E\n-------- $o-------------- == +--+ +--+\nD 10 1 3 3 7\ntight lyes yes no no no\npred. Inone A B B B\n\nVertex C has minimal estimate, and so is tight.\n\nNeighbour B is already tight.\nNeighbour D has estimate unchanged.\nNeighbour E has estimate decreased from 7 to 4 taking a shortcut via C.\n\nlA B c D E\n-------- $o-------------- == +--+ +--+\nD 10 1 3 3 4\ntight lyes yes yes no no\npred. Inone A B B Cc\n\n108\n\n\nVertex D has minimal estimate, and so is tight.\n\nNeighbour E has estimate unchanged.\n\nlA B c D E\n-------- $o-------------- == +--+ +--+\nD 10 1 3 3 4\ntight lyes yes yes yes no\npred. Inone A B B Cc\n\nVertex E has minimal estimate, and so is tight.\n\nNeighbour C is already tight.\nNeighbour D is already tight.\n\nlA B c D E\n-------- $o-------------- == +--+ +--+\nD 10 1 3 3 4\ntight lyes yes yes yes yes\npred. Inone A B B Cc\n\nEnd of Dijkstra’s computation.\n\nA shortest path from Ato Eis: ABCE.\n\nOnce it is clear what is happening at each stage, it is usually more convenient to adopt a\nshorthand notation that allows the whole process to be represented in a single table. For\nexample, using a “*” to represent tight, the distance, status and predecessor for each node at\neach stage of the above example can be listed more concisely as follows:\n\nStage | A B Cc D E\n------- poeee--- eee +--+ +--+ +--+\n1 | 0 00 00 00 00\n2 | O* 1 A 00 4 A 00\n3 | O* 1* A 3 B 3 B 7 B\n4 | O* 1* A 3 * B 3 B 4 Cc\n5 | 0 * 1* A 3*B 3*B 4 C¢\n6 | 0 * 1* A 3*B 3*B 4*C\n\nA shortest path from Ato Eis: ABCE.\n\nDijkstra’s algorithm, Version 2. The time complexity of Dijkstra’s algorithm can be\nimproved by making use of a priority queue (e.g., some form of heap) to keep track of which\nnode’s distance estimate becomes tight next. Here it is convenient to use the convention that\nlower numbers have higher priority. The previous algorithm then becomes:\n\n109\n\n\n//\n//\n\nInput: A directed graph with weight matrix ‘weight’ and\n\na start vertex\n\n‘3’.\n\n// Output: An array ‘D’ of distances as explained above.\n\n// We begin by buiding the distance overestimates.\n\nD[s] = 0\n\n// The shortest path from s to itself has length zero.\n\nfor ( each vertex z of the graph ) {\n\n}\n\nif ( z is not the star\nD(z] = infinity\n\nt vertex s )\n// This is certainly an overestimate.\n\n// Then we set up a priority queue based on the overestimates.\n\nCreate a priority queue containing all the vertices of the graph,\nwith the entries of D as the priorities\n\n// Then we implicitly build the path tree discussed above.\n\nwhile ( priority queue is not empty ) {\n\n}\n\n// The next vertex of\n\nthe path tree is called u.\n\nu = remove vertex with smallest priority from queue\nfor ( each vertex z in the queue which is adjacent to u ) {\nif ( D[u] + weight[u][z] < Diz] ) f\nD[z] = D[u] + weight [u] [z] // Lower overestimate exists.\nChange the priority of vertex z in queue to D[z]\n\n// At this point, all entries of array ‘D’ hold tight estimates.\n\nIf the priority queue is implemented as a Binary or Binomial heap, initializing D and creating\n\nthe priority queue both have com\n\nand t.\n\n1at is negligible compared\n\nchanging the priorities of element’\nheap tree by “bubbling up”, anc\n\nheigh\n\nwhere e is the number of edges in\nD is O((e + n)loggn). Thus, the total time complexity of this form of Dijkstra’s algorithm\n\nis O\n\na\n\nplexity O(n), where n is the number of vertices of the graph,\nto the rest of the algorithm. Then removing vertices and\nts in the priority queue require some rearrangement of the\nthat takes O(logyn) steps, because that is the maximum\n\nof the tree. Removals happen O(n) times, and priority changes happen O(e) times,\n\nhe graph, so the cost of maintaining the queue and updating\n\ne+ n)logyn). Using a Fibonacci heap for the priority queue allows priority updates of\n\nO(1) complexity, improving the overall complexity to O(e + nlog n).\n\nIn\n\ncomp.\n\na fully connected graph,\n\nhe number of edges e will be O(n”), and hence the time\n\nexity of this algorithm is O(n?log2n) or O(n?) depending on which kind of priority\n\nqueue is used. So, in that case,\nprevious simpler O(n?) algorithm. However, in practice, many graphs tend to be much more\n\nhe time complexity is actually greater than or equal to the\n\n110\n\n\nsparse with e = O(n). That is, there are usually not many more edges than vertices, and in\nthis case the time complexity for both priority queue versions is O(nlogg n), which is a clear\nimprovement over the previous O(n?) algorithm.\n\n11.7 Shortest paths — Floyd’s algorithm\n\nIf we are not only interested in finding the shortest path from one specific vertex to all the\nothers, but the shortest paths between every pair of vertices, we could, of course, apply\nDijkstra’s algorithm to every starting vertex. But there is actually a simpler way of doing\nthis, known as Floyd’s algorithm. This maintains a square matrix ‘distance’ which contains\nthe overestimates of the shortest paths between every pair of vertices, and systematically\ndecreases the overestimates using the same shortcut idea as above. If we also wish to keep\ntrack of the routes of the shortest paths, rather than just their lengths, we simply introduce\na second square matrix ‘predecessor’ to keep track of all the ‘previous vertices’.\n\nIn the algorithm below, we attempt to decrease the estimate of the distance from each\nvertex s to each vertex z by going systematically via each possible vertex u to see whether\nthat is a shortcut; and if it is, the overestimate of the distance is decreased to the smaller\noverestimate, and the predecessor updated:\n\n// Store initial estimates and predecessors.\n\nfor ( each vertex s ) {\nfor ( each vertex z ) {\ndistance[s][z] = weight [s] [z]\npredecessor[s][z] = s\n\n+\n// Improve them by considering all possible shortcuts u.\n\nfor ( each vertex u ) {\nfor ( each vertex s ) {\nfor ( each vertex z ) {\nif ( distance[s] [u]+distance[u] [z] < distance[s][z] ) {\ndistance[s] [z] = distance[s] [u]+distance[u] [z]\npredecessor [s] [z] = predecessor [u] [z]\n\n}\n\nAs with Dijkstra’s algorithm, this can easily be adapted to the case of non-weighted graphs\nby assigning a suitable weight matrix of Os and 1s.\n\nThe time complexity here is clearly O(n?), since it involves three nested for loops of O(n).\nThis is the same complexity as running the O(n?) Dijkstra’s algorithm once for each of the n\npossible starting vertices. In general, however, Floyd’s algorithm will be faster than Dijkstra’s,\neven though they are both in the same complexity class, because the former performs fewer\n\n111\n\n\ninstructions in each run through the loops. However, if the graph is sparse with e = O(n),\nthen multiple runs of Dijkstra’s algorithm can be made to perform with time complexity\nO(n?log n), and be faster than Floyd’s algorithm.\n\nA simple example. Suppose we want to compute the lengths of the shortest paths between\nall vertices in the following undirected weighted graph:\n\n— ~~\n( A} 1 é B )\nfo St\nye 4 | 2\nye 7\nCoy 8 (py 1 8)\nLEZ New, SY\n—— ss\n~~ 19\nWe start with distance matrix based on the connection weights, and trivial predecessors:\nA | B cC|} DIE A |B Cc; DIE\nA 0 1 14] 4 | w Ay A}|AJ]A!]AJA\nS B 1 0 | co |] w 2 By] B B B B B\ntart\nC || 14 | oc 0 8 10 Cc Cc Cc | C Cc Cc\nD 4 | co 0 1 D/;|D|DjDIDdDIOD\nE || 09 2 10 1 0 E E E/E E/E\nThen for each vertex in turn we test whether a shortcut via that vertex reduces any of the\ndistances, and update the distance and predecessor arrays with any reductions found. The\nfive steps, with the updated entries in quotes, are as follows::\nA B Cc D|E A;}B{;}|C}]D]E\nA 0 1 4 4 | cw Ay A}|AJ]A]A]A\nA: B 1 O | ‘15’ | ‘5’ | 2 B B B | ‘A’ | ‘A’ | B\n, Cy] 14 | 15°) 0 8 10 Cc Cc | ‘A’ |] C Cc Cc\nD 4 5’ 8 0 1 DJ] D|‘A’) D}| DI} D\nE |} oo 2 0 1 0 E E E/E E E\nA | B C D|E A | B cC|} DIE\nA 0 1 4) 4 | ‘3 AJ A |AJA]A | ‘B’\nB: B 1 0 5] 5 2 B B B|A|A |B\n, Cy] 14) 15) 0 8 10 Cc cC;}AIC Cc Cc\nD 4 5 8 0 1 DJ] D|As)D/D]D\nE |] ‘3’ | 2 0 1 0 E/]‘B’| E | E E E\nA |B Cc; DIE A |B C D|E\nA 0 1 4| 4 3 Aj) A} A}|AJ]AJB\nC: B 1 0 5} 5 2 B B B|A|A|B\n. Cy] 14] 15) 0 8 | 10 Cc Cc }AIC Cc Cc\nD 4 5 8 0 1 D}D;|As}DIDIOD\nE 3 2 0 1 0 E B E E E E\n\n112\n\n\nB C D A/B/C |D|E\n\nA 0 1 12’? | 4 3 Aj A} A |‘D| A | B\n\nD: B 1 0 13’ | 5 2 B]| B |B /‘D’?}| A | B\n\n, C || 12’ | 13’ | 0 8 | ‘9 Ci} ‘Db’; A | C C | ‘D’\n\nD 4 5 8 0 1 D| D;}|A}DIDi{IOD\n\nE 3 2 ‘9’ 1 0 E || B E |‘D’| E | E\n\nA B Cc D|E A;}/B/]}C}]D|E\n\nA 0 1 12 4 3 Aj A}|AJ]D/]A]B\n\nE: B 1 0 | ‘lV |} 88 | 2 By} B | BD | ‘E’|] B\n\n; Cc] i2)a1v} of 8 | 9 ci] Dm] Cc}]ciD\n\nD 4 3 8 0 1 D| D|‘E’}| D| Di} OD\n\nE 3 2 9 1 0 E; B;/|E|;|D{|E]E\nThe algorithm finishes with the matrix of shortest distances and the matrix of associated\npredecessors. So the shortest distance from C to B is 11, and the predecessors of B are E,\nthen D, then C, giving the path C D E B. Note that updating a distance does not necessarily\nmean updating the associated predecessor — for example, when introducing D as a shortcut\n\nbetween C and B, the predecessor of B remains A.\n\n11.8 Minimal spanning trees\n\nWe now move on to another common graph-based problem. Suppose you have been given a\nweighted undirected graph such as the following:\n\nWe could think of the vertices as representing houses, and the weights as the distances between\nthem. Now imagine that you are tasked with supplying all these houses with some commodity\nsuch as water, gas, or electricity. For obvious reasons, you will want to keep the amount of\ndigging and laying of pipes or cable to a minimum. So, what is the best pipe or cable layout\nthat you can find, i.e. what layout has the shortest overall length?\n\nObviously, we will have to choose some of the edges to dig along, but not all of them. For\nexample, if we have already chosen the edge between A and D, and the one between B and\nD, then there is no reason to also have the one between A and B. More generally, it is clear\nthat we want to avoid circles. Also, assuming that we have only one feeding-in point (it is of\nno importance which of the vertices that is), we need the whole layout to be connected. We\nhave seen already that a connected graph without circles is a tree.\n\n113\n\n\nHence, what we are looking for is a minimal spanning tree of the graph. A spanning tree\nof a graph is a subgraph that is a tree which connects all the vertices together, so it ‘spans’ the\noriginal graph but using fewer edges. Here, minimal refers to the sum of all the weights of the\nedges contained in that tree, so a minimal spanning tree has total weight less than or equal\nto the total weight of every other spanning tree. As we shall see, there will not necessarily be\na unique minimal spanning tree for a given graph.\n\nObservations concerning spanning trees. For the other graph algorithms we have cov-\nered so far, we started by making some observations which allowed us to come up with an\nidea for an algorithm, as well as a strategy for formulating a proof that the algorithm did\nindeed perform as desired. So, to come up with some ideas which will allow us to develop an\nalgorithm for the minimal spanning tree problem, we shall need to make some observations\nabout minimal spanning trees. Let us assume, for the time being, that all the weights in the\nabove graph were equal, to give us some idea of what kind of shape a minimal spanning tree\nmight have under those circumstances. Here are some examples:\n\nO O \\ O\nWe can immediately notice that their general shape is such that if we add any of the remaining\nedges, we would create a circle. Then we can see that going from one spanning tree to another\ncan be achieved by removing an edge and replacing it by another (to the vertex which would\notherwise be unconnected) such that no circle is created. These observations are not quite\n\nsufficient to lead to an algorithm, but they are good enough to let us prove that the algorithms\nwe find do actually work.\n\nGreedy Algorithms. We say that an algorithm is greedy if it makes its decisions based only\non what is best from the point of view of ‘local considerations’, with no concern about how the\ndecision might affect the overall picture. The general idea is to start with an approximation,\nas we did in Dijkstra’s algorithm, and then refine it in a series of steps. The algorithm is\ngreedy in the sense that the decision at each step is based only on what is best for that next\nstep, and does not consider how that will affect the quality of the final overall solution. We\nshall now consider some greedy approaches to the minimal spanning tree problem.\n\nPrim’s Algorithm — A greedy vertex-based approach. Suppose that we already have\na spanning tree connecting some set of vertices S. Then we can consider all the edges which\nconnect a vertex in S' to one outside of S, and add to S one of those that has minimal weight.\nThis cannot possibly create a circle, since it must add a vertex not yet in S. This process can\nbe repeated, starting with any vertex to be the sole element of S, which is a trivial minimal\nspanning tree containing no edges. This approach is known as Prim’s algorithm.\n\nWhen implementing Prim’s algorithm, one can use either an array or a list to keep track\nof the set of vertices S reached so far. One could then maintain another array or list closest\nwhich, for each vertex i not yet in S, keeps track of the vertex in S closest to 7. That is, the\n\n114\n\n\nvertex in S which has an edge to 7 with minimal weight. If closest also keeps track of the\nweights of those edges, we could save time, because we would then only have to check the\nweights mentioned in that array or list.\n\nFor the above graph, starting with S = {A}, the tree is built up as follows:\n\nA A\n\nIt is slightly more challenging to produce a convincing argument that this algorithm really\nworks than it has been for the other algorithms we have seen so far. It is clear that Prim’s\nalgorithm must result in a spanning tree, because it generates a tree that spans all the vertices,\nbut it is not obvious that it is minimal. There are several possible proofs that it is, but none\nare straightforward. The simplest works by showing that the set of all possible minimal\nspanning trees X; must include the output of Prim’s algorithm.\n\nLet Y be the output of Prim’s algorithm, and X; be any minimal spanning tree. The\nfollowing illustrates such a situation:\n\nWe don’t actually need to know what X, is — we just need to know the properties it must\nsatisfy, and then systematically work through all the possibilities, showing that Y is a minimal\nspanning tree in each case. Clearly, if X; = Y, then Prim’s algorithm has generated a minimal\nspanning tree. Otherwise, let e be the first edge added to Y that is not in X;. Then, since\nX, is a spanning tree, it must include a path connecting the two endpoints of e, and because\ncircles are not allowed, there must be an edge in X, that is not in Y, which we can call f.\nSince Prim’s algorithm added e rather than f, we know weight(e) < weight(f). Then create\ntree X that is X,; with f replaced by e. Clearly X2 is connected, has the same number of\nedges as X,, spans all the vertices, and has total weight no greater than X,, so it must also\n\n115\n\n\nbe a minimal spanning tree. Now we can repeat this process until we have replaced all the\nedges in X, that are not in Y, and we end up with the minimal spanning tree X,, = Y, which\ncompletes the proof that Y is a minimal spanning tree.\n\nThe time complexity of the standard Prim’s algorithm is O(n?) because at each step we\nneed to choose a vertex to add to S, and then update the closest array, not dissimilar to\nthe simplest form of Dijkstra’s algorithm. However, as with Dijkstra’s algorithm, a Binary\nor Binomial heap based priority queue can be used to speed things up by keeping track of\nwhich is the minimal weight vertex to be added next. With an adjacency list representation,\nthis can bring the complexity down to O((e + n)logn). Finally, using the more sophisticated\nFibonacci heap for the priority queue can improve this further to O(e + nlogg n). Thus, using\nthe optimal approach in each case, Prim’s algorithm is O(nlogg n) for sparse graphs that have\ne = O(n), and O(n?) for highly connected graphs that have e = O(n”).\n\nJust as with Floyd’s versus Dijkstra’s algorithm, we should consider whether it eally is\nnecessary to process every vertex at each stage, because it could be sufficient to only check\nactually existing edges. We therefore now consider an alternative edge-based strategy:\n\nKruskal’s algorithm — A greedy edge-based approach. This algorithm does not con-\nsider the vertices directly at all, but builds a minimal spanning tree by considering and adding\nedges as follows: Assume that we already have a collection of edges T. Then, from all the\nedges not yet in T, choose one with minimal weight such that its addition to T does not\nproduce a circle, and add that to T. If we start with T being the empty set, and continue\nuntil no more edges can be added, a minimal spanning tree will be produced. This approach\nis known as Kruskal’s algorithm.\nFor the same graph as used for Prim’s algorithm, this algorithm proceeds as follows:\n\nche 8p MY\nsy AA\n\nIn practice, Kruskal’s algorithm is implemented in a rather different way to Prim’s algorithm.\nThe general idea of the most efficient approaches is to start by sorting the edges according to\ntheir weights, and then simply go through that list of edges in order of increasing weight, and\neither add them to T, or reject them if they would produce a circle. There are implementations\nof that which can be achieved with overall time complexity O(elog2 e), which is dominated by\nthe O(eloge e) complexity of sorting the e edges in the first place.\n\nThis means that the choice between Prim’s algorithm and Kruskal’s algorithm depends on\nthe connectivity of the particular graph under consideration. If the graph is sparse, i.e. the\n\n116\n\n\nnumber of edges is not much more than the number of vertices, then Kruskal’s algorithm will\nhave the same O(nloggn) complexity as the optimal priority queue based versions of Prim’s\nalgorithm, but will be faster than the standard O(n?) Prim’s algorithm. However, if the graph\nis highly connected, i.e. the number of edges is near the square of the number of vertices, it\nwill have complexity O(nloggn) and be slower than the optimal O(n?) versions of Prim’s\nalgorithm.\n\n11.9 Travelling Salesmen and Vehicle Routing\n\nNote that all the graph algorithms we have considered so far have had polynomial time com-\nplexity. There are further graph based problems that are even more complex.\n\nProbably the most well known of these is the Travelling Salesman Problem, which involves\nfinding the shortest path through a graph which visits each node precisely once. There are\ncurrently no known polynomial time algorithms for solving this. Since only algorithms with\nexponential complexity are known, this makes the Travelling Salesman Problem difficult even\nor moderately sized n (e.g., all capital cities). Exercise: write an algorithm in pseudocode\nhat solves the Travelling Salesman Problem, and determine its time complexity.\n\nA variation of the shortest path problem with enormous practical importance in trans-\n\nportation is the Vehicle Routing Problem. This involves finding a series of routes to service\na number of customers with a fleet of vehicles with minimal cost, where that cost may be\nhe number of vehicles required, the total distance covered, or the total driver time required.\nOften, for practical instances, there are conflicts between the various objectives, and there is a\ntrade-off between the various costs which have to be balanced. In such cases, a multi-objective\n\noptimization approach is required which returns a Pareto front of non-dominated solutions,\ni.e. a set solutions for which there are no other solutions which are better on all objectives.\nAlso, in practice, there are usually various constraints involved, such as fixed delivery time-\nwindows, or limited capacity vehicles, that must be satisfied, and that makes finding good\nsolutions even more difficult.\n\nSince exact solutions to these problems are currently impossible for all but the smallest\ncases, heuristic approaches are usually employed, such as evolutionary computation, which\ndeliver solutions that are probably good but cannot be proved to be optimal. One popular\napproach is to maintain a whole population of solutions, and use simulated evolution by\nnatural selection to iteratively improve the quality of those solutions. That has the additional\nadvantage of being able to generate a whole Pareto front of solutions rather than just a single\n\nsolution. This is currently still a very active research area.\n\n117\n\n\nChapter 12\nEpilogue\n\nHopefully the reader will agree that these notes have achieved their objective of introducing\nthe basic data structures used in computer science, and showing how they can be used in the\ndesign of useful and efficient algorithms. The basic data structures (arrays, lists, stacks, queues\nand trees) have been employed throughout, and used as the basis of the crucial processes,\nas storing, sorting and searching data, which underly many computer science applications.\nIt has been demonstrated how ideas from combinatorics and probability theory can be used\nto compute the efficiency of algorithms as a function of problem size. We have seen how\nconsiderations of computational efficiency then drive the development of more complex data\nstructures (such as binary search trees, heaps and hash tables) and associated algorithms.\nGeneral ideas such as recursion and divide-and-conquer have been used to provide more\nefficient algorithms, and inductive definitions and invariants have been used to establish proofs\nof correctness of algorithms. Throughout, abstract data types and pseudo-code have allowed\nan implementation independent approach that facilitates application to any programming\nenvironment in the future.\n\nClearly, these notes have only been able to provide a brief introduction to the topic, and\nthe algorithms and data structures and efficiency computations discussed have been relatively\nsimple examples. However, having a good understanding of these fundamental ideas and\ndesign patterns allows them to be easily expanded and elaborated to deal with the more\ncomplex applications that will inevitably arise in the future.\n\nsuch\n\n118\n\n\nAppendix A\n\nSome Useful Formulae\n\nThe symbols a, b, c, r and s represent real numbers, m and n are positive integers, and indices\ni and j are non-negative integers.\n\nA.1 Binomial formulae\n\n(a+b)? =a? + 2ab4+ 0? (a4\n\nb)(a—b) =a —\n(a +b)? = a? + 3a7b + 3ab? + b3 (a+b)4\n\nb)4 = a4 + 4a3b + 6a?b? + dab? + bt\n\nA.2 Powers and roots\n\na®=1 ab=a\n\na\" =1/(a’) allt = vq\na’as = a\"*s a\" /a’ =a'~*\na’b® = (ab)* a®/b* = (a/b)*\n\n(a\")§ =a\"s = a®\" = (a’)\"\nand the following are special cases of the above:\n\nVaVvb = Vab aml? — w/qm = fq”\nVa/ Vb = V/a/b al\") = 1/(/a™) = 1/(X/a)”\n\nA.3 Logarithms\n\nDefinition: The logarithm of c to base a, written as log, c, is the real number b satisfying the\nequation c = a’, in which we assume that c > 0 and a> 1.\n\nThere are two special cases worth noting, namely log, 1 = 0, since a? = 1, and log, a = 1,\n! — q. From the definition, we immediately see that:\n\nsince a\nlogge __ b_\na°’e©=ce and log,a =b\n\nand we can move easily from one base a to another a’ using:\n\nlog, b = logy a * log, b.\n\n119\n\n\nThe key rules for logarithms are:\nlog, (bc) = log, b + log, ¢\n\nlog, (b/c) = log, b — log, c\nlog, (b\") = r log, b\n\nand the following are special cases of those rules:\nloga” = nloga\nlog Va = (1/n) loga\nFor large n we have the useful approximation:\n\nlogn! = nlogn + O(n)\n\nA.4 Sums\n\nWe often find it useful to abbreviate a sum as follows:\n\nn\n\nSs aj, = ay +ag+a3+-+++ An\ni=l\n\nWe can view this as an algorithm or program: Let s hold the sum at the end, and double []\na be an array holding the numbers we wish to add, that is afi] = a;, then:\n\ndouble s = 0\nfor (i=1; i<=n ; i++ )\ns=s +af[il]\n\ncomputes the sum. The most common use of sums for our purposes is when investigating the\ntime complexity of an algorithm or program. For that, we often have to count a variant of\n14+2+---+n, so it is helpful to know that:\n\nn\n\n(n+1\nyi 1424..tn= Ret )\ni=l\n\nTo illustrate this, consider the program in which k counts the instructions:\n\nfor (i=0O0;i<nj; i++) {\nfor( j=0; j <= i; jt) {\nk++ // instruction 1\nk++ // instruction 2\nk++ // instruction 3\n\n120\n\n\nUsing the above formula, the time complexity k is computed as follows:\n\nn-1l i n-1 n-1 n n(n+1)\nk= S003 = O43 3G +1) = 3 i= 3.\ni=0 j=0 i=0 i=0 i=1\n\nTwo more sums that often prove useful when computing complexities are:\n\nfoe)\n1 1 1 1 1\ngol totats tig 2\ni=0\nfoe) .\n1 1 2 4\nya =04+545 3 =!\ni 2! 2 4 8 = 16\ni=0\nbecause they can be truncated at large n to give:\nn\n1 1 1 1 1\n1+=4+-4+- +— 22 1\n> 2! 2 4 8 2” ot)\ni=0\nn\na 1 2 8 n\nO+s+7+5 +—7+2=O(1\n» 2! 2 4 8 2” (1)\n\nwhich are needed for computing the complexity of heap tree creation and certain special cases\nof quicksort.\n\nA.5 Fibonacci numbers\nThe sequence of Fibonacci numbers Fy, is defined recursively by\nFy =fy-1it+ Fn-2\n\nwith the base values Fy = 0, Fi = 1. Thus the sequence begins 0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\n55, 89, 144, 233, 377, ...\n\n121\n\n\nIndex\n\nabstract data type, 7, 12, 17, 18, 20, 35, 85 checking, 46\n\nabstraction, 40 child, 31\n\naccess, 9, 63 children, 31\n\naccessor methods, 13 circle, 99, 113\n\nadjacency lists, 101 circular doubly linked list, 19, 61\nadjacency matrix, 100 clustering, 94\n\nadjacent, 100 collision, 88\n\nalgorithm, 5, 15, 21, 26, 118 comparing, 64\n\nalphabetic, 40, 63 complete, 51\n\nancestor, 32 complexity, 26\n\nappend, 15 complexity class, 26, 29\n\narcs, 31, 99 components, 105\n\narray, 9, 21, 51, 87, 100 condition, 13, 16, 18, 19, 32, 34\naverage, 65 connected, 100, 113\n\naverage case, 25, 30, 78 connected component, 105\nAVL tree, 49 connection, 99, 100\n\nconstant complexity, 27, 92\nconstant factors, 29\nconstraints, 117\n\nB-tree, 49\nsalance factor, 49\npalanced, 36, 48, 51 construct, 13\n\nvase case, 13, 32, 34 constructed, 12, 17, 18\n\nest case, 78 constructors, 12, 13, 16-18, 32, 34\nBig-O notation, 27 contain, 102\n\n>ijection, 102 correctness proofs, 6, 10\n\nBin sort, 83 counter, 10\n\nbinary heap trees, 52, 72, 110, 116 cubic complexity, 27\n\ndinary search, 23\n\nbinary search tree, 41 data, 26\n\nbinary tree, 33, 34, 41 data structure, 5, 7, 21, 35, 85, 118\nBinomial heap, 59, 110, 116 decision tree, 65\n\nbinomial tree, 59 Declarative Programming, 5\n\noreadth first traversal, 104\noubble down, 55, 56, 73\noubble sort, 66 epth first traversal, 104\nsubble up, 54-56 erived operators, 34\n\ndelete, 45, 53, 61\nC\nC\nC\n\nBucket sort, 83 derived procedures, 15\nC\nC\nC\n\nepth, 32\n\nouckets, 91 escendent, 32\nguild, 32, 34, 41, 56 esign patterns, 7\nestructively, 13\n\nC, 5, 6, 10, 22, 23, 35, 90\n\n122\n\n\ndigraphs, 99\n\nDijkstra’s algorithm, 62, 105\ndirect chaining, 91\n\ndirected, 99\n\ndivide and conquer, 74\ndouble hashing, 92, 94\ndoubly linked list, 18\n\nedge contraction, 103\n\nedge quad-trees, 33\n\nedges, 31, 99\n\nefficiency, 5, 6, 25\n\nembeded, 103\n\nempty list, 12\n\nEmptyTree, 34\n\nencapsulation, 7\n\nencoding, 21\n\nerror message, 23, 46\nevolutionary computation, 117\nexact bound, 65\n\nexchange sort, 66\n\nexecute, 5, 27, 38\n\nexponential complexity, 27, 117\nexternal sorting algorithms, 64\n\nair, 56\n\nFibonacci heap, 61, 110, 116\nFibonacci numbers, 39, 61, 121\nfirst, 13\n\nFirst-In-First-Out (FIFO), 17\nFirst-In-Last-Out (FILO), 16\nFloyd’s algorithm, 111\n\n‘or-loop, 10\n\nully connected, 110\n\n‘unctional, 17\n\ngraph, 31, 99\n\ngraphs as arrays, 100\ngreedy, 114\n\ngrowth, 29\n\nhash function, 88\nhash table, 25, 85, 87\nheap tree, 52\nheapify, 56, 60, 73\nHeapsort, 72\n\nheight, 32, 36, 65\nheight balanced, 36\n\nheuristic, 76, 117\nhomeomorphic, 103, 104\n\nImperative Programming, 5\nimplementation, 14, 26, 37, 86\nimplementing, 5, 85\n\nincident, 100\n\nindex, 9\n\ninduction, 13, 32\n\ninduction step, 13, 32, 34\ninductive assertions, 11\ninheritance, 41\n\ninsert, 42, 53, 60, 61, 94\ninsertion sort, 67\n\ninsertion sorting, 67, 71\ninternal sorting algorithms, 64\ninvariants, 6, 10\n\nisEmpty, 13, 16\n\nisomorphic, 102\n\niteration, 10\n\nJava, 5, 6, 10, 22, 23, 35, 85, 86, 90, 101\n\nKy Ks K33, 103\n\nkeys, 40, 86\n\nKruskal’s algorithm, 116\nKuratowski’s theorem, 104\n\nabelled, 31\nLast-In-First-Out (LIFO), 16\nazy, 61\n\neaves, 32\n\neft, 34\n\neft subtree, 34\n\nength, 105\n\nevel, 32\n\nexicographic, 63\n\ninear complexity, 27\ninear probing, 92, 94\ninear search, 22\n\nines, 31, 99\n\ninked lists, 12\n\ninks, 99\n\nLisp, 14\n\nists, 12\n\noad factor, 89\n\nocating information, 21\nogarithm, 119\n\n123\n\n\nlogarithmic complexity, 27\nloop, 10\n\nloop-invariants, 10, 24\nlower bound, 64\n\nMakeList, 13\n\nMakeTree, 34\n\nmatrix, 100\n\nmedian, 79\n\nmerge, 58, 60, 61, 74, 80\nmergesort, 74, 79\n\nminimal spanning tree, 114\nminor, 103, 104\n\nmodular arithmetic, 90\nmodulo, 90\n\nmulti-objective optimization, 117\nmutators, 13\n\nneighbours, 100\nnodes, 31, 99\n\nOCaml, 5\n\noff-line, 63\n\non-line, 63\n\nopen addressing, 92\norder, 50, 63\noverestimate, 106\noverloaded, 9\n\nparent, 31, 32\n\nPareto front, 117\n\npartition, 75\n\npath, 32, 99\n\nperfectly balanced, 36, 48, 51\nperformance, 6\n\npivot, 75\n\nplanar graph, 103\n\nplanarity, 103\n\npointers, 12, 37, 42, 51, 101, 102\npoints, 31, 99\n\npolynomial time complexity, 117\npop, 16, 18\n\nprecondition, 45\n\npredecessor, 107, 111\n\nPrim’s algorithm, 62, 114\nprimary clustering, 94\n\nprimary position, 92, 94\nprimitive, 32\n\nprimitive data types, 7\nprimitive operators, 16, 32, 34\npriority, 52\n\npriority queue, 52, 72, 109\n\nprocessor, 38\nprogram, 5\nproof, 6, 115\n\nproof by induction, 11, 36\npseudocode, 5, 6, 10, 15, 22, 26\n\npush, 16, 17\n\nquad tree, 32\n\nquadratic complexity, 27\nqueue, 17, 104\nQuicksort, 74, 75\n\nRadix sort, 83\nrebalancing, 48, 50\nrecords, 37\nrecursion, 15, 37, 38\nred-black trees, 49\nrepresentation, 21\nresources, 6, 70\nrest, 13\n\nreverse, 70, 81\nright, 34\n\nright subtree, 34\nroot, 31, 34, 53\nrotation, 48, 49\n\nsearch, 21, 40, 63\n\nsearch key, 31, 40\n\nsecondary clustering, 94\nsecondary hash function, 92, 94\nselection sort, 69\n\nselection sorting, 69, 72\nselectors, 13, 16, 18, 19, 33, 34\n\nself-balancing binary search tree, 87\n\nself-loops, 99, 103\nshortcuts, 106, 111\nshortest path, 105\nsiblings, 32\n\nsimple graph, 99, 103\nsimple path, 99\n\nsize, 9, 25, 26, 32, 37, 89\nsmoothing, 102, 103\nsorting, 23, 40, 47, 63\n\n124\n\nprobably approximately correct, 65\n\n\nsorting strategies, 64\nspace complexity, 25\nspanning tree, 114\nsparse, 101, 111, 112\nspecification, 6, 22, 85\nstability, 71, 72, 74, 77, 80\nstack, 16, 38, 104\nstoring, 9, 12, 40, 51, 85\nstrongly connected, 100\nsubdivision, 102-104\nsubgraph, 102-104, 114\nsupergraph, 102\nsymmetric, 101\n\nable, 85\n\nhree phase, 84\n\nhree-cells, 18\n\night, 106\n\nime complexity, 25, 26, 64\n\nime complexity, constant, 15\n\nime complexity, linear, 15, 23, 58, 104\nime complexity, logarithmic, 24, 27\nime complexity, quadratic, 70\n\nop, 16, 18\n\nrade-off, 25, 117\n\nTravelling Salesman Problem, 65, 99, 117\nraversal, 104\n\nree, 31, 100\n\nree rotations, 48, 49\n\nrees as arrays, 51\n\nTreesort, 71\n\nwo-cells, 12\n\nundirected, 99\nupper bound, 64\n\nvalue, 31\n\nVehicle Routing Problem, 117\nverification, 6, 10\n\nvertices, 31, 99\n\nvon Mises birthday paradox, 88\n\nWagner’s theorem, 104\nweakly connected, 100\nweight matrix, 100\nweighted, 99\n\nworst case, 25, 30, 65, 78\n\nXML, 14\n\n\n",
    "time_taken": "206.03 seconds",
    "memory_used": "1352.49 MB"
}