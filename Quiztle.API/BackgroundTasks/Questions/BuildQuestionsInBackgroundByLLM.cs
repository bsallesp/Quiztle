using Microsoft.AspNetCore.Http.HttpResults;
using Microsoft.AspNetCore.Mvc;
using Newtonsoft.Json;
using Newtonsoft.Json.Linq;
using Quiztle.API.Controllers.LLM.Interfaces;
using Quiztle.CoreBusiness.Entities.Quiz;
using Quiztle.CoreBusiness.Entities.Scratch;
using Quiztle.DataContext.DataService.Repository;
using Quiztle.DataContext.DataService.Repository.Quiz;
using Quiztle.DataContext.Repositories;
using Quiztle.DataContext.Repositories.Quiz;
using System.Data;

namespace Quiztle.API.BackgroundTasks.Questions
{
    public class BuildQuestionsInBackgroundByLLM
    {
        private readonly ILLMChatGPTRequest _llmRequest;
        private readonly AILogRepository _aILogRepository;
        private readonly TestRepository _testRepository;
        private readonly DraftRepository _draftRepository;
        private readonly QuestionRepository _questionRepository;

        private static CancellationTokenSource? _cts;
        private static DateTime _lastRequestTime = DateTime.MinValue;
        private static readonly TimeSpan RequestTimeout = TimeSpan.FromMinutes(5);
        private static SemaphoreSlim _semaphore = new SemaphoreSlim(1);
        private bool debugCW = false;

        public BuildQuestionsInBackgroundByLLM(
            ILLMChatGPTRequest llmRequest,
            AILogRepository aILogRepository,
            TestRepository testRepository,
            DraftRepository draftRepository,
            QuestionRepository questionRepository)
        {
            _llmRequest = llmRequest;
            _aILogRepository = aILogRepository;
            _testRepository = testRepository;
            _draftRepository = draftRepository;
            _questionRepository = questionRepository;
        }

        public async Task<IActionResult> ExecuteAsync(Guid draftId, int questionsAmount = 5)
        {
            if (!await TryAcquireSemaphoreAsync())
                return new ObjectResult("Ação já em andamento.") { StatusCode = 429 };

            try
            {
                var draft = await _draftRepository.GetDraftByIdAsync(draftId) ?? throw new Exception("DRAFT IS NULL");

                var llmInput = await PrepareLLMInputAsync(draft, questionsAmount);
                var llmRequestResult = await _llmRequest.ExecuteAsync(llmInput);

                var questions = ParseQuestionsFromLLMResult(llmRequestResult) ?? throw new Exception("QUESTIONS ARE NULL. FINISHING THE SCOPE.");
                await AddQuestionsToDraftAsync(draft, questions);
                await _draftRepository.SaveChangesAsync();

                return new JsonResult(new { message = "All drafts processed" }) { StatusCode = 200 };
            }
            catch (OperationCanceledException)
            {
                Console.WriteLine("Request was canceled.");
                return new ObjectResult("Request was canceled.") { StatusCode = 499 };
            }
            catch (Exception ex)
            {
                await LogErrorAsync(ex);
                return new ObjectResult("An unexpected error occurred while processing your request. Please try again later.") { StatusCode = 500 };
            }
            finally
            {
                ReleaseSemaphore();
            }
        }

        public async Task<IActionResult> ExecuteAsync()
        {
            if (!await TryAcquireSemaphoreAsync())
                return new ObjectResult("Ação já em andamento.") { StatusCode = 429 };

            try
            {
                if (debugCW) Console.WriteLine("Running BuildQuestionsInBackgroundByLLM/UpdateDraftByDraftAsync...");

                var draft = await GetNextDraftAsync() ?? throw new Exception("DRAFT IS NULL");
                if (debugCW) Console.WriteLine("Total questions in draft: " + draft.Questions?.Count);
                var llmInput = await PrepareLLMInputAsync(draft);
                var llmRequestResult = await _llmRequest.ExecuteAsync(llmInput);

                var questions = ParseQuestionsFromLLMResult(llmRequestResult);
                if (questions == null) throw new Exception("QUESTIONS ARE NULL. FINISHING THE SCOPE.");

                await AddQuestionsToDraftAsync(draft, questions);
                await _draftRepository.SaveChangesAsync();

                if (debugCW) Console.WriteLine("Questions generated by LLM finished.");
                return new JsonResult(new { message = "All drafts processed" }) { StatusCode = 200 };
            }
            catch (OperationCanceledException)
            {
                Console.WriteLine("Request was canceled.");
                return new ObjectResult("Request was canceled.") { StatusCode = 499 };
            }
            catch (Exception ex)
            {
                await LogErrorAsync(ex);
                return new ObjectResult("An unexpected error occurred while processing your request. Please try again later.") { StatusCode = 500 };
            }
            finally
            {
                ReleaseSemaphore();
            }
        }

        private async Task<bool> TryAcquireSemaphoreAsync()
        {
            var now = DateTime.UtcNow;
            if (_cts != null && now - _lastRequestTime > RequestTimeout)
            {
                _cts.Cancel();
                _cts.Dispose();
                _cts = null;
            }

            _cts = new CancellationTokenSource();
            _lastRequestTime = now;

            if (!await _semaphore.WaitAsync(TimeSpan.FromSeconds(1)))
            {
                Console.WriteLine("Red light at " + DateTime.UtcNow.ToString() + "...");
                return false;
            }

            return true;
        }

        private async Task<Draft?> GetNextDraftAsync()
        {
            return await _draftRepository.GetNextDraftToMakeQuestionsAsync() ?? throw new Exception("No drafts.");
        }

        private async Task<string> PrepareLLMInputAsync(Draft draft)
        {
            var stringQuestions = draft.Questions?.Where(q => !string.IsNullOrEmpty(q.Name))
                                                  .Select(q => q.Name)
                                                  .ToList();
            var llmInput = Prompts.CreateQuestionsPrompt.GetNewQuestionFromPages(draft.OriginalContent!, stringQuestions, 5);
            await LogLLMInputAsync(llmInput);
            return llmInput;
        }

        private async Task<string> PrepareLLMInputAsync(Draft draft, int questionsAmount = 5)
        {
            var stringQuestions = draft.Questions?.Where(q => !string.IsNullOrEmpty(q.Name))
                                                  .Select(q => q.Name)
                                                  .ToList();
            var llmInput = Prompts.CreateQuestionsPrompt.GetNewQuestionFromPages(draft.OriginalContent!, stringQuestions, questionsAmount);
            await LogLLMInputAsync(llmInput);
            return llmInput;
        }

        private async Task LogLLMInputAsync(string llmInput)
        {
            await _aILogRepository.CreateAILogAsync(new CoreBusiness.Log.AILog
            {
                JSON = llmInput,
                Name = "BuildQuestionsInBackgroundByLLM - llmInput"
            });
        }

        private List<Question>? ParseQuestionsFromLLMResult(string llmRequestResult)
        {
            JObject jsonObject = JObject.Parse(llmRequestResult);
            var questionsToken = jsonObject["Questions"] ?? throw new ArgumentException("No 'Questions' found in JSON.");
            return questionsToken.ToObject<List<Question>>();
        }

        private Task AddQuestionsToDraftAsync(Draft draft, List<Question> questions)
        {
            draft.Questions ??= [];
            foreach (var item in questions)
            {
                item.Tag = draft.Tag;
            }
            draft.Questions.AddRange(questions);
            return Task.CompletedTask;
        }

        private async Task LogErrorAsync(Exception ex)
        {
            var errorMessage = $"Error in BuildQuestionsInBackgroundByLLM/UpdateDraftByDraftAsync: {ex.Message}";
            var stackTrace = ex.StackTrace ?? "No stack trace available";

            var detailedError = new
            {
                Error = errorMessage,
                StackTrace = stackTrace,
                ExceptionType = ex.GetType().Name,
                InnerException = ex.InnerException?.Message ?? "No inner exception",
                Timestamp = DateTime.UtcNow
            };

            await _aILogRepository.CreateAILogAsync(new CoreBusiness.Log.AILog
            {
                JSON = JsonConvert.SerializeObject(detailedError),
                Name = "BuildQuestionsInBackgroundByLLM - Error"
            });

            Console.WriteLine(JsonConvert.SerializeObject(detailedError, Formatting.Indented));
        }

        private void ReleaseSemaphore()
        {
            Console.WriteLine("Releasing semaphore...");
            _semaphore.Release();
        }
    }
}